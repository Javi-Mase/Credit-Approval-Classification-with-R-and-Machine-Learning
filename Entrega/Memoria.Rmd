---
title: "Practica final Aprendizaje Computacional"
author: "Manuel Francisco Hidalgo Ros Javier García Masegosa Javier Prior Gomez"
output:
  html_document:
    df_print: paged
    highlight: kate
    number_sections: true
    theme: paper
    toc: true
    toc_float: true
  pdf_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r cars}
#setwd("/home/manu/FIUM/TERCERO/PrimerCuatri/AC/Proyecto/PracticasAC")

if(!require("caret")) {
  install.packages("caret", dependencies = c("Depends", "Suggests"))
  require(caret)
}

# Descargamos la base de datos
url <- "https://archive.ics.uci.edu/static/public/27/credit+approval.zip"
download.file(url, destfile = "credit_approval.zip")


# Descomprimimos la base de datos
unzip("credit_approval.zip")


# Cargamos la base de datos, na.string = "?" quitamos los datos con ese valor y lo sustituye por NA
credit <- read.table("crx.data", header = FALSE, sep = ",", na.strings = "?")


# Cargamos en credit.trainIdx la base de datos descargada del UCI
credit.trainIdx<-readRDS("credit.trainIdx.rds")
credit.Datos.Train<-credit[credit.trainIdx,]
credit.Datos.Test<-credit[-credit.trainIdx,]

# Crear un indicador para cada conjunto
credit.Datos.Train$Origen <- "train"
credit.Datos.Test$Origen <- "test"

# Combinar los conjuntos
combined_credit <- rbind(credit.Datos.Train, credit.Datos.Test)

# Librerias usadas durante la practica
library(caret)
library(randomForest)
library(nnet)
library(pROC)
library(reshape2)
library(ggplot2)
```

# Introducción

El documento ha cargado automáticamente la base de datos, ahora vamos a ir haciendo un preprocesado de los datos, primero el siguiente comando muestra la cantidad de valores NA por columna:

```{r}
colSums(is.na(credit))
```

Esto es por si alguna columna tiene demasiados valores NA para eliminarla

Convertir automáticamente columnas de tipo 'chr' a 'factor'

```{r}
credit[sapply(credit, is.character)] <- lapply(credit[sapply(credit, is.character)], as.factor)
```

Despues de esto empezamos especulando el significado de cada uno de las variables. Esto lo vamos a hacer buscando en internet el significado esperado de cada una, haciendo suposiciones y comprobando que el análisis de cada una de las variables sea coherente.

# Análisis de Variables

## V1

Se trata de una variable categórica, con valores en el dominio `{a, b}`. Tiene un significado que podemos intuir de forma relativamente fácil. De hecho, podemos pensar que "a" se trata de mujeres y "b" de hombres. Esto se debe a que la base es de los años 80, y en esa época era más común que los hombres soliciten créditos, y en el caso de las mujeres, seguramente las que lo solicitaban eran las que estaban solteras. Se trata de una época con pensamientos cerrados, y esto es un simple estudio estadístico.

```{r}
# Eliminamos niveles no usados
credit$V1 <- droplevels(credit$V1)
summary(credit$V1)
```

Como podemos comprobar nuestra suposición puede ser cierta.

## V2

Esta columna puede ser la edad del cliente. Es continuo ya que los días los puede estar teniendo en cuenta de alguna forma. Sin embargo, en Estados Unidos los menores no pueden solicitar un crédito y hay algunos valores menores de 18. Vamos a calcular cuantos:

```{r}
num_menores_18 <- sum(credit$V2 < 18, na.rm = TRUE)
total_valores <- sum(!is.na(credit$V2))
porcentaje_menores_18 <- (num_menores_18 / total_valores) * 100
porcentaje_menores_18
```

Comprobamos que aproximadamente el 5% de los clientes son menores de edad, al no ser una cifra muy significativa, podemos asumir que son valores fuera de rango, y que efectivamente podríamos estar ante la variable de edad. En siguientes apartados veremos qué hacer con estos valores.

Decidimos que es interesante analizar V2 ya que su media y mediana son parecidas, lo cual sugiere que la distribución puede ser simétrica (cercana a normal).

Generamos un histograma base con la función hist() de R para la variable V2 con probability = TRUE para mostrar la densidad en vez de las frecuencias absolutas. Además añadimos una línea de densidad para observar la forma de la distribución de V2, usando la función lines() y especificando el color "blue" para la curva.

```{r}
hist(credit$V2, probability = TRUE, main = "Histograma de V2 con Curva de Densidad")
lines(density(credit$V2, na.rm = TRUE), col = "blue")
```

Creamos un histograma con ggplot2 para la variable V2 del dataframe 'credit', estableciendo aesthetic mapping (aes) para usar V2 en el eje x y ademas y añadimos un histograma con color de contorno naranja y relleno naranja claro (alpha = 0.2) y definimos intervalos de 5 unidades usando el argumento breaks.

```{r}
myHist <- ggplot(data = credit, aes(x = V2)) +
  geom_histogram(col = "orange", fill = "orange", alpha = 0.2, breaks = seq(0, 80, by = 5)) +
  geom_density(col = "blue", lwd = 1) +
  labs(title = "Histograma para la variable V2 con línea de densidad", x = "V2", y = "Frecuencia")

# Mostrar el gráfico
myHist

```

Añadimos una línea vertical en el histograma de ggplot para marcar la media de V2 usando geom_vline() y especificando la posición con xintercept = mean() y hacemos lo mismo para marcar la mediana.

```{r}
myHist = myHist + geom_vline(xintercept = mean(credit$V2, na.rm = TRUE), col = "blue")
myHist = myHist + geom_vline(xintercept = median(credit$V2, na.rm = TRUE), col = "red")
```

Mostramos el gráfico final en ggplot2 con el histograma de V2, junto con las líneas de media (azul) y mediana (roja).

```{r}
myHist
```

Como vemos, V2 no sigue exactamente una distribución normal, ya que la media y la mediana no son idénticas, lo que indica una posible asimetría en los datos.

Para confirmar formalmente la falta de normalidad, realizaremos un gráfico Q-Q (Quantile-Quantile), que nos permite comparar la distribución de V2 con la distribución teórica normal.

```{r}

# Creamos el gráfico Q-Q para V2 utilizando ggplot2.
p1 = ggplot(data = credit, aes(sample = V2)) +
  # Añadimos un título al gráfico Q-Q.
  ggtitle("QQ plot para V2") +
  # Añadimos los puntos del gráfico Q-Q, que compara los cuantiles de la muestra de V2 
  # con los cuantiles teóricos de una distribución normal.
  geom_qq() + 
  # Añadimos la línea Q-Q teórica (stat_qq_line), que muestra cómo se deberían 
  # alinear los puntos si la distribución de V2 fuera normal.
  stat_qq_line() + 
  # Etiquetas para los ejes: 'Distribución teórica' en el eje x y 'Distribución muestral' en el eje y.
  xlab("Distribución teórica") + 
  ylab("Distribución muestral")
```

Mostramos el gráfico Q-Q, que permite ver si los puntos se alinean (lo que indicaría normalidad) o si se desvían de la línea (indicando falta de normalidad en la distribución de V2).

```{r}
p1
```

Como vemos hay una desviación de la diagonal. Por tanto, podemos concluir que no se trata de una distribución normal.

## V3

Nuestra especulación en cuanto a esta columna es que se trata de la cantidad de deuda que tienen los clientes. Sin embargo, no tiene sentido que el que más debe, el total sea 28\$:

```{r}
summary(credit$V3)
```

Por ello, pensamos que pueda estar dividida por $10^{3}$ o similar. De igual manera no nos importa la proporción que lleve.

## V4

En esta columna podríamos estar ante el estado civil de cada uno de los clientes. Siendo cada uno de los valores {l=desconocido, u=casados, y=solteros, t=otros}. Si calculamos el porcentaje de clientes casados:

```{r}
num_mayores_18 <- sum(credit$V2 > 18, na.rm = TRUE)
num_casados <- sum(credit$V4 == "u", na.rm = TRUE)
porcent_casados <- (num_casados/num_mayores_18) * 100
porcent_casados
```

Si buscamos en internet en torno al 70% de la población adulta en Estados Unidos estaba casada. Como podemos ver se aproxima mucho a nuestro resultado. Esa pequeña diferencia puede deberse a que muchos matrimonios al casarse, compran una casa, y piden un préstamo para ello.

Observamos en V4 que hay un elemento que no aparece y es necesario añadir.

```{r}
# Eliminamos niveles no usados
#credit$V4 <- droplevels(credit$V4)

# Ahora vamos a añadir un elemento que no aparece y luego lo hacemos factor
#levels(credit$V4)<-c(levels(credit$V4),"t")
str(credit$V4)


```

## V6

Analizamos la variable categórica V6 para obtener una visión general de su distribución y composición. Mostramos un resumen estadístico de V6, que nos indica la frecuencia de cada categoría en esta variable.

```{r}
summary(credit$V6)
```

Verificamos la estructura de V6 para confirmar que es una variable categórica (factor o carácter).

```{r}
str(credit$V6)
```

Calculamos el porcentaje de cada categoría en V6, para ello primero, obtenemos la frecuencia de cada categoría con table(credit\$V6) y luego, usamos prop.table() para calcular la proporción relativa de cada categoría y multiplicamos por 100 para expresarlo en porcentaje.

```{r}
porcent <- prop.table(table(credit$V6)) * 100
```

Creamos una tabla que combina tanto el número total de observaciones (frecuencia) como el porcentaje de cada categoría: cbind() se usa para unir la frecuencia y el porcentaje en una tabla única.

```{r}
porcent_table <- cbind(total = table(credit$V6), porcentaje = porcent)
```

Mostramos el vector de porcentajes, lo que nos permite visualizar el porcentaje de cada categoría en V6.

```{r}
porcent
```

Ahora que entendemos la distribución de frecuencias y porcentajes de cada categoría, generamos un gráfico de sectores para visualizar estos porcentajes de manera gráfica y comparativa.

Creamos el gráfico de sectores (o "quesos") usando la función pie(), pasándole el vector de porcentajes. Establecemos un título con el argumento main y aplicamos diferentes colores a cada categoría usando rainbow().

```{r}
pie(porcent, main = "Diagrama de Quesos para V6", col = rainbow(length(porcent)))
```

Mostramos nuevamente el vector de porcentajes para recordar los valores antes de proceder con el análisis de los valores NA.

```{r}
porcent
```

Calculamos la suma del resumen de V6 para confirmar el número total de observaciones, incluyendo posibles NA.

```{r}
sum(summary(credit$V6))
```

Calculamos el porcentaje de valores NA en la variable V6, asumiendo que hay 9 valores NA de un total de 690 observaciones.

```{r}
porcentaje_na <- 9 / 690 * 100
porcentaje_na
```

Concluimos que, siendo aproximadamente un 1.3% de valores faltantes (NA), la eliminación de estas observaciones es razonable, ya que este porcentaje es bajo y es poco probable que afecte significativamente el análisis.

## V8

Esta variable creemos que puede ser los años de empleo. Si nos fijamos en los valores de las columnas de V8 (años contratado) y las comparamos por filas con las de V2 (edad) nos podemos dar cuenta que nunca va a superar la edad. De hecho en muchos casos tiene coherencia la edad con el número de años contratado. Esto puede ser un claro indicativo que estamos ante una buena especulación. Aún así vamos a verlo en R:

```{r}
summary(credit$V8)
```

Podemos apreciar que el valor máximo no es muy grande. Esto nos puede despistar un poco, pero basta con informarnos un poco sobre la variable edad:

```{r}
summary(credit$V2)
```

¿Aporta algo este resumen? Por supuesto, de hecho tenemos la razón de que nuestra variable de años contratados presente datos tan bajos. Estamos ante un conjunto de clientes jóvenes, y como es lógico no tienen muchos años como empleados.

## V10

Esta variable hemos podido intuir junto a alguna investigaciones que representa si un cliente está actualmente empleado. Esto es muy interesante de cara al análisis por temas obvios de solvencia. Suponemos que {f=empleado, t=no empleado}. Pero, ¿realmente es consistente la cifra de empleados? Sí, lo vemos con:

```{r}
porcentajes_empleados <- prop.table(summary(credit$V10))*100
porcentajes_empleados
```

Según hemos podido encontrar en los años ochenta en Estados Unidos había un 60% de población activa. Esto es de gran utilidad para hacer una correlación junto a V16 (aprobada/rechazada).

## V11

Esta variable es posible que sea \`\`credit score", que es es una expresión numérica que representa la solvencia de un individuo. Según hemos podido comprobar también ha sido retocada (los valores), ya que los valores no siguen la notación habitual. Pero sí sabemos que cuanto mayor es el valor, mayor es la solvencia. En la Figura\~\ref{fig:credit_score} podemos encontrar un gráfico interesante que ilustra este concepto:

![Descripción de la imagen](imagenes/creditScorepx.png)

Es importante conocer la forma que van a tener nuestros datos de esta variable:

```{r}
hist(credit$V11, 
     breaks = seq(min(credit$V11, na.rm = TRUE), max(credit$V11, na.rm = TRUE), by = 1),
     main = "Histograma de Credit Score V11",
     xlab = "Valores de V11", 
     ylab = "Frecuencia",
     col = "lightblue", 
     border = "black")
```

La mediana indica que hay muchos ceros. Y con el histograma vemos que esto efectivamente es así:

grafico v11

Lo deberíamos de tener en cuenta a la hora de hacer el análisis multivariable.

## V13

Pasamos a analizar la última variable interesante para nuestro análisis. Esta variable representa composición de la población, clasificando en: cuidadano, residente permenente o inmigrante. Según consta en los datos, la distribución es 86%, 2.6% y 6.2%. ¿Se aproximará?

```{r}
prop.table(summary(credit$V13))*100
```

Como vemos son muy parecidos. Esto es un indicativo significativo. Concluimos que {g=ciudadano, p=residente permanente, s=inmigrante}.

## Razonamiento para comparar V14 y V16

Una razonamiento útil que podemos hacer es relacionar los códigos postales con la aprobación de los créditos solicitados. Suponemos que los códigos postales han sidos modificados de alguna manera para proteger la privacidad de los datos. Esto lo hacemos porque dependiendo de la ubicación de los solicitantes, si están en barrios más ricos, o barrios más humildes, podemos hacer una distinción de gente con más poder adquisitivo, que por lo general, representarán mayor parte de solicitudes aprobadas.

```{r}
library(ggplot2)
    ggplot(credit, aes(x = V16, y = V14, fill = V16)) + 
        geom_violin() +
        labs(title = "Distribución de V14 (Edad) por V16 (Decisión de Crédito)",
            x = "Decisión de Crédito (V16)", 
            y = "Edad (V14)") +
    theme_minimal() +
    scale_fill_manual(values = c("skyblue", "orange"))
```

## V15

Vamos a analizar la variable V15, que parece tener una distribución con muchos valores cercanos a 0 pero también valores atípicos elevados, lo que podría afectar la media.

Realizamos un resumen estadístico de la variable V15 para obtener una visión general de los valores, incluyendo mínimos, máximos, media y mediana.

```{r}
summary(credit$V15)
```

Generamos un histograma básico para observar la distribución de V15. Usamos probability = TRUE para mostrar el histograma en términos de densidad. Además añadimos una curva de densidad a la gráfica para visualizar mejor la forma de la distribución, usando la función density() con na.rm = TRUE para excluir valores NA.

```{r}
hist(credit$V15, probability = TRUE,main = "Histograma de V15 con Curva de Densidad")
lines(density(credit$V15, na.rm = TRUE), col = "blue")
```

Observamos que muchos valores altos de V15 hacen difícil ver los valores menores. Para solucionar esto, usamos ggplot2 para crear un histograma que enfoque la escala en valores menores, estableciendo los intervalos hasta 1000 (ajustable según el rango de interés).

Creamos un histograma de V15 con ggplot2, aplicando intervalos de 10 en el rango de 0 a 1000.

```{r}
myHist = ggplot(data = credit, aes(credit$V15)) +
  geom_histogram(col = "orange", fill = "orange", alpha = 0.2, breaks = seq(0, 1000, by = 10)) +
  labs(title = "Histograma para la variable V15 con línea de densidad")
```

Añadimos una línea vertical azul en el histograma para indicar la media de V15, utilizando mean() y especificando que se ignoren valores NA.

```{r}
myHist = myHist + geom_vline(xintercept = mean(credit$V15, na.rm = TRUE), col = "blue")
```

Añadimos una línea vertical roja para indicar la mediana de V15, usando median().

```{r}
myHist = myHist + geom_vline(xintercept = median(credit$V15, na.rm = TRUE), col = "red")
```

Mostramos el gráfico final en ggplot2, que incluye el histograma de V15 y las líneas de media y mediana.

```{r}
myHist
```

Al observar el histograma, vemos que la mayoría de los valores están cerca de 0, lo que indica una acumulación de valores bajos. Esto se confirma con el resumen estadístico: la media es de 1017.4 y la mediana es 5.0, lo que significa que la mayoría de los valores son pequeños, pero hay algunos valores muy altos (outliers) que elevan la media, creando una diferencia significativa entre media y mediana.

```{r}
p1 = ggplot(data=credit,aes(sample=V15)) +
  ggtitle("QQ plot para V15") +
  geom_qq() + 
  stat_qq_line() + 
  xlab("Distribución teórica") + ylab("Distribución muestral")
p1
```

## V16

Comenzamos con el análisis de la variable categórica V16, que contiene las clases objetivo o categorías para nuestro análisis.

Ahora vamos a renombrar algunas columnas para ganar legibilidad

```{r}
levels(credit$V16) <- c("rechazada", "aprobada")
```

Verificamos la estructura de V16 para confirmar que es de tipo factor y ver las categorías presentes en la variable.

```{r}
str(credit$V16)
```

Calculamos el porcentaje de cada categoría en V16. Primero, usamos table(credit\$V16) para obtener las frecuencias de cada categoría, luego, aplicamos prop.table() para obtener la proporción relativa, multiplicando por 100 para obtener el porcentaje.

```{r}
porcent <- prop.table(table(credit$V16)) * 100
```

Creamos una tabla combinada que incluye tanto el número total (frecuencia) como el porcentaje de cada categoría. Usamos cbind() para unir el total (frecuencia) y el porcentaje en una tabla.

```{r}
porcent_table <- cbind(total = table(credit$V16), porcentaje = porcent)
```

Mostramos el vector de porcentajes, que nos permite ver el porcentaje de cada categoría.

```{r}
porcent
```

Ahora que entendemos la frecuencia y proporción de cada categoría, creamos un diagrama de sectores (o "quesos") para ilustrarlo de forma gráfica.

Creamos el gráfico de sectores con pie(), donde usamos el vector de porcentajes.

```{r}
pie(porcent, main = "Diagrama de Quesos para V16", col = rainbow(length(porcent)))
```

Con este gráfico de sectores, podemos ver visualmente la distribución de cada categoría en V16. Dado que las categorías tienen frecuencias similares, podemos observar que la distribución es aproximadamente uniforme y no se identifican categorías con frecuencias extremas (outliers).

```{r}

melted_data <- melt(credit, id.vars = "V16", measure.vars = "V2", variable.name = "Variable", value.name = "Value")
melted_data

ggplot(melted_data, aes(x = V16, y = Value)) +
  geom_boxplot() +
  xlab("Categoría de V16") +
  ylab("Valores de V2") +
  ggtitle("Distribución de V2 por Categoría de V16")
```

Observando el boxplot podemos decir que ambos tienen una distribución simétrica. En cuanto a los outliners, no hay en exceso (en rechazada algo más).

Ambas tienen un sesgo positivo, ya que los bigotes superiores tienen mayor longitud.

```{r}
# Gráfico de densidad ajustado
ggplot(data = melted_data, aes(x = Value, color = V16, fill = V16)) +
  geom_density(alpha = 0.6) +
  scale_fill_discrete() +
  scale_color_discrete() +
  ylab("Densidad") +  # Etiqueta para el eje y
  xlab("Valores") +   # Etiqueta para el eje x
  ggtitle("Densidad de Valores por Especie")  # Título del gráfico

```

El análisis de la distribución de V2 muestra que ambas categorías (rechazada y aprobada) presentan una asimetría positiva, con picos de densidad más altos en valores bajos y una cola extendida hacia la derecha. La curva de las solicitudes rechazadas tiene una densidad más pronunciada cerca de los valores bajos (alrededor de 20), mientras que la curva de las solicitudes aprobadas es más dispersa y se extiende hacia valores más altos, sugiriendo que las aprobaciones están asociadas con un rango más amplio de valores.

La superposición de las dos curvas, especialmente en el rango bajo de V2, indica que esta variable por sí sola no es un fuerte discriminante entre aprobaciones y rechazos. Sin embargo, a medida que los valores de V2 aumentan, las aprobaciones se vuelven más frecuentes.

En resumen, V2 parece influir en la decisión de crédito, pero debido a la considerable superposición, podría requerir análisis adicionales junto con otras variables para mejorar la capacidad predictiva.

```{r}
# Boxplot chetados con puntos
ggplot(melted_data, aes(x = V16, y = Value, color=V16, fill=V16)) +
  geom_boxplot(alpha=0.6) +
  geom_jitter(color="black") +
  scale_fill_discrete() +
  scale_color_discrete() +
  xlab("Categoría de V16") +
  ylab("Valores de V2") +
  ggtitle("Distribución de V2 por Categoría de V16")
```

# Analisis multivariable

## V16 V9

```{r}
melted_data <- melt(credit, id.vars = "V16", measure.vars = "V9", variable.name = "Variable", value.name = "Value")

ggplot(melted_data, aes(x = V16, y = Value)) +
  geom_boxplot() +
  xlab("Categoría de V16") +
  ylab("Valores de V2") +
  ggtitle("Distribución de V2 por Categoría de V16")

```

Observando el boxplot podemos decir que ambos tienen una distribución simétrica. En cuanto a los outliners, no hay en exceso (en rechazada algo más). Ambas tienen un sesgo positivo, ya que los bigotes superiores tienen mayor longitud.

```{r}

# Gráfico de densidad ajustado
ggplot(data = melted_data, aes(x = Value, color = V16, fill = V16)) +
  geom_density(alpha = 0.6) +
  scale_fill_discrete() +
  scale_color_discrete() +
  ylab("Densidad") +  # Etiqueta para el eje y
  xlab("Valores") +   # Etiqueta para el eje x
  ggtitle("Densidad de Valores por Especie")  # Título del gráfico

```

Es interesante apreciar que hay ciertas zonas en las que se ensancha el gráfico de aprobadas y adelgaza el gráfico de rechazadas y viceversa. Por la relación comentada con anterioridad. Puede ser que en los intervalos [0,100] y [300,500] aproximadamente, correspondan a casas pertenecientes a barrios más adinerados.

```{r}
# Convertir automáticamente columnas de tipo 'chr' a 'factor'
combined_credit[sapply(combined_credit, is.character)] <- lapply(combined_credit[sapply(combined_credit, is.character)], as.factor)
```

```{r}
# Ahora vamos a renombrar algunas columnas para ganar legibilidad
levels(combined_credit$V16) <- c("rechazada", "aprobada")

# Comprobamos que no hay missing-data:
sum(!complete.cases(combined_credit))
# De momento no haremos nada con estos 37 datos perdidos
```

Vamos a ver qué forma tienen nuestros datos:

```{r}
str(combined_credit)
```

\

```{r}
# Ahora vamos a renombrar algunas columnas para ganar legibilidad
levels(combined_credit$V16) <- c("rechazada", "aprobada")

# Comprobamos que no hay missing-data:
sum(!complete.cases(combined_credit))
# De momento no haremos nada con estos 37 datos perdidos
```

Vamos a ver qué forma tienen nuestros datos:

```{r}
str(combined_credit)
```

# Pre-procesado de datos (I): Tratamiento de outliers y nulos:

Comenzamos renombrando las columnas en base a la información que tenemos:

```{r}
colnames(combined_credit)[colnames(combined_credit) == "V1"] <- "Genero"
colnames(combined_credit)[colnames(combined_credit) == "V2"] <- "Edad"
colnames(combined_credit)[colnames(combined_credit) == "V3"] <- "Deuda"
colnames(combined_credit)[colnames(combined_credit) == "V4"] <- "EstadoCivil"
colnames(combined_credit)[colnames(combined_credit) == "V8"] <- "AnyosContratado"
colnames(combined_credit)[colnames(combined_credit) == "V10"] <- "Empleado"
colnames(combined_credit)[colnames(combined_credit) == "V11"] <- "Solvencia"
colnames(combined_credit)[colnames(combined_credit) == "V13"] <- "composicionPoblacion"
```

[**Tratamiento de valores fuera de rango:**]{.underline}

Para ello, vamos a diseñar una función que detecte outliners. Como hemos aprendido a hacerlo es con la siguiente fórmula: si valor **∈** [Q1 - 1.5 \* RI, Q3 + 1.5 \* RI] se considera outliner. Siendo RI (Rango Intercuatil). Solo miramos el conjunto de Train para que el conjunto de Test no se aproveche de dichos datos (p. independecia de conjuntos).

```{r}
credit.Datos.Train <- combined_credit[combined_credit$Origen == "train", ]
credit.Datos.Test <- combined_credit[combined_credit$Origen == "test", ]

detectar_outliers <- function(x) {
  Q1 <- quantile(x, 0.25, na.rm = TRUE)
  Q3 <- quantile(x, 0.75, na.rm = TRUE)
  RI <- Q3 - Q1
  limite_inferior <- Q1 - 1.5 * RI
  limite_superior <- Q3 + 1.5 * RI
  return(x < limite_inferior | x > limite_superior)
}

credit.Datos.continuas <- credit.Datos.Train[, sapply(credit.Datos.Train, is.numeric)]
outliners <- lapply(credit.Datos.continuas, detectar_outliers)

# Contar los outliers por variable
sumas_outliners <- sapply(outliners, function(x) sum(x, na.rm = TRUE))
sumas_outliners
```

Sin embargo, estos datos no son tan representativos. Necesitamos saber qué porcentaje son atípicos del total de datos.

```{r}
numero_filas <- nrow(na.omit(credit.Datos.Train))

proporcion <- function(x) {
  return(x/numero_filas*100)
}

proporciones <- lapply(sumas_outliners, proporcion)
proporciones
```

Según hemos estado investigando cuando el porcentaje de outliners es superior a 5% hay que tratarlos ya que pueden ser problemáticos. En este caso como mucho tenemos un 15% que es un valor alto, pero tampoco es elevadísimo. Siendo estos: AnyosContratado, Solvencia y V15:

```{r}
boxplot(credit.Datos.Train$AnyosContratado,boxwex=0.15,ylab="AnyosContratado")
rug(jitter(credit.Datos.Train$AnyosContratado),side=2)
abline(h=mean(credit.Datos.Train$AnyosContratado,na.rm=T),lty=2)

boxplot(credit.Datos.Train$Solvencia,boxwex=0.15,ylab="Solvencia")
rug(jitter(credit.Datos.Train$Solvencia),side=2)
abline(h=mean(credit.Datos.Train$Solvencia,na.rm=T),lty=2)

boxplot(credit.Datos.Train$V15,boxwex=0.15,ylab="V15")
rug(jitter(credit.Datos.Train$V15),side=2)
abline(h=mean(credit.Datos.Train$V15,na.rm=T),lty=2)
```

Decidimos que queremos equiparar a los valores fuera de rango con los valores extremos. Esto se llama winsorización:\

```{r}
# Función para calcular límites de winsorización (IR)
calcular_limites <- function(x) {
  Q1 <- quantile(x, 0.25, na.rm = TRUE)
  Q3 <- quantile(x, 0.75, na.rm = TRUE)
  RI <- Q3 - Q1
  limite_inferior <- Q1 - 1.5 * RI
  limite_superior <- Q3 + 1.5 * RI
  return(c(limite_inferior, limite_superior))
}

# Función para aplicar winsorización con límites definidos
winsorizar_con_limites <- function(x, limites) {
  limite_inferior <- limites[1]
  limite_superior <- limites[2]
  x[x < limite_inferior] <- limite_inferior
  x[x > limite_superior] <- limite_superior
  return(x)
}

columnas_a_winsorizar <- c("AnyosContratado", "Solvencia", "V15")

credit.Datos.Train.wins <- credit.Datos.Train
credit.Datos.Test.wins <- credit.Datos.Test

# Calcular límites en el conjunto de entrenamiento
limites_winsorizacion <- lapply(credit.Datos.Train.wins[, columnas_a_winsorizar], calcular_limites)

# Winsorizar el conjunto de entrenamiento
for (col in columnas_a_winsorizar) {
  credit.Datos.Train.wins[[col]] <- winsorizar_con_limites(credit.Datos.Train.wins[[col]], limites_winsorizacion[[col]])
}

# Aplicar los mismos límites al conjunto de prueba (p. de independencia de conjuntos)
for (col in columnas_a_winsorizar) {
  credit.Datos.Test.wins[[col]] <- winsorizar_con_limites(credit.Datos.Test.wins[[col]], limites_winsorizacion[[col]])
}
```

En el caso de que NO queramos usar la winsorización solo debemos de comentar líneas:

```{r}
credit.Datos.Train <- credit.Datos.Train.wins
credit.Datos.Test <- credit.Datos.Test.wins
```

Comprobamos que ya no hay valores fuera de rango, excepto en "Edad", "Deuda" y "V14" que el porcentaje es muy bajo.

```{r}
credit.Datos.continuas <- credit.Datos.Train[, sapply(credit.Datos.Train, is.numeric)]
outliners <- lapply(credit.Datos.continuas, detectar_outliers)

# Contar los outliers por variable
sumas_outliners <- sapply(outliners, function(x) sum(x, na.rm = TRUE))
sumas_outliners
```

Sin embargo, debemos de analizar si en "Edad", "Deuda" y "V14" hay errores evidentes que puedan perjudicar el rendimiento del modelo:

```{r}
summary(credit.Datos.Train$Edad)
summary(credit.Datos.Train$Deuda)
summary(credit.Datos.Train$V14)
```

En Deuda y V14 no parece que haya valores incorrectos. Sobre todo en Deuda no parece que haya nada raro, en cuanto a V14 no podemos decir mucho ya que no conocemos (especulado) su significado.

Sin embargo en Edad nos podemos dar cuenta que hay valores erroneos, ya que los menores de edad en Estados Unidos no pueden solicitar créditos. Por tanto, podemos sustituir los valores menos a 18, por 18:

```{r}

ajustar_edad <- function(x) {
  x[x < 18] <- 18
  return(x)
}

credit.Datos.Train$Edad <- ajustar_edad(credit.Datos.Train$Edad)
credit.Datos.Test$Edad <- ajustar_edad(credit.Datos.Test$Edad)

summary(credit.Datos.Train$Edad)
summary(credit.Datos.Test$Edad)
```

Como vemos hemos conseguido transformarlo de forma correcta.

[**Tratamiento de valores nulos:**]{.underline}

Primero vamos a ver cuántos valores nulos tiene cada variable. Esto nos indica que variables hay que transformar:

```{r}
combined_credit <- rbind(credit.Datos.Train, credit.Datos.Test)
num_na <- sapply(combined_credit, function(x) sum(is.na(x)))
print(num_na)
```

Primero de todo, vamos a analizar las variables continuas para analizar su distribución y elegir el tipo de sustitución idónea para cada una de ellas. Pero antes debemos de asegurarnos de no violar el principio de independencia de conjuntos, es necesario volver a separar los datos, para tener actualizadas las bases de datos de entrenamiento y validación:

**Breve inciso:** Realmente, no hace falta separar la base de datos, ya que en la base de datos de validación no hay nulos. Sin embargo, hemos considerado que se trata de una buena práctica, para siempre caer en ese detalle.

```{r}
credit.Datos.Train <- combined_credit[combined_credit$Origen == "train", ]
credit.Datos.Test <- combined_credit[combined_credit$Origen == "test", ]

# Verificar dimensiones
dim(credit.Datos.Train)  # Debe coincidir con la tabla original de entrenamiento
dim(credit.Datos.Test)   # Debe coincidir con la tabla original de validación

```

Como vemos las dimensiones son las correctas. De hecho tenemos una columna más, ya que con ella distinguimos si se trata de un conjunto de validación y testing.

Ahora si podemos analizar las variables de credit:\

```{r}
# histograma enriquecido para Edad
hist(credit.Datos.Train$Edad, xlab="",
main="Máximo valor de Edad", ylim=c(0,0.07),probability=T)
lines(density(credit.Datos.Train$Edad,na.rm=T))
rug(jitter(credit.Datos.Train$Edad))
```

La mediana es robusta frente a sesgos y outliers, lo que la hace ideal para distribuciones asimétricas como esta. Mantendrá el equilibrio en el rango más común (20–40 años). Tampoco es necesario complicar la imputación ya que hay pocos valores nulos. En este caso 12.

```{r}
credit.Datos.Test.imp <- credit.Datos.Test
credit.Datos.Train.imp <- credit.Datos.Train

# Calcular la mediana en el conjunto de entrenamiento
mediana_edad <- median(credit.Datos.Train.imp$Edad, na.rm = TRUE)

# Imputar en el conjunto de entrenamiento
credit.Datos.Train.imp$Edad[is.na(credit.Datos.Train.imp$Edad)] <- mediana_edad

# Imputar en el conjunto de validación usando la mediana del entrenamiento
credit.Datos.Test.imp$Edad[is.na(credit.Datos.Test.imp$Edad)] <- mediana_edad

test_imp_num_na <- sapply(credit.Datos.Test.imp, function(x) sum(is.na(x)))
train_imp_num_na <- sapply(credit.Datos.Train.imp, function(x) sum(is.na(x)))
print(train_imp_num_na+test_imp_num_na)
```

Ahora vamos a analizar la variable V14 (continua).

```{r}
# histograma enriquecido para V14
hist(credit.Datos.Train$V14, xlab="",
main="Máximo valor de V14", ylim=c(0,0.004),probability=T)
lines(density(credit.Datos.Train$V14,na.rm=T))
rug(jitter(credit.Datos.Train$V14))

summary(credit.Datos.Train$V14)
```

La variable V14, según la gráfica y el resumen estadístico, presenta una distribución muy asimétrica, con la mayoría de los valores concentrados en el rango bajo (entre 0 y 280), pero con algunos valores muy altos (hasta 2000, outliners). Dado que hay 13 valores faltantes (NA), que no son muchos, pero sí más que las anteriores, debemos de elegir una buena ténica de imputación:

```{r}
# Calcular la mediana solo en el conjunto de entrenamiento
mediana_v14 <- median(credit.Datos.Train.imp$V14, na.rm = TRUE)

# Imputar NA en el conjunto de entrenamiento
credit.Datos.Train.imp$V14[is.na(credit.Datos.Train.imp$V14)] <- mediana_v14

# Imputar NA en el conjunto de validación
credit.Datos.Test.imp$V14[is.na(credit.Datos.Test.imp$V14)] <- mediana_v14

test_imp_num_na <- sapply(credit.Datos.Test.imp, function(x) sum(is.na(x)))
train_imp_num_na <- sapply(credit.Datos.Train.imp, function(x) sum(is.na(x)))
print(train_imp_num_na+test_imp_num_na)
```

Ahora solo nos quedan variables categóricas.

Procedemos con la primera variable cetegórica, Género:\

```{r}
frecuencias <- table(credit.Datos.Train$Genero)

barplot(frecuencias, 
        main = "Distribución de Genero", 
        xlab = "Genero", 
        ylab = "Frecuencia", 
        col = "skyblue")

```

Debido al bajo número de NA y sabiendo que la categoría altamente dominante de b (hombres, según hemos especulado). Pensamos que lo más apropiado es asumir que son hombres, imputación por la moda. Imputar por "unknown" pensamos que no beneficia en absoluto el algoritmo.

```{r}
moda_genero <- "b"

# Imputar en el conjunto de entrenamiento
credit.Datos.Train.imp$Genero[is.na(credit.Datos.Train.imp$Genero)] <- moda_genero

# Imputar en el conjunto de validación
credit.Datos.Test.imp$Genero[is.na(credit.Datos.Test.imp$Genero)] <- moda_genero

test_imp_num_na <- sapply(credit.Datos.Test.imp, function(x) sum(is.na(x)))
train_imp_num_na <- sapply(credit.Datos.Train.imp, function(x) sum(is.na(x)))
print(train_imp_num_na+test_imp_num_na)
```

La siguiente es la variable "EstadoCivil". Variable categórica con 3 valores categóricos, siendo su dominio {l,u,y}. Vamos a ver la distribución que sigue:

```{r}
frecuencias <- table(credit.Datos.Train$EstadoCivil)

barplot(frecuencias, 
        main = "Distribución de EstadoCivil", 
        xlab = "EstadoCivil", 
        ylab = "Frecuencia", 
        col = "skyblue")
```

Como en el caso anterior, y con más razón aún, vamos a imputar por la moda. Es evidente que hay una categoría muy dominante, y el bajo número de NA hace que no vaya a variar prácticamente la distribución:

```{r}
moda_genero <- "u"

# Imputar en el conjunto de entrenamiento
credit.Datos.Train.imp$EstadoCivil[is.na(credit.Datos.Train.imp$EstadoCivil)] <- moda_genero

# Imputar en el conjunto de validación
credit.Datos.Test.imp$EstadoCivil[is.na(credit.Datos.Test.imp$EstadoCivil)] <- moda_genero

test_imp_num_na <- sapply(credit.Datos.Test.imp, function(x) sum(is.na(x)))
train_imp_num_na <- sapply(credit.Datos.Train.imp, function(x) sum(is.na(x)))
print(train_imp_num_na+test_imp_num_na)
```

Analizamos la variable categórica V5 para saber qué forma tienen los datos:

```{r}
frecuencias <- table(credit.Datos.Train$V5)

barplot(frecuencias, 
        main = "Distribución de V5", 
        xlab = "V5", 
        ylab = "Frecuencia", 
        col = "skyblue")
```

Podemos imputar por la moda, ya que la categoría "g" es claramente dominante, y no cambiará mucho la distribución:

```{r}
moda_genero <- "g"

# Imputar en el conjunto de entrenamiento
credit.Datos.Train.imp$V5[is.na(credit.Datos.Train.imp$V5)] <- moda_genero

# Imputar en el conjunto de validación
credit.Datos.Test.imp$V5[is.na(credit.Datos.Test.imp$V5)] <- moda_genero

test_imp_num_na <- sapply(credit.Datos.Test.imp, function(x) sum(is.na(x)))
train_imp_num_na <- sapply(credit.Datos.Train.imp, function(x) sum(is.na(x)))
print(train_imp_num_na+test_imp_num_na)
```

Procedemos a evaluar V6 (categórica):

```{r}
frecuencias <- table(credit.Datos.Train$V6)

barplot(frecuencias, 
        main = "Distribución de V6", 
        xlab = "V6", 
        ylab = "Frecuencia", 
        col = "skyblue")
```

Como V6 no tiene categorías tan predominantes, por tanto, necesitamos emplear otra técnica. Como tenemos 9 NA, que no son muchos, tampoco tenemos por qué complicarlo mucho. Algo interesante que podemos hacer es imputación aleatoria ponderada. Como su propio nombre indica consiste en generar categorías de forma aleatoria teniendo en cuenta su participación en la variable.

```{r}
set.seed(123)
categorias <- names(table(credit.Datos.Train.imp$V6))
probabilidades <- prop.table(table(credit.Datos.Train.imp$V6))

# Imputar valores NA
credit.Datos.Train.imp$V6[is.na(credit.Datos.Train.imp$V6)] <- sample(categorias, size = sum(is.na(credit.Datos.Train.imp$V6)), replace = TRUE, prob = probabilidades)

sum(is.na(credit.Datos.Train.imp$V6))

test_imp_num_na <- sapply(credit.Datos.Test.imp, function(x) sum(is.na(x)))
train_imp_num_na <- sapply(credit.Datos.Train.imp, function(x) sum(is.na(x)))
print(train_imp_num_na+test_imp_num_na)
```

Por último, analizamos la variable categórica V7:

```{r}
frecuencias <- table(credit.Datos.Train$V7)

barplot(frecuencias, 
        main = "Distribución de V7", 
        xlab = "V7", 
        ylab = "Frecuencia", 
        col = "skyblue")
```

Es evidente que una sustitución por la moda es muy interesante en esta variable también:

```{r}
moda_genero <- "v"

# Imputar en el conjunto de entrenamiento
credit.Datos.Train.imp$V7[is.na(credit.Datos.Train.imp$V7)] <- moda_genero

# Imputar en el conjunto de validación
credit.Datos.Test.imp$V7[is.na(credit.Datos.Test.imp$V7)] <- moda_genero

test_imp_num_na <- sapply(credit.Datos.Test.imp, function(x) sum(is.na(x)))
train_imp_num_na <- sapply(credit.Datos.Train.imp, function(x) sum(is.na(x)))
print(train_imp_num_na+test_imp_num_na)
```

A continuación, vamos a comprobar que la distribución de las variables imputadas, siguen siendo casi iguales (no hayan cambiado mucho):

```{r}
# Configurar la ventana gráfica para dos gráficos lado a lado
par(mfrow = c(1, 2))  # 1 fila, 2 columnas

#____________________Genero_________________________
frecuencias <- table(credit.Datos.Train$Genero)
barplot(frecuencias, 
        main = "Distribución de Genero", 
        xlab = "Genero", 
        ylab = "Frecuencia", 
        col = "skyblue")

frecuencias <- table(credit.Datos.Train.imp$Genero)
barplot(frecuencias, 
        main = "Distribución de Genero", 
        xlab = "Genero", 
        ylab = "Frecuencia", 
        col = "lightgreen")

#____________________Edad_________________________
hist(credit.Datos.Train$Edad, 
     main = "Histograma de Edad", 
     xlab = "Edad", 
     col = "skyblue")

hist(credit.Datos.Train.imp$Edad, 
     main = "Histograma de Edad", 
     xlab = "Edad", 
     col = "lightgreen")

#________________________EstadoCivil______________________
frecuencias <- table(credit.Datos.Train$EstadoCivil)
barplot(frecuencias, 
        main = "Distribución de EstadoCivil", 
        xlab = "EstadoCivil", 
        ylab = "Frecuencia", 
        col = "skyblue")

frecuencias <- table(credit.Datos.Train.imp$EstadoCivil)
barplot(frecuencias, 
        main = "Distribución de EstadoCivil", 
        xlab = "EstadoCivil", 
        ylab = "Frecuencia", 
        col = "lightgreen")

#____________________V5_______________
frecuencias <- table(credit.Datos.Train$V5)
barplot(frecuencias, 
        main = "Distribución de V5", 
        xlab = "V5", 
        ylab = "Frecuencia", 
        col = "skyblue")

frecuencias <- table(credit.Datos.Train.imp$V5)
barplot(frecuencias, 
        main = "Distribución de V5", 
        xlab = "V5", 
        ylab = "Frecuencia", 
        col = "lightgreen")

#____________________V6_______________
frecuencias <- table(credit.Datos.Train$V6)
barplot(frecuencias, 
        main = "Distribución de V6", 
        xlab = "V6", 
        ylab = "Frecuencia", 
        col = "skyblue")

frecuencias <- table(credit.Datos.Train.imp$V6)
barplot(frecuencias, 
        main = "Distribución de V6", 
        xlab = "V6", 
        ylab = "Frecuencia", 
        col = "lightgreen")

#____________________V7_______________
frecuencias <- table(credit.Datos.Train$V7)
barplot(frecuencias, 
        main = "Distribución de V7", 
        xlab = "V7", 
        ylab = "Frecuencia", 
        col = "skyblue")

frecuencias <- table(credit.Datos.Train.imp$V7)
barplot(frecuencias, 
        main = "Distribución de V7", 
        xlab = "V7", 
        ylab = "Frecuencia", 
        col = "lightgreen")

#____________________V14_______________
hist(credit.Datos.Train$V14, 
     main = "Histograma de V14", 
     xlab = "V14", 
     col = "skyblue")

hist(credit.Datos.Train.imp$V14, 
     main = "Histograma de V14", 
     xlab = "V14", 
     col = "lightgreen")
```

Como es evidente no ha cambiado casi nada (inapreciable). Entre otras cosas, debido al bajo número de NA.

En cuanto a la sustitución mediante estudio de correlaciones, podemos ver si hay alguna de la siguiente manera (lo hacemos con los datos sin imputar para justificar que no era necesario usarlo):

```{r}
# Solo las columnas numéricas
credit.Datos.numericas <- credit.Datos.Train[, sapply(credit.Datos.Train, is.numeric)]

matriz_correlacion <- cor(credit.Datos.numericas, use = "complete.obs")

print(matriz_correlacion)
```

Es evidente que no hay ninguna fuerte correlación entre variables, por tanto, hemos excluido esta opción de imputación. Al no haber correlación tampoco es interesante la sustitución de variables numéricas mediante clustering (knn).

```{r}
gen_plots <- function() {

  credit.Datos.continuas <- credit.Datos.Train.imp[, sapply(credit.Datos.Train.imp, is.numeric)]
  
  for (name in colnames(credit.Datos.continuas)) {
    hist(credit.Datos.continuas[[name]], 
         main = paste("Histograma de", name),
         xlab = name,
         col = "lightgreen")
  }
}

# Ejecutar la función
gen_plots()

```

En el caso en el que queramos quitar toda la imputación, comentar este código:

```{r}
credit.Datos.Test <- credit.Datos.Test.imp
credit.Datos.Train <- credit.Datos.Train.imp
```

# Pre-procesado de datos (II): Eliminar predictores correlados o de poca Varianza

## Eliminar variables con poca Varianza

Debemos de comprobar si la base de datos tiene columnas con poca varianza. Esto lo podemos comprobar con nearZero():

**Aclaración:** No tenemos en cuenta la última columna con valores {train, test} ya que no cuentan para el análisis.

```{r}
nearZeroVar(credit.Datos.Train[, 1:16])
nearZeroVar(credit.Datos.Test[, 1:16])
```

Como vemos no hay ninguna columna que tenga varianza cercana a cero, lo que nos indica que los datos están ya bastante "limpios".

Quitamos la última columna

```{r}
credit.Datos.Train$Origen <- NULL
credit.Datos.Test$Origen <- NULL
```

# Entrenamiento de modelos que proporciona Caret

## **GBM**

Lo primero que debemos de buscar son las características de este modelo para ver si es interesante para nuestro caso. En este enlace podemos encontrar mucha información acerca del mismo:\
<https://www.linkedin.com/pulse/ai-algorithms-deep-dive-gradient-boosting-machines-gbm-vasu-rao-snesc>

En dicho enlace, en el apartado "Applications of GBM in Enterprises", recomienda usarlo para "Credit Scoring":

-   **Credit Scoring:** Financial institutions use GBM to predict the creditworthiness of loan applicants, enabling informed lending decisions and minimizing risk.

También contamos con información sobre sus debilidades, y es importante tenerlas en cuenta:

While GBM is a powerful tool, it's not without its limitations:

-   **Computational Intensity:** Training GBM models can be computationally expensive, especially when dealing with large datasets and many boosting stages (iterations).

-   **Risk of Overfitting:** Without proper tuning and regularization techniques, GBM models can overfit, particularly on noisy data. Overfitting occurs when the model performs well on the training data but poorly on unseen data.

-   **Parameter Sensitivity:** The performance of GBM models is highly dependent on the chosen hyperparameters. Careful tuning is necessary to achieve optimal results.

Por lo tanto, puede ser una buena opción. Una vez hemos investigado sobre su funcionamiento, es importante ver con qué requisitos de preprocesado gbm se encuentra más cómodo. En el enlace anterior recomiendan:

1.  **Data Preprocessing:** Clean your data to ensure optimal GBM performance. This includes handling missing values, normalizing numerical features, and encoding categorical variables.

Hacemos una copia de la base de datos para no sobrescribirla.

```{r}
credit.Datos.gbm.Train <- credit.Datos.Train
credit.Datos.gbm.Test <- credit.Datos.Test
```

Nota: Para poder hacer muchas pruebas y automatizar todo el proceso de configuraciones de preprocesado hemos ideado una automatización (comprobada experimentalmente) que te permite probar todas las configuraciones existentes, entrenar con cada una de ellas y seleccionar la "mejor" en cuanto al intervalo de confianza de cada una de las configuraciones. A continuación le mostramos como se hace:

Para empezar vamos a mencionar cada una de los elementos de preprocesado que van a estar presentes en las configuraciones, y por qué lo hacemos:

1.  [**Dummy:**]{.underline} Preprocesado prometedor. Siguiendo las recomendaciones del enlace anterior, se recomienda encarecidamente hacer dummy.

2.  [**PCA:**]{.underline} Pese a no indicar en ningún sitio que PCA/ICA sea necesario en gbm, debemos de comprobarlo experimentalmente. Por lo tanto, lo incluimos en nuestro preprocesado.

3.  [**ICA:**]{.underline} Igual que PCA.

4.  [**Normalización Range:**]{.underline} En la web anterior recomiendan normalizar los datos antes del entrenamiento, sin embargo, como sabemos gbm está basado en árboles de decisión, y la normalización no les afecta mucho. Como podemos encontrar en: [https://datascience.stackexchange.com/questions/6721/how-to-preprocess-different-kinds-of-data-continuous-discrete-categorical-be](#0){.uri}

    *"In fact, the results should be consistent regardless of any scaling or translational normalization, since the trees can choose equivalent splitting points"*

    Por lo tanto, ante esta contradicción, la única forma de comprobarlo es realizando los dos tipos de normalización que encontramos en el documento de caret y viendo resultados.

5.  [**Normalización Center-Scale:**]{.underline} Igual que Range.

Nota: En PCA, debido a la opción fullRank=TRUE, se está eliminando una categoría de cada variable categórica después de crear las dummies.

### Declaración de funciones necesarias para configuraciones:

#### Dummy

```{r}
dummy <- function(train, test) {
  credit.Datos.gbm.dummy.Train <- get(train, envir = .GlobalEnv)
  credit.Datos.gbm.dummy.Test <- get(test, envir = .GlobalEnv)
  
  #DUMMY
  # Definir la variable de salida y las variables de entrada
  credit.Var.Salida.Usada <- "V16"
  credit.Vars.Entrada.Usadas <- setdiff(names(credit.Datos.gbm.dummy.Train), credit.Var.Salida.Usada)
  
  # Convertir la variable de salida a factor (para clasificación)
  credit.Datos.gbm.dummy.Train[[credit.Var.Salida.Usada]] <- as.factor(credit.Datos.gbm.dummy.Train[[credit.Var.Salida.Usada]])
  credit.Datos.gbm.dummy.Test[[credit.Var.Salida.Usada]] <- as.factor(credit.Datos.gbm.dummy.Test[[credit.Var.Salida.Usada]])
  
  # Crear el modelo dummy solo para las variables de entrada
  dummy_model <- dummyVars(~ ., data = credit.Datos.gbm.dummy.Train[, credit.Vars.Entrada.Usadas], fullRank = TRUE)
  
  # Transformar las variables de entrada a dummies (sin tocar la salida)
  credit.Datos.Train.Dummies <- data.frame(predict(dummy_model, credit.Datos.gbm.dummy.Train[, credit.Vars.Entrada.Usadas]))
  credit.Datos.Test.Dummies <- data.frame(predict(dummy_model, credit.Datos.gbm.dummy.Test[, credit.Vars.Entrada.Usadas]))
  
  # Añadir la variable de salida de nuevo a los conjuntos transformados
  credit.Datos.Train.Dummies[[credit.Var.Salida.Usada]] <- credit.Datos.gbm.dummy.Train[[credit.Var.Salida.Usada]]
  credit.Datos.Test.Dummies[[credit.Var.Salida.Usada]] <- credit.Datos.gbm.dummy.Test[[credit.Var.Salida.Usada]]
  
  assign(train, credit.Datos.Train.Dummies, envir = .GlobalEnv)
  assign(test, credit.Datos.Test.Dummies, envir = .GlobalEnv)
}
```

#### PCA

```{r}
pca <- function(train, test) {
  credit.Datos.gbm.pca.Train <- get(train, envir = .GlobalEnv)
  credit.Datos.gbm.pca.Test <- get(test, envir = .GlobalEnv)
  
  # PCA
  credit.PreProc.Pca.Mod <- preProcess(credit.Datos.gbm.pca.Train, method = "pca", thresh = 0.95)  # 95% de la varianza explicada
  print(credit.PreProc.Pca.Mod)
  credit.Datos.Train.Pca <- predict(credit.PreProc.Pca.Mod, credit.Datos.gbm.pca.Train)
  credit.Datos.Test.Pca <- predict(credit.PreProc.Pca.Mod, credit.Datos.gbm.pca.Test)
  
  assign(train, credit.Datos.Train.Pca, envir = .GlobalEnv)
  assign(test, credit.Datos.Test.Pca, envir = .GlobalEnv)
}
```

#### ICA

```{r}
ica <- function(train, test) {
  credit.Datos.gbm.ica.Train <- get(train, envir = .GlobalEnv)
  credit.Datos.gbm.ica.Test <- get(test, envir = .GlobalEnv)
  
  # ICA
  credit.PreProc.Ica.Mod <- preProcess(credit.Datos.gbm.ica.Train, method = "ica", n.comp = 10)  # 10 componentes independientes
  credit.PreProc.Ica.Mod
  credit.Datos.Train.Ica <- predict(credit.PreProc.Ica.Mod, credit.Datos.gbm.ica.Train)
  credit.Datos.Test.Ica <- predict(credit.PreProc.Ica.Mod, credit.Datos.gbm.ica.Test)
  
  assign(train, credit.Datos.Train.Ica, envir = .GlobalEnv)
  assign(test, credit.Datos.Test.Ica, envir = .GlobalEnv)
}
```

#### Normalización range

```{r}
norm_range <- function(train, test) {
  credit.Datos.gbm.norm.range.Train <- get(train, envir = .GlobalEnv)
  credit.Datos.gbm.norm.range.Test <- get(test, envir = .GlobalEnv)
  
  # Normalización range
  # Identificar las variables numéricas del conjunto de entrenamiento
  numeric_vars <- names(credit.Datos.gbm.norm.range.Train)[sapply(credit.Datos.gbm.norm.range.Train, is.numeric)]
  
  # Ajustar el modelo de preprocesamiento en el entrenamiento (normalización Min-Max)
  # "range" realiza una normalización entre 0 y 1
  preProcParams <- preProcess(credit.Datos.gbm.norm.range.Train[, numeric_vars], method = c("range"))
  
  # Aplicar la normalización al conjunto de entrenamiento
  credit.Datos.Train.norm <- predict(preProcParams, credit.Datos.gbm.norm.range.Train)
  
  # Aplicar la misma transformación al conjunto de test
  credit.Datos.Test.norm <- predict(preProcParams, credit.Datos.gbm.norm.range.Test)
  
  assign(train, credit.Datos.Train.norm, envir = .GlobalEnv)
  assign(test, credit.Datos.Test.norm, envir = .GlobalEnv)
}
```

#### Normalización Center-Scale

```{r}
norm_center_scale <- function(train, test) {
  credit.Datos.gbm.norm.cs.Train <- get(train, envir = .GlobalEnv)
  credit.Datos.gbm.norm.cs.Test <- get(test, envir = .GlobalEnv)
  
  # Normalización center-scale
  credit.Var.Salida.Usada <- c("V16")
  credit.Var.Entrada.Usadas <- setdiff(names(credit.Datos.gbm.norm.cs.Train),credit.Var.Salida.Usada)
  
  credit.preProc.CS.Mod<-preProcess(credit.Datos.gbm.norm.cs.Train[credit.Var.Entrada.Usadas],
                                    method=c("center","scale"))
  credit.Datos.Train.CS <- predict(credit.preProc.CS.Mod,credit.Datos.gbm.norm.cs.Train)
  credit.Datos.Test.CS <- predict(credit.preProc.CS.Mod,credit.Datos.gbm.norm.cs.Test)
  
  assign(train, credit.Datos.Train.CS, envir = .GlobalEnv)
  assign(test, credit.Datos.Test.CS, envir = .GlobalEnv)
}
```

### Configuración

```{r}
configuracion <- list(
  conf1  = list("dummy"),
  conf2  = list("pca"),
  conf3  = list("ica"),
  conf4  = list("norm_range"),
  conf5  = list("norm_center_scale"),
  
  # Combinaciones de Dummy
  conf6  = list("dummy", "pca"),
  conf7  = list("dummy", "ica"),
  conf8  = list("dummy", "norm_range"),
  conf9  = list("dummy", "norm_center_scale"),
  
  # Combinaciones con PCA
  conf10 = list("pca", "norm_range"),
  conf11 = list("pca", "norm_center_scale"),
  
  # Combinaciones con ICA
  conf12 = list("ica", "norm_range"),
  conf13 = list("ica", "norm_center_scale"),
  
  # Combinaciones triples
  conf14 = list("dummy", "pca", "norm_range"),
  conf15 = list("dummy", "pca", "norm_center_scale"),
  conf16 = list("dummy", "ica", "norm_range"),
  conf17 = list("dummy", "ica", "norm_center_scale"),
  
  # Combinaciones Cuádruples
  conf18 = list("dummy", "pca", "norm_range", "ica"),
  conf19 = list("dummy", "ica", "norm_center_scale", "norm_range")
)
```

### Eliminar variables con varianza cercana a cero

```{r}
eliminar_varianza_cero <- function(train_data, test_data) {
  
  # Detectar columnas con varianza cero
  cols_varianza_cero <- nearZeroVar(train_data, saveMetrics = TRUE)
  
  # Mostrar columnas eliminadas
  print("Columnas Eliminadas por Varianza Cero:")
  print(rownames(cols_varianza_cero[cols_varianza_cero$nzv == TRUE, ]))
  
  # Eliminar columnas de Train y Test
  train_data <- train_data[, !cols_varianza_cero$nzv]
  test_data <- test_data[, !cols_varianza_cero$nzv]
  
  # Verificar que no queden columnas con varianza cero
  print("Verificación - Varianza Cero después de Eliminación:")
  print(paste("Train:", length(nearZeroVar(train_data))))
  print(paste("Test:", length(nearZeroVar(test_data))))
  
  # Retornar los datos actualizados
  return(list(train = train_data, test = test_data))
}
```

### Ejecutar funciones según configuración

```{r}
ejecutar_configuracion <- function(config, train, test) {
  if ("dummy" %in% config) dummy(train, test)
  if ("pca" %in% config) pca(train, test)
  if ("ica" %in% config) ica(train, test)
  if ("norm_range" %in% config) norm_range(train, test)
  if ("norm_center_scale" %in% config) norm_center_scale(train, test)
}
```

### Almacenar modelos entrenados

```{r}
modelos_entrenados <- list()
```

### Definir control de entrenamiento

```{r}
# Definir control de entrenamiento
credit.trainCtrl <- trainControl(
  method = "repeatedcv",  # Validación cruzada repetida
  number = 10,            # 10 folds
  repeats = 3,            # 3 repeticiones
  classProbs = TRUE,      # Probabilidades de clase para métricas avanzadas
  summaryFunction = defaultSummary  # ROC, Sensibilidad, Especificidad
)
```

### Ejecutamos todas las configuraciones

```{r}
for (i in seq_along(configuracion)) {
  # Crear nombre de configuración
  nombre_config <- names(configuracion)[i]
  preprocesamientos <- configuracion[[i]]
  
  # Restaurar bases de datos originales
  credit.Datos.gbm.Train <- credit.Datos.Train
  credit.Datos.gbm.Test <- credit.Datos.Test
  
  # Aplicar configuración
  ejecutar_configuracion(preprocesamientos, "credit.Datos.gbm.Train", "credit.Datos.gbm.Test")
  
  # Quitar variables con varianza cercana a cero
  resultados <- eliminar_varianza_cero(credit.Datos.gbm.Train, credit.Datos.gbm.Test)

  # Actualizar los conjuntos de datos
  credit.Datos.gbm.Train <- resultados$train
  credit.Datos.gbm.Test <- resultados$test

  
  # Definir variables de entrada y salida
  credit.Var.Salida.Usada <- "V16"
  credit.Vars.Entrada.Usadas <- setdiff(names(credit.Datos.gbm.Train), credit.Var.Salida.Usada)
  
  # Entrenar el modelo
  modelo_entrenado <- train(
    x = credit.Datos.gbm.Train[, credit.Vars.Entrada.Usadas],
    y = credit.Datos.gbm.Train[[credit.Var.Salida.Usada]],
    method = "gbm",
    trControl = credit.trainCtrl,
    tuneGrid = data.frame(
      n.trees = 500,
      interaction.depth = 5,
      shrinkage = 0.01,
      n.minobsinnode = 10
    ),
    metric = "Accuracy",
    verbose = FALSE
  )
  
  # Guardar modelo
  modelos_entrenados[[nombre_config]] <- modelo_entrenado
  
  # Mostrar información del modelo actual
  print(paste("Modelo entrenado para la configuración:", nombre_config))
}
```

### Interpretamos los resultados obtenidos

```{r}
# Comparar modelos usando resamples()
resultados_resampling <- resamples(modelos_entrenados)

# Mostrar estadísticas resumidas
summary(resultados_resampling)

# Graficar resultados
bwplot(resultados_resampling, metric = "Accuracy")
dotplot(resultados_resampling, metric = "Accuracy")

# Prueba estadística de diferencias
diferencias_modelos <- diff(resultados_resampling)
summary(diferencias_modelos)
```

Basándonos en el gráfico, **conf1** parece ser la mejor configuración porque tiene la mayor media de Accuracy con un intervalo de confianza relativamente pequeño. Como podemos ver en la matriz de configuraciones, conf1 se corresponde al uso exclusivo de dummy.

Por lo tanto, antes de encontrar los mejores hiper-parámetros, debemos de guardar la configuración definitiva, que hemos dicho que será solo aplicando dummy:

```{r}
credit.Datos.gbm.Train <- credit.Datos.Train
credit.Datos.gbm.Test <- credit.Datos.Test

dummy("credit.Datos.gbm.Train", "credit.Datos.gbm.Test")

# Quitar variables con varianza cercana a cero
resultados <- eliminar_varianza_cero(credit.Datos.gbm.Train, credit.Datos.gbm.Test)

# Actualizar los conjuntos de datos
credit.Datos.gbm.Train <- resultados$train
credit.Datos.gbm.Test <- resultados$test
```

### Encontrar los mejores hiper-parámetros y ajustar modelo final

```{r}
library(caret)
library(gbm)

# Configuración de la validación cruzada
# Hemos probado varias y la que mejor resultado ha dado es "repeatedcv"
credit.trainCtrl.3cv10.resampAll <- trainControl(
  method = "repeatedcv", # Validación cruzada de 10 pliegues
  number = 10,           # Número de pliegues
  repeats = 3,           # Repeticiones
  verboseIter = F,       # Voy a quitar Verbose porque inunda toda la pantalla
  returnResamp = "all"   # Guardar todos los resultados
)

# Rnago de hiperparámetros para GBM
gbm.grid <- expand.grid(
  n.trees = c(100, 500),              # Árboles limitados
  shrinkage = c(0.01, 0.1),           # Tasas de aprendizaje clave
  n.minobsinnode = c(5, 10),          # Nodos hijos mínimos
  interaction.depth = c(3, 5)         # Profundidad limitada
)

# Definir variables predictoras y objetivo
credit.Var.Salida.Usada <- "V16"
credit.Vars.Entrada.Usadas <- setdiff(names(credit.Datos.gbm.Train), credit.Var.Salida.Usada)

set.seed(2)
credit.modelo.3cv10.grid.gbm<-train(
  credit.Datos.gbm.Train[credit.Vars.Entrada.Usadas],
  credit.Datos.gbm.Train[[credit.Var.Salida.Usada]],
  method="gbm", trControl=credit.trainCtrl.3cv10.resampAll,
  tuneGrid = gbm.grid
,verbose=F
)

# Mostrar resultados
print(credit.modelo.3cv10.grid.gbm)

# Evaluación en el conjunto de test
predictions <- predict(credit.modelo.3cv10.grid.gbm, credit.Datos.gbm.Test)

# Calcular matriz de confusión y Accuracy
conf_mat <- confusionMatrix(predictions, credit.Datos.gbm.Test[[credit.Var.Salida.Usada]])

# Mostrar el porcentaje de acierto
accuracy <- conf_mat$overall["Accuracy"]
print(paste("Porcentaje de Acierto:", round(accuracy * 100, 2), "%"))
```

Obtenemos un porcentaje de acierto de un 86.86% pero va a tener algo de peeking. Por lo tanto, vamos a hacer un entrenamiento final con hiper-parámetros fijos.

### Entrenamos con hiper-parámetros fijos

Una vez hemos encontrado los mejores hiper-parámetros debemos de entrenar con dichos hiper-parámetros. Los que mejor resultado han dado son:\
\
Árboles seleccionados: 500\
Profundidad máxima: 5\
Tasa de aprendizaje: 0.01\
Mínimos nodos por hoja: 10\
\
Por lo tanto, vamos a entrenar con dichos hiper-parámetros:

```{r}

# Configuración para entrenamiento final
credit.trainCtrl.none <- trainControl(
  method = "none",  # Sin remuestreo
  classProbs = TRUE # Para calcular métricas de clasificación
)

# Hiperparámetros seleccionados previamente
best_hyperparams <- data.frame(
  n.trees = 500,              # Número de árboles
  interaction.depth = 5,      # Profundidad máxima
  shrinkage = 0.01,           # Tasa de aprendizaje
  n.minobsinnode = 10         # Nodos mínimos por hoja
)

# Definir variables predictoras y objetivo
credit.Var.Salida.Usada <- "V16"
credit.Vars.Entrada.Usadas <- setdiff(names(credit.Datos.gbm.Train), credit.Var.Salida.Usada)

# Entrenar el modelo final con los hiperparámetros fijos
set.seed(123)
modelo_final_gbm <- train(
  x = credit.Datos.gbm.Train[, credit.Vars.Entrada.Usadas],
  y = credit.Datos.gbm.Train[[credit.Var.Salida.Usada]],
  method = "gbm",
  trControl = credit.trainCtrl.none,
  tuneGrid = best_hyperparams,  # Hiperparámetros fijos
  metric = "Accuracy",          # Métrica de evaluación
  verbose = FALSE               # Sin salida en consola
)

# Evaluar el modelo final en el conjunto de prueba
predicciones <- predict(modelo_final_gbm, credit.Datos.gbm.Test)

# Calcular métricas de evaluación en el conjunto de prueba
library(caret)
confusion_matriz <- confusionMatrix(predicciones, credit.Datos.gbm.Test[[credit.Var.Salida.Usada]])

# Mostrar métricas de rendimiento
print(confusion_matriz)
print(paste("Porcentaje de Acierto:", round(confusion_matriz$overall["Accuracy"] * 100, 2), "%"))

```

**Es importante destacar que:\
**Es curioso que en la búsqueda de hiper-parámetros obtengamos diferentes rendimientos que cuando "replicamos" el entrenamiento seleccionando ya los parámetros. Mientras en el primero obtenemos 86.86% en el otro obtenemos un 86.13%.

El 86.13% es probablemente el acierto real en el conjunto de prueba, mientras que el 86.86% es un resultado estimado basado en la validación cruzada repetida, en la que se introduce peeking. El procentaje estimado en la búsqueda de hiper-parámetros suele ser más optimista, mientras que el entrenamiento final refleja mejor el rendimiento real.

Para concluir, para gbm lo único realmente necesario es hacer Dummy, lo demás, para este caso y este modelo es innecesario.

Según hemos podido encontrar en esta página web: <https://www.619.io/blog/2017/06/30/preliminary-investigation-pca-and-boosting/>\

"It is a bit disappointing to observe no improvement after reducing the number of features using PCA; and even worse, we get some noticeable degradation of the baseline result. Sometimes things like that happen… A more thorough feature tinkering might help here to reduce the number of features and achieve better performance!"

### Conclusión:

Terminamos este entrenamiento habiendo probado muchísimas configuraciones de preprocesado (de forma automática), eligiendo la más prometedora, y buscando los parámetros idóneos que debería de tener nuestro modelo para obtener buenas predicciones. Siendo nuestro porcentaje de acierto experimental: 86.13%.

**Nota:** Cabe destacar que hemos probado otro tipo de preprocesados como la eliminación de variables no usadas con frecuencia, sin conseguir prácticamente ninguna mejora, por eso consideramos no ponerlo.

## **SVM**

En primer lugar vamos a tratar de buscar información a cerca de las características del modelo. Gran parte de la información obtenida es de la siguiente pagina.\
[https://medium.com/\@vdeshpande551/understanding-the-inner-workings-of-svm-from-data-preprocessing-to-model-deployment-cdc9d72a2d34](https://medium.com/@vdeshpande551/understanding-the-inner-workings-of-svm-from-data-preprocessing-to-model-deployment-cdc9d72a2d34){.uri}\
El modelo SVM (Support Vector Machines) funciona buscando el hiperplano óptimo que maximiza la separación entre las clases en el espacio de características. Por esta razón, SVM es sensible a la escala de los datos, ya que utiliza medidas de distancia. Por ello, probaremos varios modelos con diferentes métodos de escalado.

Además, es importante tener en cuenta que SVM no trabaja directamente con variables categóricas, por lo que será necesario transformarlas en dummies. Supondremos que todas las variables categóricas son de tipo ranked.

Otro aspecto necesario para que nuestro modelo funcione es eliminar las variables con poca correlación con la variable objetivo. También realizaremos otras pruebas, como:

-   Reducir la dimensionalidad con diferentes métodos.

-   Tratar los datos desbalanceados.

-   Eliminar ciertas variables que no tengan importancia en el modelo final.

De este modo, porbaremos distintos preprocesados de datos con el objetivo de encontrar el que mejor rendimiento proporcione a nuestro model.

```{r}
credit.Datos.svm.Train <- credit.Datos.Train
credit.Datos.svm.Test <- credit.Datos.Test
```

### Declaracion de funciones.

Para entrenar nuestro modelo, utilizaremos las funciones definidas en GBM. Sin embargo, añadiremos una nueva función para eliminar las variables con varianza igual a cero, de modo que podamos ejecutarla cuando sea necesario, en lugar de eliminar estas variables antes de entrenar el modelo, como se hacía en GBM.

```{r}
eliminar_varianza_cero_svm <- function(train_name, test_name) {
  train_data <- get(train_name, envir = .GlobalEnv)
  test_data <- get(test_name, envir = .GlobalEnv)
  
  
  nzv <- nearZeroVar(train_data)
  train_data_clean <- train_data[, -nzv]
  test_data_clean <- test_data[, names(train_data_clean)]
  
  assign(train_name, train_data_clean, envir = .GlobalEnv)
  assign(test_name, test_data_clean, envir = .GlobalEnv)
}
```

### Configuración

```{r}
configuracion_svm <- list(
  conf1  = list("dummy", "var_cero"),
  conf2  = list("norm_center_scale", "dummy", "var_cero"),
  conf3  = list("norm_range", "dummy", "var_cero"),
  conf4  = list("dummy", "var_cero", "pca"),
  conf5  = list("dummy", "var_cero", "ica"),
  conf6  = list("norm_center_scale", "dummy", "var_cero", "pca"),
  conf7  = list("norm_center_scale", "dummy", "var_cero", "ica"),
  conf8  = list("norm_range", "dummy", "var_cero", "pca"),
  conf9  = list("norm_range", "dummy", "var_cero", "ica")
)
```

### Ejecutar funciones según configuración

```{r}
ejecutar_configuracion_svm <- function(config, train, test) {
  if ("norm_range" %in% config) norm_range(train, test)
  if ("norm_center_scale" %in% config) norm_center_scale(train, test)
  if ("dummy" %in% config) dummy(train, test)
  if ("var_cero" %in% config) eliminar_varianza_cero_svm(train, test)
  if ("pca" %in% config) pca(train, test)
  if ("ica" %in% config) ica(train, test)
}
```

### Almacenar modelos entrenados

```{r}
modelos_entrenados_svm <- list()
```

### Ejecutamos todas las configuraciones

```{r}
for (i in seq_along(configuracion_svm)) {
  # Crear nombre de configuración
  nombre_config <- names(configuracion_svm)[i]
  preprocesamientos <- configuracion_svm[[i]]
  
  # Restaurar bases de datos originales
  credit.Datos.svm.Train <- credit.Datos.Train
  credit.Datos.svm.Test <- credit.Datos.Test
  
  # Aplicar configuración
  ejecutar_configuracion_svm(preprocesamientos, "credit.Datos.svm.Train", "credit.Datos.svm.Test")
  
  # Definir variables de entrada y salida
  credit.Var.Salida.Usada <- "V16"
  credit.Vars.Entrada.Usadas <- setdiff(names(credit.Datos.svm.Train), credit.Var.Salida.Usada)
  
  # Entrenar el modelo
  set.seed(1234)
  modelo_entrenado <- train(
    x = credit.Datos.svm.Train[, credit.Vars.Entrada.Usadas],
    y = credit.Datos.svm.Train[[credit.Var.Salida.Usada]],
    method = "svmRadial",
    trControl = credit.trainCtrl.3cv10.resampAll,
    tuneGrid = data.frame(
      sigma = 0.03951598,              
      C = 1         
    ),
    metric = "Accuracy",
    verbose = FALSE
  )
  
  # Guardar modelo
  modelos_entrenados_svm[[nombre_config]] <- modelo_entrenado
  
  # Mostrar información del modelo actual
  print(paste("Modelo entrenado para la configuración:", nombre_config))
}
```

### Interpretamos los resultados obtenidos

```{r}
# Comparar modelos usando resamples()
resultados_resampling <- resamples(modelos_entrenados_svm)

# Mostrar estadísticas resumidas
summary(resultados_resampling)

# Graficar resultados
bwplot(resultados_resampling, metric = "Accuracy")
dotplot(resultados_resampling, metric = "Accuracy")

# Prueba estadística de diferencias
diferencias_modelos <- diff(resultados_resampling)
summary(diferencias_modelos)
```

Aunque no podemos asegurar caul es la mejor ya que los intervalos coincide, de acerdo con la información busacada nos vamos a quedar con conf4.

```{r}
credit.Datos.svm.Train <- credit.Datos.Train
credit.Datos.svm.Test <- credit.Datos.Test

dummy("credit.Datos.svm.Train", "credit.Datos.svm.Test")
eliminar_varianza_cero_svm("credit.Datos.svm.Train", "credit.Datos.svm.Test")
pca("credit.Datos.svm.Train", "credit.Datos.svm.Test")

```

### Encontrar los mejores hiper-parámetros y ajustar modelo final

```{r}
set.seed(1234)
credit.modelo.svm <- train(
  V16 ~ .,
  data = credit.Datos.svm.Train,                    # Datos de entrenamiento
  method = "svmRadial",                             
  trControl = credit.trainCtrl.3cv10.resampAll,   # Validación cruzada d
  tuneLength = 20                                         # Probar diferentes valores de parámetros
)

#Mostrar porcentaje final
# Realizar predicciones en el conjunto de prueba
predicciones_test <- predict(credit.modelo.svm, credit.Datos.svm.Test)

# Generar la matriz de confusión
matriz_confusion <- confusionMatrix(predicciones_test, credit.Datos.svm.Test[[credit.Var.Salida.Usada]])

# Extraer el porcentaje de acierto
porcentaje_acierto <- matriz_confusion$overall["Accuracy"] * 100

# Mostrar el porcentaje de acierto
print(paste("Porcentaje de acierto:", round(porcentaje_acierto, 2), "%"))
```

Obtenemos un porcentaje de acierto de un 86.13% pero va a tener algo de peeking. Por lo tanto, vamos a hacer un entrenamiento final con hiper-parámetros fijos.

### Entrenamos con hiper-parámetros fijos

Tras probar 20 combinaciones de hiper-parametros vamos a cuales han sido los que finalmente se han elegido para el modelo final.

```{r}
modelLookup(("svmRadial"))
```

El parámetro C es el parámetro de penalización, cuanto más altos sean los valores que este tome más estricto será, dando prioridad a clasificar correctamente todos los puntos. Con valores mas pequeños permite un margen más amplio, tolerando así más errores de clasificación.

El parámetro sigma, al igual que C cuanto mayor es su valor más cercanos serán los puntos considerados frontera de decisión y viceversa.

<https://stackabuse.com/understanding-svm-hyperparameters/>

Los hiper-parámetros de nuestro modelo final son los siguientes:

```{r}
credit.modelo.svm$bestTune
```

```{r}
set.seed(2)
credit.modelo.final.svm <- train(
  V16 ~ .,
  data = credit.Datos.svm.Train,
  method = "svmRadial",
  trControl =  credit.trainCtrl.none,
  tuneGrid = data.frame(
    sigma = 0.03951598,              
    C = 1         
  ),
  metric = "Accuracy",          # Métrica de evaluación
  verbose = FALSE               # Sin salida en consola
)

# Evaluación en el conjunto de prueba
predictions <- predict(credit.modelo.final.svm, credit.Datos.svm.Test)

# Calcular matriz de confusión y accuracy
conf_mat <- confusionMatrix(predictions, credit.Datos.svm.Test[[credit.Var.Salida.Usada]])
accuracy <- conf_mat$overall["Accuracy"]

# Mostrar resultados
print(credit.modelo.final.svm)
print(paste("Porcentaje de Acierto (Modelo Final):", round(accuracy * 100, 2), "%"))
```

### Submuestreo con clases desvalanceadas

Para ver si nuestro modelo mejora vamos a tratar las clases desvalanceadas en el conjunto de los datos de entrenamiento.

```{r}
credit.Var.Salida.Usada <- c("V16")
credit.Vars.Entrada.Usadas <- setdiff(names(credit.Datos.svm.Train),credit.Var.Salida.Usada)
```

#### down-sampling:

Consiste en reducir el número de ejemplos de las clases más frecuentes para igualar la clase menos frecuente.

```{r}
set.seed(12345)
credit.Datos.Train.svm.downsmpld<-downSample(x=credit.Datos.svm.Train[credit.Vars.Entrada.Usadas],
                                       y=credit.Datos.svm.Train[[credit.Var.Salida.Usada]],
                                       yname=credit.Var.Salida.Usada)
credit.Datos.Test.svm.downsmpld <- credit.Datos.svm.Test
```

#### up-sampling:

En este caso hace lo contrario, remuestrea (con reemplazamiento) la clase minoritaria para igualarla a la mayoritaria (repite varios datos).

```{r}
set.seed(1234)
credit.Datos.Train.svm.upsmpld<-upSample(x=credit.Datos.svm.Train[credit.Vars.Entrada.Usadas],
                                   y=credit.Datos.svm.Train[[credit.Var.Salida.Usada]],
                                   yname=credit.Var.Salida.Usada)
credit.Datos.Test.svm.upsmpld<- credit.Datos.svm.Test
```

Creamos y probamos un modelo para cada una de estas configuraciones:

```{r}
set.seed(1234)
credit.modelo.svm.downsmpld <- train(
  V16 ~ .,  # Variable de salida ~ Variables de entrada
  data = credit.Datos.Train.svm.downsmpld,                                    # Datos de entrenamiento
  method = "svmRadial",
  trControl =  credit.trainCtrl.none,
  tuneGrid = data.frame(
    sigma = 0.03951598,              
    C = 1         
  ),
  metric = "Accuracy",          # Métrica de evaluación
  verbose = FALSE
)
#Mostrar porcentaje final
# Realizar predicciones en el conjunto de prueba
predicciones_test <- predict(credit.modelo.svm.downsmpld, credit.Datos.Test.svm.downsmpld)

# Generar la matriz de confusión
matriz_confusion <- confusionMatrix(predicciones_test, credit.Datos.Test.svm.downsmpld[[credit.Var.Salida.Usada]])

# Extraer el porcentaje de acierto
porcentaje_acierto <- matriz_confusion$overall["Accuracy"] * 100

# Mostrar el porcentaje de acierto
print(paste("Porcentaje de acierto:", round(porcentaje_acierto, 2), "%"))
```

```{r}
set.seed(1234)
credit.modelo.svm.upsmpld <- train(
  V16 ~ .,  # Variable de salida ~ Variables de entrada
  data = credit.Datos.Train.svm.upsmpld,                                    # Datos de entrenamiento
  method = "svmRadial",
  trControl =  credit.trainCtrl.none,
  tuneGrid = data.frame(
    sigma = 0.03951598,              
    C = 1         
  ),
  metric = "Accuracy",          # Métrica de evaluación
  verbose = FALSE
)
#Mostrar porcentaje final
# Realizar predicciones en el conjunto de prueba
predicciones_test <- predict(credit.modelo.svm.upsmpld, credit.Datos.Test.svm.upsmpld)

# Generar la matriz de confusión
matriz_confusion <- confusionMatrix(predicciones_test, credit.Datos.Test.svm.upsmpld[[credit.Var.Salida.Usada]])

# Extraer el porcentaje de acierto
porcentaje_acierto <- matriz_confusion$overall["Accuracy"] * 100

# Mostrar el porcentaje de acierto
print(paste("Porcentaje de acierto:", round(porcentaje_acierto, 2), "%"))
```

Como vemos, estos modelos no mejoran el porcentaje de acierto que teniamos.

### Eliminación de variables

```{r}
#Ver importancia de las variables de entrada
#Metodo -> varImp()
varImp(credit.modelo.final.svm)
#Ver seleccion de variables del modelo 
#Metodo -> predictors(modelo$finalModel)
predictors(credit.modelo.final.svm)
```

Como vemos la variable PC5 no tiene importancia a la hora de realizar la clasificación. Vamos a probar a eliminarla para ver si nuestro modelo mejora.

```{r}
credit.Datos.Train.svm.Reduc <- credit.Datos.svm.Train
credit.Datos.Test.svm.Reduc <- credit.Datos.svm.Test
credit.Datos.Train.svm.Reduc$PC5 <- NULL
credit.Datos.Test.svm.Reduc$PC5 <- NULL
```

```{r}
set.seed(1234)
credit.modelo.svm.Reduc <- train(
  V16 ~ .,  # Variable de salida ~ Variables de entrada
  data = credit.Datos.Train.svm.Reduc,                                    # Datos de entrenamiento
  method = "svmRadial",
  trControl =  credit.trainCtrl.none,
  tuneGrid = data.frame(
    sigma = 0.03951598,              
    C = 1         
  ),
  metric = "Accuracy",          # Métrica de evaluación
  verbose = FALSE
)

predicciones_test <- predict(credit.modelo.svm.Reduc, credit.Datos.Test.svm.Reduc)

# Generar la matriz de confusión
matriz_confusion <- confusionMatrix(predicciones_test, credit.Datos.Test.svm.Reduc[[credit.Var.Salida.Usada]])

# Extraer el porcentaje de acierto
porcentaje_acierto <- matriz_confusion$overall["Accuracy"] * 100

# Mostrar el porcentaje de acierto
print(paste("Porcentaje de acierto:", round(porcentaje_acierto, 2), "%"))
```

Como podemos observar la eliminación de la variable PC5, que no tenía impotancia el modelo final entrenado, no supone un aumento en el porcentaje de acierto.

### Conclusión

Para la configuración de nuestro modelo SVM, el mejor resultado lo obtienen los datos tras haber hecho Dummys, eliminado las variables con poca varianza y habiendo aplicado PCA. De esta forma obtenemos un porcentaje de acierto de 85.4%.

El modelo final sería:

```{r}
print(credit.modelo.final.svm)
```

## **KNN**

K-Nearest Neighbors (KNN) es un algoritmo de clasificación que predice la clase de una observación en función de las k observaciones más cercanas en el espacio de características. En nuestro caso, nos interesa aplicar KNN para clasificación.

Debido a la forma en que funciona KNN, la normalización de los datos será fundamental, ya que el algoritmo depende de las distancias entre las observaciones. También será necesario transformar las variables categóricas en dummys, para que el modelo pueda manejarlas adecuadamente.

No nos centraremos tanto en la reducción de la dimensionalidad, ya que KNN no se ve tan afectado por la cantidad de características como otros algoritmos, aunque podría aplicarse si se considera necesario. Por último, realizaremos pruebas eliminando variables poco importantes y también trataremos el desbalanceo de clases o sesgos en los datos.

<https://www.datacamp.com/tutorial/k-nearest-neighbors-knn-classification-with-r-tutorial>

<https://www.datacamp.com/tutorial/preprocessing-in-data-science-part-1-centering-scaling-and-knn>

```{r}
credit.Datos.knn.Train <- credit.Datos.Train
credit.Datos.knn.Test <- credit.Datos.Test
```

### Configuración

Para la configuración de knn vamos a usar la misma que svm

```{r}
configuracion_knn <- list(
  conf1  = list("dummy"),
  conf2  = list("norm_center_scale", "dummy"),
  conf3  = list("norm_range", "dummy"),
  conf4  = list("dummy", "pca"),
  conf5  = list("dummy", "ica"),
  conf6  = list("norm_center_scale", "dummy", "pca"),
  conf7  = list("norm_center_scale", "dummy", "ica"),
  conf8  = list("norm_range", "dummy", "var_cero", "pca"),
  conf9  = list("norm_range", "dummy", "var_cero", "ica"),
  conf10 = list("norm_center_scale", "dummy", "var_cero"),
  conf11 = list("norm_range", "dummy", "var_cero"),
  conf12 = list("dummy", "var_cero", "pca"),
  conf13 = list("dummy", "var_cero", "ica")
)
```

### Almacenar modelos entrenados

```{r}
modelos_entrenados_knn <- list()
```

### Ejecutamos todas las configuraciones

```{r}
for (i in seq_along(configuracion_knn)) {
  # Crear nombre de configuración
  nombre_config <- names(configuracion_knn)[i]
  preprocesamientos <- configuracion_knn[[i]]
  
  # Restaurar bases de datos originales
  credit.Datos.knn.Train <- credit.Datos.Train
  credit.Datos.knn.Test <- credit.Datos.Test
  
  # Aplicar configuración
  ejecutar_configuracion_svm(preprocesamientos, "credit.Datos.knn.Train", "credit.Datos.knn.Test")

  
  # Definir variables de entrada y salida
  credit.Var.Salida.Usada <- "V16"
  credit.Vars.Entrada.Usadas <- setdiff(names(credit.Datos.knn.Train), credit.Var.Salida.Usada)
  
  # Entrenar el modelo
  modelo_entrenado <- train(
    x = credit.Datos.knn.Train[, credit.Vars.Entrada.Usadas],
    y = credit.Datos.knn.Train[[credit.Var.Salida.Usada]],
    method = "knn",
    trControl = credit.trainCtrl,
    tuneGrid = data.frame(
      k = 13       
    ),
    metric = "Accuracy"
  )
  
  # Guardar modelo
  modelos_entrenados_knn[[nombre_config]] <- modelo_entrenado
  
  # Mostrar información del modelo actual
  print(paste("Modelo entrenado para la configuración:", nombre_config))
}
```

### Interpretamos los resultados obtenidos

```{r}
# Comparar modelos usando resamples()
resultados_resampling <- resamples(modelos_entrenados_knn)

# Mostrar estadísticas resumidas
summary(resultados_resampling)

# Graficar resultados
bwplot(resultados_resampling, metric = "Accuracy")
dotplot(resultados_resampling, metric = "Accuracy")

# Prueba estadística de diferencias
diferencias_modelos <- diff(resultados_resampling)
summary(diferencias_modelos)
```

Aunque no podemos asegurar caul es la mejor ya que los intervalos coincide, nos vamos a quedar con la tercera configuración.

```{r}
credit.Datos.knn.Train <- credit.Datos.Train
credit.Datos.knn.Test <- credit.Datos.Test

norm_range("credit.Datos.knn.Train", "credit.Datos.knn.Test")
dummy("credit.Datos.knn.Train", "credit.Datos.knn.Test")
eliminar_varianza_cero_svm("credit.Datos.knn.Train", "credit.Datos.knn.Test")

```

### Encontrar los mejores hiper-parámetros y ajustar modelo final

```{r}
# Configurar el modelo KNN
set.seed(1234)
# Entrenamiento del modelo k-NN
credit.modelo.knn <- train(
  V16 ~ .,  
  data = credit.Datos.knn.Train,  
  method = "knn",  # Especificamos k-NN
  trControl = credit.trainCtrl,   
  tuneLength = 20)

#Mostrar porcentaje final
# Realizar predicciones en el conjunto de prueba
predicciones_test <- predict(credit.modelo.knn, credit.Datos.knn.Test)

# Generar la matriz de confusión
matriz_confusion <- confusionMatrix(predicciones_test, credit.Datos.knn.Test[[credit.Var.Salida.Usada]])

# Extraer el porcentaje de acierto
porcentaje_acierto <- matriz_confusion$overall["Accuracy"] * 100

# Mostrar el porcentaje de acierto
print(paste("Porcentaje de acierto:", round(porcentaje_acierto, 2), "%"))
```

### Entrenamos con hiper-parámetros fijos

Tras probar 20 combinaciones de hiper-parametros vamos a cuales han sido los que finalmente se han elegido para el modelo final.

```{r}
modelLookup(("knn"))
```

El parámetro k en K-Nearest Neighbors (KNN) representa el número de vecinos más cercanos que se consideran para hacer una predicción.

-   Valor pequeño de k: El modelo es más sensible al ruido y puede sobreajustarse, ya que se basa solo en los pocos vecinos más cercanos.

-   Valor grande de k: El modelo es más general y robusto, ya que considera una mayor cantidad de vecinos, pero puede subajustarse si se elige un valor demasiado grande.

```{r}
credit.modelo.knn$bestTune
```

En nuestro caso k=7 proporcionara un equilibrio entre la sensibilidad al ruido y la generalidad.

Vamos a entrenar un modelo finjando el número de vecinos mas cercano.

```{r}
set.seed(2)
credit.modelo.final.knn <- train(
  V16 ~ .,
  data = credit.Datos.knn.Train,
  method = "knn",
  trControl =  credit.trainCtrl.none,
  tuneGrid = data.frame(
    k = 7         
  ),
  metric = "Accuracy"          # Métrica de evaluación
)

# Evaluación en el conjunto de prueba
predictions <- predict(credit.modelo.final.knn, credit.Datos.knn.Test)

# Calcular matriz de confusión y accuracy
conf_mat <- confusionMatrix(predictions, credit.Datos.knn.Test[[credit.Var.Salida.Usada]])
accuracy <- conf_mat$overall["Accuracy"]

# Mostrar resultados
print(credit.modelo.final.knn)
print(paste("Porcentaje de Acierto (Modelo Final):", round(accuracy * 100, 2), "%"))
```

### Submuestreo con clases desvalanceadas

Para ver si nuestro modelo mejora vamos a tratar las clases desvalanceadas en el conjunto de los datos de entrenamiento.

```{r}
credit.Var.Salida.Usada <- c("V16")
credit.Vars.Entrada.Usadas <- setdiff(names(credit.Datos.knn.Train),credit.Var.Salida.Usada)
```

#### down-sampling:

Consiste en reducir el número de ejemplos de las clases más frecuentes para igualar la clase menos frecuente.

```{r}
set.seed(12345)
credit.Datos.Train.knn.downsmpld<-downSample(x=credit.Datos.knn.Train[credit.Vars.Entrada.Usadas],
                                       y=credit.Datos.knn.Train[[credit.Var.Salida.Usada]],
                                       yname=credit.Var.Salida.Usada)
credit.Datos.Test.knn.downsmpld <- credit.Datos.knn.Test
```

#### up-sampling:

En este caso hace lo contrario, remuestrea (con reemplazamiento) la clase minoritaria para igualarla a la mayoritaria (repite varios datos).

```{r}
set.seed(1234)
credit.Datos.Train.knn.upsmpld<-upSample(x=credit.Datos.knn.Train[credit.Vars.Entrada.Usadas],
                                   y=credit.Datos.knn.Train[[credit.Var.Salida.Usada]],
                                   yname=credit.Var.Salida.Usada)
credit.Datos.Test.knn.upsmpld<- credit.Datos.knn.Test
```

Creamos y probamos un modelo para cada una de estas configuraciones:

```{r}
set.seed(1234)
credit.modelo.knn.downsmpld <- train(
  V16 ~ .,  # Variable de salida ~ Variables de entrada
  data = credit.Datos.Train.knn.downsmpld,                                    # Datos de entrenamiento
  method = "knn",
  trControl =  credit.trainCtrl.none,
  tuneGrid = data.frame(
    k = 7         
  ),
  metric = "Accuracy" 
)
#Mostrar porcentaje final
# Realizar predicciones en el conjunto de prueba
predicciones_test <- predict(credit.modelo.knn.downsmpld, credit.Datos.Test.knn.downsmpld)

# Generar la matriz de confusión
matriz_confusion <- confusionMatrix(predicciones_test, credit.Datos.Test.knn.downsmpld[[credit.Var.Salida.Usada]])

# Extraer el porcentaje de acierto
porcentaje_acierto <- matriz_confusion$overall["Accuracy"] * 100

# Mostrar el porcentaje de acierto
print(paste("Porcentaje de acierto:", round(porcentaje_acierto, 2), "%"))
```

```{r}
set.seed(1234)
credit.modelo.knn.upsmpld <- train(
  V16 ~ .,  # Variable de salida ~ Variables de entrada
  data = credit.Datos.Train.knn.upsmpld,                                    # Datos de entrenamiento
  method = "knn",
  trControl =  credit.trainCtrl.none,
  tuneGrid = data.frame(
    k = 7         
  ),
  metric = "Accuracy" 
)
#Mostrar porcentaje final
# Realizar predicciones en el conjunto de prueba
predicciones_test <- predict(credit.modelo.knn.upsmpld, credit.Datos.Test.knn.upsmpld)

# Generar la matriz de confusión
matriz_confusion <- confusionMatrix(predicciones_test, credit.Datos.Test.knn.upsmpld[[credit.Var.Salida.Usada]])

# Extraer el porcentaje de acierto
porcentaje_acierto <- matriz_confusion$overall["Accuracy"] * 100

# Mostrar el porcentaje de acierto
print(paste("Porcentaje de acierto:", round(porcentaje_acierto, 2), "%"))
```

### Eliminación de variables

```{r}
#Ver importancia de las variables de entrada
#Metodo -> varImp()
varImp(credit.modelo.final.knn)
#Ver seleccion de variables del modelo 
#Metodo -> predictors(modelo$finalModel)
predictors(credit.modelo.final.knn)
```

En este caso, no hay variables que no se empleen o que tengan poca importancia.

### Conclusión

Para el modelo de Knn, el mejor resultado lo obtenemos tras una normalización y la transformación de las variables categóricas y la eliminación de la variables con varianza 0. El modelo obtendría un porcentaje de acierto de 87.59 %.

```{r}
print(credit.modelo.knn)
```

## **Random Forest**

El Random Forest es un algoritmo de aprendizaje automático basado en un conjunto de árboles de decisión. Es especialmente util para tareas de clasificacion por lo tanto es un buen candidato para usarlo en esta tarea.

Para empezar vamos a ver que hiperparametros nos ofrece caret del random forest..

```{r}
modelLookup(("rf"))
```

En este caso solo disponemos de un hiperparámetro gestionado por caret, esto significa que en tuneGrid no podemos sponer mas hiperparametros de random forest porque caret no lo sabe gestionar por lo que tendremos que introducirlos manualmente en el train(). Dentro de los demas hiperparamentros existentes vamos a probar tambien ntree que es el numero de arboles de decision, para ello crearemos un bucle y entrenaremos modelos segun para cada numero de arboles.

### Preprocesado de datos

En cuanto al preprocesado de datos para random forest no es muy exigente. RF no se ve afectado por la escala de las variables porque utiliza arboles de decision por lo tanto obviamos la normalizacion. RF tambien maneja bastante bien las variables altamente correlacionadas.

Lo que no sabe gestioniar RF son los valores NA pero como nuestras bases de entrenamiento han sido tratadas previamente en el preprocesado de datos para valores nulos no haria falta hacer mas.

```{r}
varObjetivo <- "V16"
varsPredictoras <- setdiff(names(credit.Datos.Train.imp), varObjetivo)
```

### Entrenamiento

Vamos usar trainControl() para configurar procesos del entrenamiento donde vamos a poner una configuracion habitual recomendada por el caret la cual consiste en cross validacion cruzada de 10 plieges y 3 repeticiones. Ademas vamos a usar twoClassSummary junto con ROC ya que proporciona métricas robustas que evalúan el desempeño del modelo considerando todos los posibles umbrales de decisión. ROC es especialmente útil para comparar modelos, ya que mide la capacidad de separar correctamente las clases positivas y negativas, independientemente del balanceo entre ellas.

```{r}
rf.trainControl.3cv10.twoSummary.roc <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 3,
  # No mostramos el proceso porque seria muy extenso
  verboseIter = FALSE,
  # Métricas adicionales como ROC (para clasificación binaria)
  summaryFunction = twoClassSummary,
  # Necesario si queremos usar roc
  classProbs = TRUE,
  # Guardamos todo para hacer diagramas
  returnResamp = "all"
)
```

Creamos el grid para el hiperparametros mtry.

```{r}
rf.grid <- expand.grid(mtry = c(1, 2, 4, 6, 10))
```

Ahora procederemos a entrenar modelos de random forest, vamos a distinguir dos subsecciones de entrenamiento. El entrenamiento con los valores NA imputados y el entrenamiento con los valores NA eliminados.

Ahora procedemos a entrenar nuestro primer modelo de random forest. Vamos a realizar tres entrenamientos cada uno con un número de arboles distinto, ya que como hemos mencionado anteriormente al ser el numero de arboles un hiperparametros que no gestiona caret hay que ponerlo directamente en el train().

```{r}

# Entrenamiento para 100 arboles
set.seed(123)
rf.modelo.3cv10.twoSummary.roc.100 <- train(
  x = credit.Datos.Train.imp[varsPredictoras],
  y = credit.Datos.Train.imp[[varObjetivo]],
  method = "rf", 
  metric = "ROC",
  trControl = rf.trainControl.3cv10.twoSummary.roc,
  tuneGrid = rf.grid,
  ntree = 100
)

# Entrenamiento para 500 arboles
set.seed(123)
rf.modelo.3cv10.twoSummary.roc.500 <- train(
  x = credit.Datos.Train.imp[varsPredictoras],
  y = credit.Datos.Train.imp[[varObjetivo]],
  method = "rf", 
  metric = "ROC",
  trControl = rf.trainControl.3cv10.twoSummary.roc,
  tuneGrid = rf.grid,
  ntree = 500
)

# Entrenamiento para 1000 arboles
set.seed(123)
rf.modelo.3cv10.twoSummary.roc.1000 <- train(
  x = credit.Datos.Train.imp[varsPredictoras],
  y = credit.Datos.Train.imp[[varObjetivo]],
  method = "rf", 
  metric = "ROC",
  trControl = rf.trainControl.3cv10.twoSummary.roc,
  tuneGrid = rf.grid,
  ntree = 1000
)
```

Vamos a ver con el siguiente código cual es el maximo ROC de cada uno de los modelos y para que hiperparametro, recordemos que cuanto más cercanos sea el ROC a uno mejores predicciones tendrá el modelo.

```{r}
maxIndexRf100 <- which.max(rf.modelo.3cv10.twoSummary.roc.100$results$ROC)

# Obtener los valores correspondientes
maxRocRf100 <- rf.modelo.3cv10.twoSummary.roc.100$results$ROC[maxIndexRf100]

mtry100 <- rf.modelo.3cv10.twoSummary.roc.100$results$mtry[maxIndexRf100]

# Imprimir los resultados
cat("El valor más alto de ROC para 100 arboles es:", maxRocRf100, "\n")
cat("Corresponde a mtry:", mtry100, "\n")


maxIndexRf500 <- which.max(rf.modelo.3cv10.twoSummary.roc.500$results$ROC)

# Obtener los valores correspondientes
maxRocRf500 <- rf.modelo.3cv10.twoSummary.roc.500$results$ROC[maxIndexRf500]

mtry500 <- rf.modelo.3cv10.twoSummary.roc.500$results$mtry[maxIndexRf500]

# Imprimir los resultados
cat("El valor más alto de ROC para 500 arboles es:", maxRocRf500, "\n")
cat("Corresponde a mtry:", mtry500, "\n")


maxIndexRf1000 <- which.max(rf.modelo.3cv10.twoSummary.roc.1000$results$ROC)

# Obtener los valores correspondientes
maxRocRf1000 <- rf.modelo.3cv10.twoSummary.roc.1000$results$ROC[maxIndexRf1000]

mtry1000 <- rf.modelo.3cv10.twoSummary.roc.1000$results$mtry[maxIndexRf1000]

# Imprimir los resultados
cat("El valor más alto de ROC para 1000 arboles es:", maxRocRf1000, "\n")
cat("Corresponde a mtry:", mtry1000, "\n")


```

Vemos que probablemente el modelo se comporta mejor con valores de entre 2 y 4 de mtry.

Ahora veamos una comparativa segun el nummero de arboles.

```{r}
# Crear un data.frame con los resultados
resultados_df <- data.frame(
  mtry = rep(c(1, 2, 4, 6, 10), times = 3), # Repetimos 5 valores de mtry, una vez por modelo
  ntree = rep(c(100, 500, 1000), each = 5), # 3 modelos, cada uno con 5 valores de mtry
  ROC = c(
    rf.modelo.3cv10.twoSummary.roc.100$results$ROC[1:5], 
    rf.modelo.3cv10.twoSummary.roc.500$results$ROC[1:5], 
    rf.modelo.3cv10.twoSummary.roc.1000$results$ROC[1:5]
  )   
)

# Crear el histograma
ggplot(resultados_df, aes(x = as.factor(mtry), y = ROC, fill = as.factor(ntree))) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(
    title = "ROC según Número de mtry y Número de Árboles (imputados NA)",
    x = "Número de mtry",
    y = "ROC",
    fill = "Número de Árboles (ntree)"
  ) +
  scale_fill_manual(values = c("100" = "green", "500" = "purple", "1000" = "red")) +
  theme_minimal() +
  theme(legend.position = "bottom") +
  coord_cartesian(ylim = c(0.9, 1))
```

Se puede ver que la diferencia entre 500 y 1000 arboles es muy pequeña por lo que poner 1000 arboles no merece la pena ya que conlleva mucho tiempo de ejecución.

### Test de modelos

Probamos los distintos modelos que hemos entrenado para ver cual tiene mejor prediccion, esto lo hacemos sobre nuestro conjunto test.

```{r}

# Entrenamiento con valores NA imputados
prediccion.rf.modelo.3cv10.twoSummary.roc.100 <- predict(rf.modelo.3cv10.twoSummary.roc.100, newdata = credit.Datos.Test.imp[varsPredictoras])

prediccion.rf.modelo.3cv10.twoSummary.roc.500 <- predict(rf.modelo.3cv10.twoSummary.roc.500, newdata = credit.Datos.Test.imp[varsPredictoras])

prediccion.rf.modelo.3cv10.twoSummary.roc.1000 <- predict(rf.modelo.3cv10.twoSummary.roc.1000, newdata = credit.Datos.Test.imp[varsPredictoras])

```

### Comparativa de modelos

Veamos los resultados

```{r}
# Generar matriz de confusión
matrizConfusion.rf.modelo.3cv10.twoSummary.roc.100 <- confusionMatrix(
  data = prediccion.rf.modelo.3cv10.twoSummary.roc.100,
  reference = credit.Datos.Test[[varObjetivo]],
  positive = "aprobada" # Cambia esto por la clase objetivo positiva
)

matrizConfusion.rf.modelo.3cv10.twoSummary.roc.500 <- confusionMatrix(
  data = prediccion.rf.modelo.3cv10.twoSummary.roc.500,
  reference = credit.Datos.Test[[varObjetivo]],
  positive = "aprobada" # Cambia esto por la clase objetivo positiva
)

matrizConfusion.rf.modelo.3cv10.twoSummary.roc.1000 <- confusionMatrix(
  data = prediccion.rf.modelo.3cv10.twoSummary.roc.1000,
  reference = credit.Datos.Test[[varObjetivo]],
  positive = "aprobada" # Cambia esto por la clase objetivo positiva
)


# Extraer Accuracy de las matrices de confusión
accuracy_imputados <- c(
  matrizConfusion.rf.modelo.3cv10.twoSummary.roc.100$overall["Accuracy"] * 100,
  matrizConfusion.rf.modelo.3cv10.twoSummary.roc.500$overall["Accuracy"] * 100,
  matrizConfusion.rf.modelo.3cv10.twoSummary.roc.1000$overall["Accuracy"] * 100
)

# Crear un data frame con los resultados
resultados <- data.frame(
  Modelo = c("100 Árboles", "500 Árboles", "1000 Árboles"),
  Accuracy = accuracy_imputados
)

# Cargar ggplot2
library(ggplot2)

# Crear el histograma
ggplot(resultados, aes(x = Modelo, y = Accuracy, fill = Modelo)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_text(aes(label = round(Accuracy, 2)), vjust = -0.5) + # Agregar etiquetas con valores
  labs(
    title = "Accuracy por Número de Árboles",
    x = "Modelo",
    y = "Accuracy (%)"
  ) +
  scale_fill_manual(values = c("100 Árboles" = "blue", "500 Árboles" = "green", "1000 Árboles" = "orange")) +
  theme_minimal() +
  coord_cartesian(ylim = c(80, 100)) +
  theme(legend.position = "none")


```

Vemos que con el numero de arboles de 100 es como mejor resultados esta dando.

### Reentrenamiento y testeo

Vamoa ahora a reentrenar los modelos con los mejores hiperparametros para evitar el peaking.

```{r}


rf.grid.final.2 <- expand.grid(mtry = c(2))
rf.grid.final.4 <- expand.grid(mtry = c(4))


rf.modelo.3cv10.twoSummary.roc.100.final <- train(
  x = credit.Datos.Train.imp[varsPredictoras],
  y = credit.Datos.Train.imp[[varObjetivo]],
  method = "rf", 
  metric = "ROC",
  trControl = rf.trainControl.3cv10.twoSummary.roc,
  tuneGrid = rf.grid.final.2,
  ntree = 100
)

rf.modelo.3cv10.twoSummary.roc.500.final <- train(
  x = credit.Datos.Train.imp[varsPredictoras],
  y = credit.Datos.Train.imp[[varObjetivo]],
  method = "rf", 
  metric = "ROC",
  trControl = rf.trainControl.3cv10.twoSummary.roc,
  tuneGrid = rf.grid.final.4,
  ntree = 500
)

```

Vamos a probar si ha dado mejores predicciones.

```{r}

prediccion.rf.modelo.3cv10.twoSummary.roc.100.final <- predict(rf.modelo.3cv10.twoSummary.roc.100.final, newdata = credit.Datos.Test.imp[varsPredictoras])

prediccion.rf.modelo.3cv10.twoSummary.roc.500.final <- predict(rf.modelo.3cv10.twoSummary.roc.500.final, newdata = credit.Datos.Test.imp[varsPredictoras])

```

Vamos a comparar resultados.

```{r}
matrizConfusion.rf.modelo.3cv10.twoSummary.roc.100.final <- confusionMatrix(
  data = prediccion.rf.modelo.3cv10.twoSummary.roc.100.final,
  reference = credit.Datos.Test[[varObjetivo]],
  positive = "aprobada" # Cambia esto por la clase objetivo positiva
)

matrizConfusion.rf.modelo.3cv10.twoSummary.roc.500.final <- confusionMatrix(
  data = prediccion.rf.modelo.3cv10.twoSummary.roc.500.final,
  reference = credit.Datos.Test[[varObjetivo]],
  positive = "aprobada" # Cambia esto por la clase objetivo positiva
)

# Extraer Accuracy de las matrices de confusión
accuracy_values <- c(
  matrizConfusion.rf.modelo.3cv10.twoSummary.roc.100$overall["Accuracy"] * 100,
  matrizConfusion.rf.modelo.3cv10.twoSummary.roc.100.final$overall["Accuracy"] * 100,
  matrizConfusion.rf.modelo.3cv10.twoSummary.roc.500$overall["Accuracy"] * 100,
  matrizConfusion.rf.modelo.3cv10.twoSummary.roc.500.final$overall["Accuracy"] * 100
)

# Crear un data frame con los resultados
resultados <- data.frame(
  Modelo = c("100 Árboles", "100 Árboles Final", "500 Árboles", "500 Árboles Final"),
  Accuracy = accuracy_values
)

# Cargar ggplot2
library(ggplot2)

# Crear el histograma
ggplot(resultados, aes(x = Modelo, y = Accuracy, fill = Modelo)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_text(aes(label = round(Accuracy, 2)), vjust = -0.5) + # Agregar etiquetas con valores
  labs(
    title = "Accuracy de Modelos Random Forest",
    x = "Modelo",
    y = "Accuracy (%)"
  ) +
  scale_fill_manual(values = c("100 Árboles" = "blue", "100 Árboles Final" = "lightblue", 
                               "500 Árboles" = "green", "500 Árboles Final" = "lightgreen")) +
  theme_minimal() +
  coord_cartesian(ylim = c(80, 90)) +
  theme(legend.position = "none") # Ocultar leyenda, ya que el eje X describe los modelos

```
Vemos que el reentrenamiento con hiperparametros fijos solo ha resultado efecto en el modelo de 100 arboles.

### Postprocesado de datos

Antes de elegir definitivamente el modelo vamos a hacer un postprocesado de los datos y ver que variables se usan mas.

```{r}
varImp(rf.modelo.3cv10.twoSummary.roc.100)
varImp(rf.modelo.3cv10.twoSummary.roc.100.final)

varImp(rf.modelo.3cv10.twoSummary.roc.500)
varImp(rf.modelo.3cv10.twoSummary.roc.500.final)
```

Vemos que la V7 es la que menos impacto tiene pero aun asi tiene el suficiente impacto como para no eliminarla ya que por ejemplo de la variable empleado distya de 4 puntos y de la edad de 9 puntos por lo que no me parece una decision correcta eliminarla y entrenar de nuevo el modelo.

### Conclusión

Tenemos que nuestro modelo final ha dado un 86,86% de acierto y ademas de ser un porcentaje bastante alto, random forest no ha requerido un preprocesado tan extenso como en los modelos anteriores por lo que es una opcion bastante interesante.

Nuestro modelo final seria:
```{r}
print(rf.modelo.3cv10.twoSummary.roc.100.final)
```

### Bibliografía Random forest

Aprendizaje Computacional. (s.f.). Análisis de datos y machine learning con R (caret). Versión 0.105 (pre-release)

Fernández, A. (s.f.). Machine Learning en R con caret. Recuperado de https://anderfernandez.com/blog/machine-learning-caret-r/

Prabhakaran, S. (2018). Caret Package – A Practical Guide to Machine Learning in R. Recuperado de https://www.machinelearningplus.com/machine-learning/caret-package/

Barter, R. (2017). A basic tutorial of caret: the machine learning package in R. Recuperado de https://rebeccabarter.com/blog/2017-11-17-caret_tutorial

Guru99. (2023). R Random Forest Tutorial with Example. Recuperado de https://www.guru99.com/r-random-forest-tutorial.html

Escuela de Datos Vivos. (2019). Tutorial de creación de un random forest con caret en R [Video]. YouTube. Recuperado de https://www.youtube.com/watch?v=XnZyi5H8gU8


## **Redes neuronales**

### Preprocesado

Vamos a ver a continuación que preprocesados son necesarios para el modelo de redes neuronales.

```{r}
credit.Datos.nnet.Train <- credit.Datos.Train
credit.Datos.nnet.Test <- credit.Datos.Test
```


#### Imputación de valores NA

Nnet no puede maneja valores NA por lo que imputaremos sus valores.

```{r}
credit.Datos.nnet.Train <- credit.Datos.Train.imp
credit.Datos.nnet.Test <- credit.Datos.Test.imp
```


#### Normalización

nnet es sensible a las magnitudes de las variables. Si los predictores tienen diferentes escalas (por ejemplo, ingresos en miles frente a edades en decenas), esto puede afectar negativamente al entrenamiento.

```{r}
ejecutar_configuracion("norm_range", "credit.Datos.nnet.Train", "credit.Datos.nnet.Test")
ejecutar_configuracion("norm_center_scale", "credit.Datos.nnet.Train", "credit.Datos.nnet.Test")
```


#### Dummy

nnet requiere que las variables categóricas se representen como variables dummy (una columna por cada nivel de la variable).

```{r}
ejecutar_configuracion("dummy", "credit.Datos.nnet.Train", "credit.Datos.nnet.Test")
```


#### Eliminación de variables con variancia cercana a cero.

Variables altamente correlacionadas o con baja varianza pueden aumentar el tiempo de entrenamiento y provocar sobreajuste.

```{r}
ejecutar_configuracion("var_cero", "credit.Datos.nnet.Train", "credit.Datos.nnet.Test")
```

### Entrenamiento

Pasamos ahora a entrenar el modelo de redes neuronales, primero vemos los posibles hiperparametros disponbles que tiene:

```{r}
modelLookup(("nnet"))
```

Caret no sofrece dos hiperparametros facilmente accesibles que son el size referente al numero de nodos en la capa oculta y decay que es el parametro de regularización por lo que creamos un grid para ellos.

```{r}
nnet.Grid <- expand.grid(
  size = c(1, 2, 3, 4, 7, 10),
  decay = c(0.01, 0.1, 0.5, 0.6, 0.7, 1, 2)
)
```

Para el train control usamos la misma configuaración que en random forest

```{r}
nnet.trainControl <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 3,
  verboseIter = FALSE,
  classProbs = TRUE,
  summaryFunction = twoClassSummary,
  returnResamp = "all"
)
```


Procedemos ahora a entrenar modelos, y como sucedica con el random forest, vamos a entrenar tres modelos correspondiente a distintos valores del hiperparamtro maxit (numero de iteraciones) que no esta manejado por caret y hay que introducirlo directamente en el train().

```{r}
varObjetivo <- "V16"
varsPredictoras <- setdiff(names(credit.Datos.nnet.Train), varObjetivo)
```


```{r}
set.seed(123)
nnet.train.100 <- train(
  x = credit.Datos.nnet.Train[varsPredictoras],
  y = credit.Datos.nnet.Train[[varObjetivo]],
  method = "nnet",
  metric = "ROC",
  trControl = nnet.trainControl,
  tuneGrid = nnet.Grid,
  trace = FALSE,
  maxit = 100
)

set.seed(123)
nnet.train.300 <- train(
  x = credit.Datos.nnet.Train[varsPredictoras],
  y = credit.Datos.nnet.Train[[varObjetivo]],
  method = "nnet",
  metric = "ROC",
  trControl = nnet.trainControl,
  tuneGrid = nnet.Grid,
  trace = FALSE,
  maxit = 300
)

set.seed(123)
nnet.train.1000 <- train(
  x = credit.Datos.nnet.Train[varsPredictoras],
  y = credit.Datos.nnet.Train[[varObjetivo]],
  method = "nnet",
  metric = "ROC",
  trControl = nnet.trainControl,
  tuneGrid = nnet.Grid,
  trace = FALSE,
  maxit = 1000
)


```

```{r}
# Encontrar el índice del máximo valor de ROC
max_index100 <- which.max(nnet.train.100$results$ROC)

# Obtener los valores correspondientes
max_roc100 <- nnet.train.100$results$ROC[max_index100]
corresponding_size100 <- nnet.train.100$results$size[max_index100]
corresponding_decay100 <- nnet.train.100$results$decay[max_index100]

# Imprimir los resultados
cat("El valor más alto de ROC para 100 iteraciones es:", max_roc100, "\n")
cat("Corresponde a size:", corresponding_size100, "y decay:", corresponding_decay100, "\n")

# Encontrar el índice del máximo valor de ROC
max_index300 <- which.max(nnet.train.300$results$ROC)

# Obtener los valores correspondientes
max_roc300 <- nnet.train.300$results$ROC[max_index300]
corresponding_size300 <- nnet.train.300$results$size[max_index300]
corresponding_decay300 <- nnet.train.300$results$decay[max_index300]

# Imprimir los resultados
cat("El valor más alto de ROC para 300 iteraciones es:", max_roc300, "\n")
cat("Corresponde a size:", corresponding_size300, "y decay:", corresponding_decay300, "\n")

# Encontrar el índice del máximo valor de ROC
max_index1000 <- which.max(nnet.train.1000$results$ROC)

# Obtener los valores correspondientes
max_roc1000 <- nnet.train.1000$results$ROC[max_index1000]
corresponding_size1000 <- nnet.train.1000$results$size[max_index1000]
corresponding_decay1000 <- nnet.train.1000$results$decay[max_index1000]

# Imprimir los resultados
cat("El valor más alto de ROC para 1000 iteraciones es:", max_roc1000, "\n")
cat("Corresponde a size:", corresponding_size1000, "y decay:", corresponding_decay1000, "\n")

```
omo vemos el ROC varia muy levemente por lo que la mejor opción aparentemente es la de 100 itereaciónes ya que requerirá menos tiempo de computo con valores de size de 4 y un valor de dacay de 0.5. Vamos a verificar esto con las predicciones en el conjunto test.

### Test de modelos

Probamos los distintos modelos que hemos entrenado para ver cual tiene mejor prediccion

```{r}
prediccion.nnet.100 <- predict(nnet.train.100, newdata = credit.Datos.nnet.Test[varsPredictoras])

prediccion.nnet.300 <- predict(nnet.train.300, newdata = credit.Datos.nnet.Test[varsPredictoras])

prediccion.nnet.1000 <- predict(nnet.train.1000, newdata = credit.Datos.nnet.Test[varsPredictoras])
```

### Comparativa de modelos

Una vez entrenados los modelos y probados sobre nuestro conjunto test vamos a ver los resultados. Para ello creamos primero las matrices de confusión.

```{r}
# Generar matriz de confusión
matrizConfusion.nnet.100 <- confusionMatrix(
  data = prediccion.nnet.100,
  reference = credit.Datos.Test[[varObjetivo]],
  positive = "aprobada" # Cambia esto por la clase objetivo positiva
)

matrizConfusion.nnet.300 <- confusionMatrix(
  data = prediccion.nnet.300,
  reference = credit.Datos.Test[[varObjetivo]],
  positive = "aprobada" # Cambia esto por la clase objetivo positiva
)

matrizConfusion.nnet.1000 <- confusionMatrix(
  data = prediccion.nnet.1000,
  reference = credit.Datos.Test[[varObjetivo]],
  positive = "aprobada" # Cambia esto por la clase objetivo positiva
)

```

Ahora creamos un histograma para ver que modelo arroja mejores resultados.

```{r}

# Extraer Accuracy de las matrices de confusión
accuracy_values <- c(
  matrizConfusion.nnet.100$overall["Accuracy"] * 100,
  matrizConfusion.nnet.300$overall["Accuracy"] * 100,
  matrizConfusion.nnet.1000$overall["Accuracy"] * 100
)

# Crear un data frame con los resultados
resultados <- data.frame(
  Modelo = c("100 iteraciones", "300 iteraciones", "1000 iteraciones"),
  Accuracy = accuracy_values
)


# Crear el histograma
ggplot(resultados, aes(x = Modelo, y = Accuracy, fill = Modelo)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_text(aes(label = round(Accuracy, 2)), vjust = -0.5) + # Agregar etiquetas con valores
  labs(
    title = "Accuracy de Modelos con Diferentes Iteraciones",
    x = "Modelo",
    y = "Accuracy (%)"
  ) +
  scale_fill_manual(values = c("100 iteraciones" = "blue", "300 iteraciones" = "green", "500 iteraciones" = "orange")) +
  theme_minimal() +
  theme(legend.position = "none") +
  coord_cartesian(ylim = c(70, 100))

```

Por lo tanto concluimos que el que mejor resultados da es con 100 iteraciones.

### Reentrenamiento y testeo

Para evitar picking vamos a reentrenar nuestro mejor modelo anterior con hiperparametros fijos para tener un valor mas real de las predicciones, por lo que entrenaremos el modelo de 100 iteraciones con un valor de size de 4 y de decay de 0.5.

```{r}
nnet.Grid.final <- expand.grid(
  size = c(4),
  decay = c(0.5)
)

set.seed(123)
nnet.train.100.final <- train(
  x = credit.Datos.nnet.Train[varsPredictoras],
  y = credit.Datos.nnet.Train[[varObjetivo]],
  method = "nnet",
  metric = "ROC",
  trControl = nnet.trainControl,
  tuneGrid = nnet.Grid.final,
  trace = FALSE,
  maxit = 100
)

```

Vamos con la predicción en nuestro conjunto test.

```{r}
prediccion.nnet.100.final <- predict(nnet.train.100.final, newdata = credit.Datos.nnet.Test[varsPredictoras])

matrizConfusion.nnet.100.final <- confusionMatrix(
  data = prediccion.nnet.100.final,
  reference = credit.Datos.Test[[varObjetivo]],
  positive = "aprobada" # Cambia esto por la clase objetivo positiva
)

```

Comparamos ahora los resultados.

```{r}
# Extraer Accuracy de las matrices de confusión
accuracy_values <- c(
  matrizConfusion.nnet.100$overall["Accuracy"] * 100,
  matrizConfusion.nnet.100.final$overall["Accuracy"] * 100
)

# Crear un data frame con los resultados
resultados <- data.frame(
  Modelo = c("nnet 100 iteraciones", "nnet 100 iteraciones Final"),
  Accuracy = accuracy_values
)

# Crear el histograma
ggplot(resultados, aes(x = Modelo, y = Accuracy, fill = Modelo)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_text(aes(label = round(Accuracy, 2)), vjust = -0.5) + # Agregar etiquetas con valores
  labs(
    title = "Accuracy del Modelo nnet (100 iteraciones)",
    x = "Modelo",
    y = "Accuracy (%)"
  ) +
  scale_fill_manual(values = c("nnet 100 iteraciones" = "blue", "nnet 100 iteraciones Final" = "lightblue")) +
  theme_minimal() +
  coord_cartesian(ylim = c(80, 90)) +
  theme(legend.position = "none") # Ocultar leyenda, ya que el eje X describe los modelos
```
Con los hiperparametros fijos nos da incluso mejores resultados por lo que este será nuestro modelo final.

### Postprocesado de datos

Antes de elegir definitivamente el modelo vamos a hacer un postprocesado de los datos y ver que variables se usan mas.

```{r}
varImp(nnet.train.100.final)
```
Vemos que no hay ninguna variable que se use "poco" por lo qu eno sería necesario hacer un postprocesado de los datos. 

### Conclusión

El modelo final de redes neuronales nos ha dado un porcentaje de acierto 84,67%, esta por debajo de otros modelos pero la ventaja que he detectado en este modelo es que a pesar de tener un preprocesado de datos mas complejo de random forest, funciona muy bien con iteraciones bajas lo que mejora mucho el tiempo de computo.

Nuestro modelo final es por tanto:
```{r}
print(nnet.train.100.final)
```
### Bibliografía Redes Neuronales

Aprendizaje Computacional. (s.f.). Análisis de datos y machine learning con R (caret). Versión 0.105 (pre-release)

Amat Rodrigo, J. (n.d.). Machine Learning con R y caret. Recuperado de https://cienciadedatos.net/documentos/41_machine_learning_con_r_y_caret

Fernández, A. (n.d.). Machine Learning en R con caret. Recuperado de https://anderfernandez.com/blog/machine-learning-caret-r/

Charte, F. (2016). Redes neuronales con R. Recuperado de https://fcharte.com/tutoriales/20160203-R-RedesNeuronales/

Amat Rodrigo, J. (n.d.). Redes neuronales con R. Recuperado de https://cienciadedatos.net/documentos/68-redes-neuronales-r

ISGlobal BRGE. (n.d.). La librería caret. Recuperado de https://isglobal-brge.github.io/curso_machine_learning/caret.html

# Conclusiónes finales

En este trabajo, hemos evaluado múltiples modelos de aprendizaje automático, considerando tanto su rendimiento como los requisitos de preprocesado. Cada modelo ha mostrado fortalezas y debilidades, lo que nos permite reflexionar sobre su idoneidad según las necesidades específicas:

KNN obtuvo el mayor porcentaje de acierto (87.59%) tras aplicar normalización, transformación de variables categóricas y eliminación de variables con varianza nula. Sin embargo, su rendimiento depende de un preprocesado relativamente extenso.

Random Forest destacó por un alto porcentaje de acierto (86.86%) con requisitos mínimos de preprocesado, lo que lo convierte en una opción robusta y eficiente para escenarios donde se busca simplicidad en la preparación de datos.

GBM logró un acierto del 86.13%, empleando configuraciones automáticas de preprocesado y optimización de parámetros. Aunque no fue el modelo con mayor precisión, mostró una gran estabilidad y flexibilidad.

SVM, con un porcentaje de acierto del 85.4%, benefició significativamente de un preprocesado avanzado, incluyendo PCA y eliminación de variables con baja varianza, pero mostró menor rendimiento en comparación con modelos menos exigentes en esta etapa.

Redes Neuronales (nnet), aunque ligeramente por debajo en precisión (84.67%), ofrecieron una ventaja en términos de tiempo de cómputo, funcionando eficazmente con iteraciones bajas pese a requerir un preprocesado más complejo.

Conclusión General: El modelo Random Forest se posiciona como el más equilibrado al ofrecer un alto porcentaje de acierto con requisitos mínimos de preprocesado, siendo ideal para implementaciones rápidas y prácticas. Sin embargo, en situaciones donde se puede invertir más tiempo en preprocesado, KNN sería una excelente alternativa por su rendimiento superior. Esta experiencia resalta la importancia de adaptar los modelos y su configuración a las necesidades y restricciones del problema.
