---
title: "normalizar"
output:
  pdf_document: default
  html_document: default
date: "2024-11-20"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r cars}
#setwd("/home/manu/FIUM/TERCERO/PrimerCuatri/AC/Proyecto/PracticasAC")

if(!require("caret")) {
  install.packages("caret", dependencies = c("Depends", "Suggests"))
  require(caret)
}

# Descargamos la base de datos
url <- "https://archive.ics.uci.edu/static/public/27/credit+approval.zip"
download.file(url, destfile = "credit_approval.zip")


# Descomprimimos la base de datos
unzip("credit_approval.zip")


# Cargamos la base de datos, na.string = "?" quitamos los datos con ese valor y lo sustituye por NA
credit <- read.table("crx.data", header = FALSE, sep = ",", na.strings = "?")


# Cargamos en credit.trainIdx la base de datos descargada del UCI
credit.trainIdx<-readRDS("credit.trainIdx.rds")
credit.Datos.Train<-credit[credit.trainIdx,]
credit.Datos.Test<-credit[-credit.trainIdx,]

# Crear un indicador para cada conjunto
credit.Datos.Train$Origen <- "train"
credit.Datos.Test$Origen <- "test"

# Combinar los conjuntos
combined_credit <- rbind(credit.Datos.Train, credit.Datos.Test)

# Librerias usadas durante la practica
library(caret)
library(randomForest)
library(nnet)
library(pROC)


```

```{r}

# Convertir automáticamente columnas de tipo 'chr' a 'factor'
combined_credit[sapply(combined_credit, is.character)] <- lapply(combined_credit[sapply(combined_credit, is.character)], as.factor)
```

\

```{r}
# Ahora vamos a renombrar algunas columnas para ganar legibilidad
levels(combined_credit$V16) <- c("rechazada", "aprobada")

# Comprobamos que no hay missing-data:
sum(!complete.cases(combined_credit))
# De momento no haremos nada con estos 37 datos perdidos
```

Vamos a ver qué forma tienen nuestros datos:

```{r}
str(combined_credit)
```

## Pre-procesado de datos (I): Tratamiento de outliers y nulos:

Comenzamos renombrando las columnas en base a la información que tenemos:

```{r}
colnames(combined_credit)[colnames(combined_credit) == "V1"] <- "Genero"
colnames(combined_credit)[colnames(combined_credit) == "V2"] <- "Edad"
colnames(combined_credit)[colnames(combined_credit) == "V3"] <- "Deuda"
colnames(combined_credit)[colnames(combined_credit) == "V4"] <- "EstadoCivil"
colnames(combined_credit)[colnames(combined_credit) == "V8"] <- "AnyosContratado"
colnames(combined_credit)[colnames(combined_credit) == "V10"] <- "Empleado"
colnames(combined_credit)[colnames(combined_credit) == "V11"] <- "Solvencia"
colnames(combined_credit)[colnames(combined_credit) == "V13"] <- "composicionPoblacion"
```

[**Tratamiento de valores fuera de rango:**]{.underline}

Para ello, vamos a diseñar una función que detecte outliners. Como hemos aprendido a hacerlo es con la siguiente fórmula: si valor **∈** [Q1 - 1.5 \* RI, Q3 + 1.5 \* RI] se considera outliner. Siendo RI (Rango Intercuatil). Solo miramos el conjunto de Train para que el conjunto de Test no se aproveche de dichos datos (p. independecia de conjuntos).

```{r}
credit.Datos.Train <- combined_credit[combined_credit$Origen == "train", ]
credit.Datos.Test <- combined_credit[combined_credit$Origen == "test", ]

detectar_outliers <- function(x) {
  Q1 <- quantile(x, 0.25, na.rm = TRUE)
  Q3 <- quantile(x, 0.75, na.rm = TRUE)
  RI <- Q3 - Q1
  limite_inferior <- Q1 - 1.5 * RI
  limite_superior <- Q3 + 1.5 * RI
  return(x < limite_inferior | x > limite_superior)
}

credit.Datos.continuas <- credit.Datos.Train[, sapply(credit.Datos.Train, is.numeric)]
outliners <- lapply(credit.Datos.continuas, detectar_outliers)

# Contar los outliers por variable
sumas_outliners <- sapply(outliners, function(x) sum(x, na.rm = TRUE))
sumas_outliners
```

Sin embargo, estos datos no son tan representativos. Necesitamos saber qué porcentaje son atípicos del total de datos.

```{r}
numero_filas <- nrow(na.omit(credit.Datos.Train))

proporcion <- function(x) {
  return(x/numero_filas*100)
}

proporciones <- lapply(sumas_outliners, proporcion)
proporciones
```

Según hemos estado investigando cuando el porcentaje de outliners es superior a 5% hay que tratarlos ya que pueden ser problemáticos. En este caso como mucho tenemos un 15% que es un valor alto, pero tampoco es elevadísimo. Siendo estos: AnyosContratado, Solvencia y V15:

```{r}
boxplot(credit.Datos.Train$AnyosContratado,boxwex=0.15,ylab="AnyosContratado")
rug(jitter(credit.Datos.Train$AnyosContratado),side=2)
abline(h=mean(credit.Datos.Train$AnyosContratado,na.rm=T),lty=2)

boxplot(credit.Datos.Train$Solvencia,boxwex=0.15,ylab="Solvencia")
rug(jitter(credit.Datos.Train$Solvencia),side=2)
abline(h=mean(credit.Datos.Train$Solvencia,na.rm=T),lty=2)

boxplot(credit.Datos.Train$V15,boxwex=0.15,ylab="V15")
rug(jitter(credit.Datos.Train$V15),side=2)
abline(h=mean(credit.Datos.Train$V15,na.rm=T),lty=2)
```

Decidimos que queremos equiparar a los valores fuera de rango con los valores extremos. Esto se llama winsorización:\

```{r}
# Función para calcular límites de winsorización (IR)
calcular_limites <- function(x) {
  Q1 <- quantile(x, 0.25, na.rm = TRUE)
  Q3 <- quantile(x, 0.75, na.rm = TRUE)
  RI <- Q3 - Q1
  limite_inferior <- Q1 - 1.5 * RI
  limite_superior <- Q3 + 1.5 * RI
  return(c(limite_inferior, limite_superior))
}

# Función para aplicar winsorización con límites definidos
winsorizar_con_limites <- function(x, limites) {
  limite_inferior <- limites[1]
  limite_superior <- limites[2]
  x[x < limite_inferior] <- limite_inferior
  x[x > limite_superior] <- limite_superior
  return(x)
}

columnas_a_winsorizar <- c("AnyosContratado", "Solvencia", "V15")

credit.Datos.Train.wins <- credit.Datos.Train
credit.Datos.Test.wins <- credit.Datos.Test

# Calcular límites en el conjunto de entrenamiento
limites_winsorizacion <- lapply(credit.Datos.Train.wins[, columnas_a_winsorizar], calcular_limites)

# Winsorizar el conjunto de entrenamiento
for (col in columnas_a_winsorizar) {
  credit.Datos.Train.wins[[col]] <- winsorizar_con_limites(credit.Datos.Train.wins[[col]], limites_winsorizacion[[col]])
}

# Aplicar los mismos límites al conjunto de prueba (p. de independencia de conjuntos)
for (col in columnas_a_winsorizar) {
  credit.Datos.Test.wins[[col]] <- winsorizar_con_limites(credit.Datos.Test.wins[[col]], limites_winsorizacion[[col]])
}
```

En el caso de que NO queramos usar la winsorización solo debemos de comentar líneas:

```{r}
credit.Datos.Train <- credit.Datos.Train.wins
credit.Datos.Test <- credit.Datos.Test.wins
```

Comprobamos que ya no hay valores fuera de rango, excepto en "Edad", "Deuda" y "V14" que el porcentaje es muy bajo.

```{r}
credit.Datos.continuas <- credit.Datos.Train[, sapply(credit.Datos.Train, is.numeric)]
outliners <- lapply(credit.Datos.continuas, detectar_outliers)

# Contar los outliers por variable
sumas_outliners <- sapply(outliners, function(x) sum(x, na.rm = TRUE))
sumas_outliners
```

Sin embargo, debemos de analizar si en "Edad", "Deuda" y "V14" hay errores evidentes que puedan perjudicar el rendimiento del modelo:

```{r}
summary(credit.Datos.Train$Edad)
summary(credit.Datos.Train$Deuda)
summary(credit.Datos.Train$V14)
```

En Deuda y V14 no parece que haya valores incorrectos. Sobre todo en Deuda no parece que haya nada raro, en cuanto a V14 no podemos decir mucho ya que no conocemos (especulado) su significado.

Sin embargo en Edad nos podemos dar cuenta que hay valores erroneos, ya que los menores de edad en Estados Unidos no pueden solicitar créditos. Por tanto, podemos sustituir los valores menos a 18, por 18:

```{r}

ajustar_edad <- function(x) {
  x[x < 18] <- 18
  return(x)
}

credit.Datos.Train$Edad <- ajustar_edad(credit.Datos.Train$Edad)
credit.Datos.Test$Edad <- ajustar_edad(credit.Datos.Test$Edad)

summary(credit.Datos.Train$Edad)
summary(credit.Datos.Test$Edad)
```

Como vemos hemos conseguido transformarlo de forma correcta.

[**Tratamiento de valores nulos:**]{.underline}

Primero vamos a ver cuántos valores nulos tiene cada variable. Esto nos indica que variables hay que transformar:

```{r}
combined_credit <- rbind(credit.Datos.Train, credit.Datos.Test)
num_na <- sapply(combined_credit, function(x) sum(is.na(x)))
print(num_na)
```

Primero de todo, vamos a analizar las variables continuas para analizar su distribución y elegir el tipo de sustitución idónea para cada una de ellas. Pero antes debemos de asegurarnos de no violar el principio de independencia de conjuntos, es necesario volver a separar los datos, para tener actualizadas las bases de datos de entrenamiento y validación:

**Breve inciso:** Realmente, no hace falta separar la base de datos, ya que en la base de datos de validación no hay nulos. Sin embargo, hemos considerado que se trata de una buena práctica, para siempre caer en ese detalle.

```{r}
credit.Datos.Train <- combined_credit[combined_credit$Origen == "train", ]
credit.Datos.Test <- combined_credit[combined_credit$Origen == "test", ]

# Verificar dimensiones
dim(credit.Datos.Train)  # Debe coincidir con la tabla original de entrenamiento
dim(credit.Datos.Test)   # Debe coincidir con la tabla original de validación

```

Como vemos las dimensiones son las correctas. De hecho tenemos una columna más, ya que con ella distinguimos si se trata de un conjunto de validación y testing.

Ahora si podemos analizar las variables de credit:\

```{r}
# histograma enriquecido para Edad
hist(credit.Datos.Train$Edad, xlab="",
main="Máximo valor de Edad", ylim=c(0,0.07),probability=T)
lines(density(credit.Datos.Train$Edad,na.rm=T))
rug(jitter(credit.Datos.Train$Edad))
```

La mediana es robusta frente a sesgos y outliers, lo que la hace ideal para distribuciones asimétricas como esta. Mantendrá el equilibrio en el rango más común (20–40 años). Tampoco es necesario complicar la imputación ya que hay pocos valores nulos. En este caso 12.

```{r}
library(caret)

credit.Datos.Test.imp <- credit.Datos.Test
credit.Datos.Train.imp <- credit.Datos.Train

# Calcular la mediana en el conjunto de entrenamiento
mediana_edad <- median(credit.Datos.Train.imp$Edad, na.rm = TRUE)

# Imputar en el conjunto de entrenamiento
credit.Datos.Train.imp$Edad[is.na(credit.Datos.Train.imp$Edad)] <- mediana_edad

# Imputar en el conjunto de validación usando la mediana del entrenamiento
credit.Datos.Test.imp$Edad[is.na(credit.Datos.Test.imp$Edad)] <- mediana_edad

test_imp_num_na <- sapply(credit.Datos.Test.imp, function(x) sum(is.na(x)))
train_imp_num_na <- sapply(credit.Datos.Train.imp, function(x) sum(is.na(x)))
print(train_imp_num_na+test_imp_num_na)
```

Ahora vamos a analizar la variable V14 (continua).

```{r}
# histograma enriquecido para V14
hist(credit.Datos.Train$V14, xlab="",
main="Máximo valor de V14", ylim=c(0,0.004),probability=T)
lines(density(credit.Datos.Train$V14,na.rm=T))
rug(jitter(credit.Datos.Train$V14))

summary(credit.Datos.Train$V14)
```

La variable V14, según la gráfica y el resumen estadístico, presenta una distribución muy asimétrica, con la mayoría de los valores concentrados en el rango bajo (entre 0 y 280), pero con algunos valores muy altos (hasta 2000, outliners). Dado que hay 13 valores faltantes (NA), que no son muchos, pero sí más que las anteriores, debemos de elegir una buena ténica de imputación:

```{r}
# Calcular la mediana solo en el conjunto de entrenamiento
mediana_v14 <- median(credit.Datos.Train.imp$V14, na.rm = TRUE)

# Imputar NA en el conjunto de entrenamiento
credit.Datos.Train.imp$V14[is.na(credit.Datos.Train.imp$V14)] <- mediana_v14

# Imputar NA en el conjunto de validación
credit.Datos.Test.imp$V14[is.na(credit.Datos.Test.imp$V14)] <- mediana_v14

test_imp_num_na <- sapply(credit.Datos.Test.imp, function(x) sum(is.na(x)))
train_imp_num_na <- sapply(credit.Datos.Train.imp, function(x) sum(is.na(x)))
print(train_imp_num_na+test_imp_num_na)
```

Ahora solo nos quedan variables categóricas.

Procedemos con la primera variable cetegórica, Género:\

```{r}
frecuencias <- table(credit.Datos.Train$Genero)

barplot(frecuencias, 
        main = "Distribución de Genero", 
        xlab = "Genero", 
        ylab = "Frecuencia", 
        col = "skyblue")

```

Debido al bajo número de NA y sabiendo que la categoría altamente dominante de b (hombres, según hemos especulado). Pensamos que lo más apropiado es asumir que son hombres, imputación por la moda. Imputar por "unknown" pensamos que no beneficia en absoluto el algoritmo.

```{r}
moda_genero <- "b"

# Imputar en el conjunto de entrenamiento
credit.Datos.Train.imp$Genero[is.na(credit.Datos.Train.imp$Genero)] <- moda_genero

# Imputar en el conjunto de validación
credit.Datos.Test.imp$Genero[is.na(credit.Datos.Test.imp$Genero)] <- moda_genero

test_imp_num_na <- sapply(credit.Datos.Test.imp, function(x) sum(is.na(x)))
train_imp_num_na <- sapply(credit.Datos.Train.imp, function(x) sum(is.na(x)))
print(train_imp_num_na+test_imp_num_na)
```

La siguiente es la variable "EstadoCivil". Variable categórica con 3 valores categóricos, siendo su dominio {l,u,y}. Vamos a ver la distribución que sigue:

```{r}
frecuencias <- table(credit.Datos.Train$EstadoCivil)

barplot(frecuencias, 
        main = "Distribución de EstadoCivil", 
        xlab = "EstadoCivil", 
        ylab = "Frecuencia", 
        col = "skyblue")
```

Como en el caso anterior, y con más razón aún, vamos a imputar por la moda. Es evidente que hay una categoría muy dominante, y el bajo número de NA hace que no vaya a variar prácticamente la distribución:

```{r}
moda_genero <- "u"

# Imputar en el conjunto de entrenamiento
credit.Datos.Train.imp$EstadoCivil[is.na(credit.Datos.Train.imp$EstadoCivil)] <- moda_genero

# Imputar en el conjunto de validación
credit.Datos.Test.imp$EstadoCivil[is.na(credit.Datos.Test.imp$EstadoCivil)] <- moda_genero

test_imp_num_na <- sapply(credit.Datos.Test.imp, function(x) sum(is.na(x)))
train_imp_num_na <- sapply(credit.Datos.Train.imp, function(x) sum(is.na(x)))
print(train_imp_num_na+test_imp_num_na)
```

Analizamos la variable categórica V5 para saber qué forma tienen los datos:

```{r}
frecuencias <- table(credit.Datos.Train$V5)

barplot(frecuencias, 
        main = "Distribución de V5", 
        xlab = "V5", 
        ylab = "Frecuencia", 
        col = "skyblue")
```

Podemos imputar por la moda, ya que la categoría "g" es claramente dominante, y no cambiará mucho la distribución:

```{r}
moda_genero <- "g"

# Imputar en el conjunto de entrenamiento
credit.Datos.Train.imp$V5[is.na(credit.Datos.Train.imp$V5)] <- moda_genero

# Imputar en el conjunto de validación
credit.Datos.Test.imp$V5[is.na(credit.Datos.Test.imp$V5)] <- moda_genero

test_imp_num_na <- sapply(credit.Datos.Test.imp, function(x) sum(is.na(x)))
train_imp_num_na <- sapply(credit.Datos.Train.imp, function(x) sum(is.na(x)))
print(train_imp_num_na+test_imp_num_na)
```

Procedemos a evaluar V6 (categórica):

```{r}
frecuencias <- table(credit.Datos.Train$V6)

barplot(frecuencias, 
        main = "Distribución de V6", 
        xlab = "V6", 
        ylab = "Frecuencia", 
        col = "skyblue")
```

Como V6 no tiene categorías tan predominantes, por tanto, necesitamos emplear otra técnica. Como tenemos 9 NA, que no son muchos, tampoco tenemos por qué complicarlo mucho. Algo interesante que podemos hacer es imputación aleatoria ponderada. Como su propio nombre indica consiste en generar categorías de forma aleatoria teniendo en cuenta su participación en la variable.

```{r}
set.seed(123)
categorias <- names(table(credit.Datos.Train.imp$V6))
probabilidades <- prop.table(table(credit.Datos.Train.imp$V6))

# Imputar valores NA
credit.Datos.Train.imp$V6[is.na(credit.Datos.Train.imp$V6)] <- sample(categorias, size = sum(is.na(credit.Datos.Train.imp$V6)), replace = TRUE, prob = probabilidades)

sum(is.na(credit.Datos.Train.imp$V6))

test_imp_num_na <- sapply(credit.Datos.Test.imp, function(x) sum(is.na(x)))
train_imp_num_na <- sapply(credit.Datos.Train.imp, function(x) sum(is.na(x)))
print(train_imp_num_na+test_imp_num_na)
```

Por último, analizamos la variable categórica V7:

```{r}
frecuencias <- table(credit.Datos.Train$V7)

barplot(frecuencias, 
        main = "Distribución de V7", 
        xlab = "V7", 
        ylab = "Frecuencia", 
        col = "skyblue")
```

Es evidente que una sustitución por la moda es muy interesante en esta variable también:

```{r}
moda_genero <- "v"

# Imputar en el conjunto de entrenamiento
credit.Datos.Train.imp$V7[is.na(credit.Datos.Train.imp$V7)] <- moda_genero

# Imputar en el conjunto de validación
credit.Datos.Test.imp$V7[is.na(credit.Datos.Test.imp$V7)] <- moda_genero

test_imp_num_na <- sapply(credit.Datos.Test.imp, function(x) sum(is.na(x)))
train_imp_num_na <- sapply(credit.Datos.Train.imp, function(x) sum(is.na(x)))
print(train_imp_num_na+test_imp_num_na)
```

A continuación, vamos a comprobar que la distribución de las variables imputadas, siguen siendo casi iguales (no hayan cambiado mucho):

```{r}
# Configurar la ventana gráfica para dos gráficos lado a lado
par(mfrow = c(1, 2))  # 1 fila, 2 columnas

#____________________Genero_________________________
frecuencias <- table(credit.Datos.Train$Genero)
barplot(frecuencias, 
        main = "Distribución de Genero", 
        xlab = "Genero", 
        ylab = "Frecuencia", 
        col = "skyblue")

frecuencias <- table(credit.Datos.Train.imp$Genero)
barplot(frecuencias, 
        main = "Distribución de Genero", 
        xlab = "Genero", 
        ylab = "Frecuencia", 
        col = "lightgreen")

#____________________Edad_________________________
hist(credit.Datos.Train$Edad, 
     main = "Histograma de Edad", 
     xlab = "Edad", 
     col = "skyblue")

hist(credit.Datos.Train.imp$Edad, 
     main = "Histograma de Edad", 
     xlab = "Edad", 
     col = "lightgreen")

#________________________EstadoCivil______________________
frecuencias <- table(credit.Datos.Train$EstadoCivil)
barplot(frecuencias, 
        main = "Distribución de EstadoCivil", 
        xlab = "EstadoCivil", 
        ylab = "Frecuencia", 
        col = "skyblue")

frecuencias <- table(credit.Datos.Train.imp$EstadoCivil)
barplot(frecuencias, 
        main = "Distribución de EstadoCivil", 
        xlab = "EstadoCivil", 
        ylab = "Frecuencia", 
        col = "lightgreen")

#____________________V5_______________
frecuencias <- table(credit.Datos.Train$V5)
barplot(frecuencias, 
        main = "Distribución de V5", 
        xlab = "V5", 
        ylab = "Frecuencia", 
        col = "skyblue")

frecuencias <- table(credit.Datos.Train.imp$V5)
barplot(frecuencias, 
        main = "Distribución de V5", 
        xlab = "V5", 
        ylab = "Frecuencia", 
        col = "lightgreen")

#____________________V6_______________
frecuencias <- table(credit.Datos.Train$V6)
barplot(frecuencias, 
        main = "Distribución de V6", 
        xlab = "V6", 
        ylab = "Frecuencia", 
        col = "skyblue")

frecuencias <- table(credit.Datos.Train.imp$V6)
barplot(frecuencias, 
        main = "Distribución de V6", 
        xlab = "V6", 
        ylab = "Frecuencia", 
        col = "lightgreen")

#____________________V7_______________
frecuencias <- table(credit.Datos.Train$V7)
barplot(frecuencias, 
        main = "Distribución de V7", 
        xlab = "V7", 
        ylab = "Frecuencia", 
        col = "skyblue")

frecuencias <- table(credit.Datos.Train.imp$V7)
barplot(frecuencias, 
        main = "Distribución de V7", 
        xlab = "V7", 
        ylab = "Frecuencia", 
        col = "lightgreen")

#____________________V14_______________
hist(credit.Datos.Train$V14, 
     main = "Histograma de V14", 
     xlab = "V14", 
     col = "skyblue")

hist(credit.Datos.Train.imp$V14, 
     main = "Histograma de V14", 
     xlab = "V14", 
     col = "lightgreen")
```

Como es evidente no ha cambiado casi nada (inapreciable). Entre otras cosas, debido al bajo número de NA.

En cuanto a la sustitución mediante estudio de correlaciones, podemos ver si hay alguna de la siguiente manera (lo hacemos con los datos sin imputar para justificar que no era necesario usarlo):

```{r}
# Solo las columnas numéricas
credit.Datos.numericas <- credit.Datos.Train[, sapply(credit.Datos.Train, is.numeric)]

matriz_correlacion <- cor(credit.Datos.numericas, use = "complete.obs")

print(matriz_correlacion)
```

Es evidente que no hay ninguna fuerte correlación entre variables, por tanto, hemos excluido esta opción de imputación. Al no haber correlación tampoco es interesante la sustitución de variables numéricas mediante clustering (knn).

```{r}
gen_plots <- function() {

  credit.Datos.continuas <- credit.Datos.Train.imp[, sapply(credit.Datos.Train.imp, is.numeric)]
  
  for (name in colnames(credit.Datos.continuas)) {
    hist(credit.Datos.continuas[[name]], 
         main = paste("Histograma de", name),
         xlab = name,
         col = "lightgreen")
  }
}

# Ejecutar la función
gen_plots()

```

En el caso en el que queramos quitar toda la imputación, comentar este código:

```{r}
credit.Datos.Test <- credit.Datos.Test.imp
credit.Datos.Train <- credit.Datos.Train.imp
```

## Pre-procesado de datos (II): Eliminar predictores correlados o de poca Varianza

#### 1. Eliminar variables con poca Varianza

Debemos de comprobar si la base de datos tiene columnas con poca varianza. Esto lo podemos comprobar con nearZero():

**Aclaración:** No tenemos en cuenta la última columna con valores {train, test} ya que no cuentan para el análisis.

```{r}
nearZeroVar(credit.Datos.Train[, 1:16])
nearZeroVar(credit.Datos.Test[, 1:16])
```

Como vemos no hay ninguna columna que tenga varianza cercana a cero, lo que nos indica que los datos están ya bastante "limpios".

Quitamos la última columna

```{r}
credit.Datos.Train$Origen <- NULL
credit.Datos.Test$Origen <- NULL
```

## Entrenamiento de modelos que proporciona Caret

### **1. GBM:**

Lo primero que debemos de buscar son las características de este modelo para ver si es interesante para nuestro caso. En este enlace podemos encontrar mucha información acerca del mismo:\
<https://www.linkedin.com/pulse/ai-algorithms-deep-dive-gradient-boosting-machines-gbm-vasu-rao-snesc>

En dicho enlace, en el apartado "Applications of GBM in Enterprises", recomienda usarlo para "Credit Scoring":

-   **Credit Scoring:** Financial institutions use GBM to predict the creditworthiness of loan applicants, enabling informed lending decisions and minimizing risk.

También contamos con información sobre sus debilidades, y es importante tenerlas en cuenta:

While GBM is a powerful tool, it's not without its limitations:

-   **Computational Intensity:** Training GBM models can be computationally expensive, especially when dealing with large datasets and many boosting stages (iterations).

-   **Risk of Overfitting:** Without proper tuning and regularization techniques, GBM models can overfit, particularly on noisy data. Overfitting occurs when the model performs well on the training data but poorly on unseen data.

-   **Parameter Sensitivity:** The performance of GBM models is highly dependent on the chosen hyperparameters. Careful tuning is necessary to achieve optimal results.

Por lo tanto, puede ser una buena opción. Una vez hemos investigado sobre su funcionamiento, es importante ver con qué requisitos de preprocesado gbm se encuentra más cómodo. En el enlace anterior recomiendan:

1.  **Data Preprocessing:** Clean your data to ensure optimal GBM performance. This includes handling missing values, normalizing numerical features, and encoding categorical variables.

En el enlace hay mucha información de utilidad que luego nos será de gran utilidad. Seguimos las recomendaciones, y el siguiente paso es hacer Dummy. Aunque antes deberíamos de hacer una copia de la base de datos para no sobrescribirla.

```{r}
credit.Datos.gbm.Train <- credit.Datos.Train
credit.Datos.gbm.Test <- credit.Datos.Test
```

#### 1.1 Dummy:

```{r}
# Cogemos la base de datos que queremos
credit.Datos.gbm.dummy.Train <- credit.Datos.gbm.Train
credit.Datos.gbm.dummy.Test <- credit.Datos.gbm.Test

# Definir la variable de salida y las variables de entrada
credit.Var.Salida.Usada <- "V16"
credit.Vars.Entrada.Usadas <- setdiff(names(credit.Datos.gbm.dummy.Train), credit.Var.Salida.Usada)

# Convertir la variable de salida a factor (para clasificación)
credit.Datos.gbm.dummy.Train[[credit.Var.Salida.Usada]] <- as.factor(credit.Datos.gbm.dummy.Train[[credit.Var.Salida.Usada]])
credit.Datos.gbm.dummy.Test[[credit.Var.Salida.Usada]] <- as.factor(credit.Datos.gbm.dummy.Test[[credit.Var.Salida.Usada]])

# Crear el modelo dummy solo para las variables de entrada
dummy_model <- dummyVars(~ ., data = credit.Datos.gbm.dummy.Train[, credit.Vars.Entrada.Usadas], fullRank = TRUE)

# Transformar las variables de entrada a dummies (sin tocar la salida)
credit.Datos.Train.Dummies <- data.frame(predict(dummy_model, credit.Datos.gbm.dummy.Train[, credit.Vars.Entrada.Usadas]))
credit.Datos.Test.Dummies <- data.frame(predict(dummy_model, credit.Datos.gbm.dummy.Test[, credit.Vars.Entrada.Usadas]))

# Añadir la variable de salida de nuevo a los conjuntos transformados
credit.Datos.Train.Dummies[[credit.Var.Salida.Usada]] <- credit.Datos.gbm.dummy.Train[[credit.Var.Salida.Usada]]
credit.Datos.Test.Dummies[[credit.Var.Salida.Usada]] <- credit.Datos.gbm.dummy.Test[[credit.Var.Salida.Usada]]

# Guardamos (en caso de querer eliminar, eliminamos estas líneas)
credit.Datos.gbm.Train <- credit.Datos.Train.Dummies
credit.Datos.gbm.Test <- credit.Datos.Test.Dummies
```

#### 1.2 PCA:

Pese a no indicar en ningún sitio que PCA/ICA sea necesario en gbm, debemos de comprobarlo experimentalmente. Por lo tanto, lo incluimos en nuestro procesado:

```{r}
# Cogemos la base de datos que nos interesa, lo hacemos así para que sea configurable
credit.Datos.gbm.pca.Train <- credit.Datos.gbm.Train
credit.Datos.gbm.pca.Test <- credit.Datos.gbm.Test

credit.PreProc.Pca.Mod <- preProcess(credit.Datos.gbm.pca.Train, method = "pca", thresh = 0.95)  # 95% de la varianza explicada
print(credit.PreProc.Pca.Mod)
credit.Datos.Train.Pca <- predict(credit.PreProc.Pca.Mod, credit.Datos.gbm.pca.Train)
credit.Datos.Test.Pca <- predict(credit.PreProc.Pca.Mod, credit.Datos.gbm.pca.Test)

# Guardamos (en caso de querer eliminar, eliminamos estas líneas)
#credit.Datos.gbm.Train <- credit.Datos.Train.Pca
#credit.Datos.gbm.Test <- credit.Datos.Test.Pca
```

#### 1.3 ICA:

```{r}
# Cogemos la base de datos que nos interesa, lo hacemos así para que sea configurable
credit.Datos.gbm.ica.Train <- credit.Datos.gbm.Train
credit.Datos.gbm.ica.Test <- credit.Datos.gbm.Test

credit.PreProc.Ica.Mod <- preProcess(credit.Datos.gbm.ica.Train, method = "ica", n.comp = 10)  # 10 componentes independientes
credit.PreProc.Ica.Mod
credit.Datos.Train.Ica <- predict(credit.PreProc.Ica.Mod, credit.Datos.gbm.ica.Train)
credit.Datos.Test.Ica <- predict(credit.PreProc.Ica.Mod, credit.Datos.gbm.ica.Test)

# Guardamos (en caso de querer eliminar, eliminamos estas líneas)
#credit.Datos.gbm.Train <- credit.Datos.Train.Ica
#credit.Datos.gbm.Test <- credit.Datos.Test.Ica
```

\
En la web anterior recomiendan normalizar los datos antes del entrenamiento, sin embargo, como sabemos gbm está basado en árboles de decisión, y la normalización no les afecta mucho. Como podemos encontrar en: <https://datascience.stackexchange.com/questions/6721/how-to-preprocess-different-kinds-of-data-continuous-discrete-categorical-be>

*"In fact, the results should be consistent regardless of any scaling or translational normalization, since the trees can choose equivalent splitting points"*\

Por lo tanto, antes esta contradicción, la única forma de comprobarlo es realizando los dos tipos de normalización que encontramos en el documento de caret y viendo resultados:

#### 1.4. Normalización range:

```{r}
library(caret)

# Cogemos la base de datos que nos interesa, lo hacemos así para que sea configurable
credit.Datos.gbm.norm.range.Train <- credit.Datos.gbm.Train
credit.Datos.gbm.norm.range.Test <- credit.Datos.gbm.Test

# Identificar las variables numéricas del conjunto de entrenamiento
numeric_vars <- names(credit.Datos.gbm.norm.range.Train)[sapply(credit.Datos.gbm.norm.range.Train, is.numeric)]

# Ajustar el modelo de preprocesamiento en el entrenamiento (normalización Min-Max)
# "range" realiza una normalización entre 0 y 1
preProcParams <- preProcess(credit.Datos.gbm.norm.range.Train[, numeric_vars], method = c("range"))

# Aplicar la normalización al conjunto de entrenamiento
credit.Datos.Train.norm <- predict(preProcParams, credit.Datos.gbm.norm.range.Train)

# Aplicar la misma transformación al conjunto de test
credit.Datos.Test.norm <- predict(preProcParams, credit.Datos.gbm.norm.range.Test)


# Guardamos (en caso de querer eliminar, eliminamos estas líneas)
#credit.Datos.gbm.Train <- credit.Datos.Train.norm
#credit.Datos.gbm.Test <- credit.Datos.Test.norm
```

#### 1.5. Normalización center-scale:

```{r}
# Cogemos la base de datos que nos interesa, lo hacemos así para que sea configurable
credit.Datos.gbm.norm.cs.Train <- credit.Datos.gbm.Train
credit.Datos.gbm.norm.cs.Test <- credit.Datos.gbm.Test

credit.Var.Salida.Usada <- c("V16")
credit.Var.Entrada.Usadas <- setdiff(names(credit.Datos.gbm.norm.cs.Train),credit.Var.Salida.Usada)

credit.preProc.CS.Mod<-preProcess(credit.Datos.gbm.norm.cs.Train[credit.Var.Entrada.Usadas],
                                  method=c("center","scale"))
credit.Datos.Train.CS <- predict(credit.preProc.CS.Mod,credit.Datos.gbm.norm.cs.Train)
credit.Datos.Test.CS <- predict(credit.preProc.CS.Mod,credit.Datos.gbm.norm.cs.Test)

#credit.Datos.gbm.Train <- credit.Datos.Train.CS
#credit.Datos.gbm.Test <- credit.Datos.Test.CS
```

#### 1.6. Últimas comprobaciones antes de entrenar:

Comprobamos si se ha generado alguna variable con varianza cercana a cero, y eliminamos dichas columnas (pasará cuando hagamos dummy y no hagamos PCA/ICA):

```{r}
nearZeroVar(credit.Datos.gbm.Train)
nearZeroVar(credit.Datos.gbm.Test)

# Detectar columnas con varianza cero en Train
cols_varianza_cero <- nearZeroVar(credit.Datos.gbm.Train, saveMetrics = TRUE)

# Ver columnas eliminadas
print("Columnas Eliminadas por Varianza Cero:")
print(rownames(cols_varianza_cero[cols_varianza_cero$nzv == TRUE, ]))

# Eliminar columnas de Train y Test
credit.Datos.gbm.Train <- credit.Datos.gbm.Train[, !cols_varianza_cero$nzv]
credit.Datos.gbm.Test <- credit.Datos.gbm.Test[, !cols_varianza_cero$nzv]

nearZeroVar(credit.Datos.gbm.Train)
nearZeroVar(credit.Datos.gbm.Test)

```

Todo correcto.

#### 1.8. Entrenamiento:

```{r}
credit.Var.Salida.Usada <- "V16"
credit.Vars.Entrada.Usadas <- setdiff(names(credit.Datos.gbm.Train), credit.Var.Salida.Usada)

credit.trainCtrl.none.clssProb <- trainControl(method = "none", classProbs = TRUE)
set.seed(2)
credit.modelo.none.gbm.ROC<-train(credit.Datos.gbm.Train[credit.Vars.Entrada.Usadas],
    credit.Datos.gbm.Train[[credit.Var.Salida.Usada]],
    method = "gbm",
    trControl=credit.trainCtrl.none.clssProb,
    verbose = FALSE,
    tuneGrid = data.frame(interaction.depth = 4,
    n.trees = 100,
    shrinkage = .1,
    n.minobsinnode = 20),
    metric = "ROC")

# Ver los detalles del modelo entrenado
print(credit.modelo.none.gbm.ROC)

# Predecir en el conjunto de prueba
credit.Pred.Test <- predict(credit.modelo.none.gbm.ROC, credit.Datos.gbm.Test)

# Evaluar el rendimiento en el conjunto de prueba
matriz_confusion <- confusionMatrix(credit.Pred.Test, credit.Datos.gbm.Test[[credit.Var.Salida.Usada]])

# Extraer el porcentaje de acierto
porcentaje_acierto <- matriz_confusion$overall["Accuracy"] * 100

# Mostrar el porcentaje de acierto
print(paste("Porcentaje de acierto:", round(porcentaje_acierto, 2), "%"))
```

#### 1.9. Encontrar los mejores hiper-parámetros y ajustar modelo final:

```{r}
library(caret)
library(gbm)

# Configuración de la validación cruzada
credit.trainCtrl.3cv10.resampAll <- trainControl(
  method = "repeatedcv", # Validación cruzada de 10 pliegues
  number = 10,           # Número de pliegues
  repeats = 3,           # Repeticiones
  verboseIter = F,    # Voy a quitar Verbose porque inunda toda la pantalla
  returnResamp = "all"   # Guardar todos los resultados
)

# Rejilla de hiperparámetros para GBM
gbm.grid <- expand.grid(
  n.trees = c(100, 500),              # Árboles limitados
  shrinkage = c(0.01, 0.1),           # Tasas de aprendizaje clave
  n.minobsinnode = c(5, 10),          # Nodos hijos mínimos
  interaction.depth = c(3, 5)         # Profundidad limitada
)

# Definir variables predictoras y objetivo
credit.Var.Salida.Usada <- "V16"
credit.Vars.Entrada.Usadas <- setdiff(names(credit.Datos.gbm.Train), credit.Var.Salida.Usada)

set.seed(2)
credit.modelo.3cv10.grid.gbm<-train(
  credit.Datos.gbm.Train[credit.Vars.Entrada.Usadas],
  credit.Datos.gbm.Train[[credit.Var.Salida.Usada]],
  method="gbm", trControl=credit.trainCtrl.3cv10.resampAll,
  tuneGrid = gbm.grid
,verbose=F
)

# Mostrar resultados
print(credit.modelo.3cv10.grid.gbm)

# Evaluación en el conjunto de test
predictions <- predict(credit.modelo.3cv10.grid.gbm, credit.Datos.gbm.Test)

# Calcular matriz de confusión y Accuracy
conf_mat <- confusionMatrix(predictions, credit.Datos.gbm.Test[[credit.Var.Salida.Usada]])

# Mostrar el porcentaje de acierto
accuracy <- conf_mat$overall["Accuracy"]
print(paste("Porcentaje de Acierto:", round(accuracy * 100, 2), "%"))
```

#### 1.10. Entrenamos con hiper-parámetros fijos:

Una vez hemos encontrado los mejores hiper-parámetros debemos de entrenar con dichos hiper-parámetros. Los que mejor resultado han dado son:\
\
Árboles seleccionados: 500\
Profundidad máxima: 5\
Tasa de aprendizaje: 0.01\
Mínimos nodos por hoja: 10\
\
Por lo tanto, vamos a entrenar con dichos hiper-parámetros:

```{r}
library(caret)
library(gbm)

# Configuración sin remuestreo para entrenar el modelo final
credit.trainCtrl.none <- trainControl(
  method = "none",  # Sin remuestreo
  classProbs = TRUE # Para evaluar métricas de rendimiento
)

# Definir variables predictoras y objetivo
credit.Var.Salida.Usada <- "V16"
credit.Vars.Entrada.Usadas <- setdiff(names(credit.Datos.gbm.Train), credit.Var.Salida.Usada)

# Entrenar el modelo con los mejores hiperparámetros seleccionados
set.seed(2)
credit.modelo.final.gbm <- train(
  x = credit.Datos.gbm.Train[, credit.Vars.Entrada.Usadas],
  y = credit.Datos.gbm.Train[[credit.Var.Salida.Usada]],
  method = "gbm",
  trControl = credit.trainCtrl.none,
  tuneGrid = data.frame(
    n.trees = 500,              # Árboles seleccionados
    interaction.depth = 5,      # Profundidad máxima
    shrinkage = 0.01,           # Tasa de aprendizaje
    n.minobsinnode = 10         # Mínimos nodos por hoja
  ),
  metric = "Accuracy",          # Métrica de evaluación
  verbose = FALSE               # Sin salida en consola
)

# Evaluación en el conjunto de prueba
predictions <- predict(credit.modelo.final.gbm, credit.Datos.gbm.Test)

# Calcular matriz de confusión y accuracy
conf_mat <- confusionMatrix(predictions, credit.Datos.gbm.Test[[credit.Var.Salida.Usada]])
accuracy <- conf_mat$overall["Accuracy"]

# Mostrar resultados
print(credit.modelo.final.gbm)
print(paste("Porcentaje de Acierto (Modelo Final):", round(accuracy * 100, 2), "%"))

```






# Random Forest




Vamos a comenzar a entrenar modelos random forest. Para ellos vamos primero a ver que hiperparametros nos ofrece caret de forma directa para usar.

```{r}
modelLookup(("rf"))
```
En este caso solo disponemos de un hiperparametro, pero random forest tiene mas hiperparametros como puede ser ntree que es el numero de arboles generado el cual lo fijaremos en 1000 en todos los train crontrol para darle estabilidad al modelo. Para mtry creamos un grid con distintos valores para este hiperparametro.

```{r}
rf.grid <- expand.grid(mtry = c(1, 2, 3, 10, 15))
```


## Configuraciones de trainControl

Vamos usar trainControl() para configurar procesos del entrenamiento donde vamos a poner una configuracion habitual recomendada por el caret la cual consiste en cross validacion cruzada de 10 plieges y 3 repeticiones.


```{r}
rf.trainControl.3cv10.twoSummary.roc <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 3,
  # No mostramos el proceso porque seria muy extenso
  verboseIter = FALSE,
  # Métricas adicionales como ROC (para clasificación binaria)
  summaryFunction = twoClassSummary,
  # Necesario si queremos usar roc
  classProbs = TRUE,
  # Guardamos todo para hacer diagramas
  returnResamp = "all"
)
```


## Preprocesado de datos para el modelo

### Imputando valores NA


El modelo random forest no permite valores NA por lo que usaremos la base de datos cuyos valores NA estan imputados, comprobamos que efectivamente no existe valores NA en la base de datos que vamos a usar.

```{r}
any(is.na(credit.Datos.Train.imp))
```

Por lo demas nuestra base de datos a priori es apta para el entrenamiento

```{r}

varObjetivo <- "V16" # Cambia esto por tu variable de salida
varsPredictoras <- setdiff(names(credit.Datos.Train.imp), varObjetivo)
```

### Eliminando valores NA

Otra manera de preprocesar los datos para el random forest es eliminando los valores NA, para ello usamos los siguiente.

```{r}
credit.Datos.Train.EliminadoNA <- na.omit(credit.Datos.Train)
credit.Datos.Test.EliminadoNA <- na.omit(credit.Datos.Test)
```

```{r}

varObjetivo <- "V16" # Cambia esto por tu variable de salida
varsPredictoras <- setdiff(names(credit.Datos.Train.EliminadoNA), varObjetivo)
```

## Entrenamiento

### Imputando datos NA

Ahora procedemos a entrenar nuestro primer modelo de random forest



```{r}
set.seed(123)
rf.modelo.3cv10.twoSummary.roc <- train(
  x = credit.Datos.Train.imp[varsPredictoras],
  y = credit.Datos.Train.imp[[varObjetivo]],
  method = "rf", 
  metric = "ROC",
  trControl = rf.trainControl.3cv10.twoSummary.roc,
  tuneGrid = rf.grid,
  ntree = 1000
)

print(rf.modelo.3cv10.twoSummary.roc)
plot(rf.modelo.3cv10.twoSummary.roc)
```


### Eliminando datos NA


```{r}
set.seed(123)
rf.modelo.3cv10.twoSummary.roc <- train(
  x = credit.Datos.Train.EliminadoNA[varsPredictoras],
  y = credit.Datos.Train.EliminadoNA[[varObjetivo]],
  method = "rf", 
  metric = "ROC",
  trControl = rf.trainControl.3cv10.twoSummary.roc,
  tuneGrid = rf.grid,
  ntree = 1000
)

print(rf.modelo.3cv10.twoSummary.roc)
plot(rf.modelo.3cv10.twoSummary.roc)
```


## Test de modelos

```{r}

# 9. Guardar el modelo entrenado
#saveRDS(modelo_rf, "modelo_rf.rds")

# 10. Evaluación del modelo en el conjunto de prueba
# Predicción en el conjunto de prueba
predicciones <- predict(rf.modelo.3cv10.twoSummary.roc, newdata = credit.Datos.Test.imp[varsPredictoras])

# Generar matriz de confusión
confusionMatrix(
  data = predicciones,
  reference = credit.Datos.Test[[varObjetivo]],
  positive = "aprobada" # Cambia esto por la clase objetivo positiva
)


```



## Comparativa de modelos

```{r}

data.frame(Model = c("rf.modelo.3cv10.kappa", "rf.modelo.3cv10.twoSummary.roc", "rf.modelo.3cv10.multiClassSummary"), Accuracy = c(max(rf.modelo.3cv10.kappa$results$Accuracy), max(rf.modelo.3cv10.twoSummary.roc$results$Accuracy), max(rf.modelo.3cv10.multiClassSummary$results$Accuracy)))

```



## Redes neuronales

Pasamos ahroa a entrenar el modelo de redes neuronales, primero vemis los posibkes hiperparametros disponbles que tiene:

```{r}
modelLookup(("nnet"))
```
## Entrenamiento


```{r}
nnetGrid <- expand.grid(
  size = c(1, 3, 5, 7),   # Número de nodos en la capa oculta
  decay = c(0, 0.01, 0.1, 0.2) # Parámetro de regularización
)
```






```{r}
nnet.trainControl <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 3,
  verboseIter = FALSE,
  classProbs = TRUE,           # Probabilidades de clase (para clasificación)
  summaryFunction = twoClassSummary, # Métricas adicionales como ROC
  returnResamp = "all" # Guardamos todo para hacer diagramas
)
```



```{r}
set.seed(123)
nnet.train <- train(
  x = credit.Datos.Train.imp[varsPredictoras],
  y = credit.Datos.Train.imp[[varObjetivo]],
  method = "nnet",
  trControl = nnet.trainControl,
  tuneGrid = nnetGrid,               # Grid de hiperparámetros
  trace = FALSE,                            # Evita imprimir mensajes internos de nnet
  maxit = 200                               # Número máximo de iteraciones
)

print(nnet.train)
plot(nnet.train)

```



```{r}



# 7. Evaluar el modelo en el conjunto de prueba
# Predicción en el conjunto de prueba
predicciones_nnet <- predict(modelo_nnet, newdata = credit.Datos.Test[vars_predictoras])

# Matriz de confusión
matrizConfusionNNET <- confusionMatrix(
  data = predicciones_nnet,
  reference = credit.Datos.Test[[var_objetivo]],
  positive = "aprobada" # Cambia esto por el nivel positivo correcto
)



# Extraer y mostrar el porcentaje de aciertos (Accuracy)
print(paste("Porcentaje de aciertos: ", round(matrizConfusionNNET$overall["Accuracy"] * 100, 2), "%", sep = ""))

```




# Postprocesado rf


```{r}

varImp(modelo_rf)
```

La variable V7 la podriamos eliminar para el modelo ya que apenas tiene impacto


```{r}
predictors(modelo_rf$finalModel)

```























**Es importante destacar que:\
**Es curioso que en la búsqueda de hiper-parámetros obtengamos diferentes rendimientos que cuando "replicamos" el entrenamiento seleccionando ya los parámetros. Mientras en el primero obtenemos 86.86% en el otro obtenemos un 85.%.

El 85.4% es probablemente el accuracy real en el conjunto de prueba, mientras que el 86.86% es un resultado estimado basado en la validación cruzada repetida. El segundo valor suele ser más optimista, mientras que el entrenamiento final refleja mejor el rendimiento real.

**1.11. ¿Con qué configuración hemos hecho los entrenamientos?**

Consideramos que la mejor forma de ver si la teoría se ha cumplido, es ejecutar muchas configuraciones distintas para gbm. En este caso las pruebas las hemos hecho con la versión optimista ya que todavía no sabíamos que hiper-parámetros poner al no tener la configuración seleccionada. Obtenemos:

+------------------------------+---------------+
| Configuración                | Acierto       |
+==============================+===============+
| Dummy                        | ```           |
|                              | 86.86 %       |
|                              | ```           |
+------------------------------+---------------+
| PCA                          | ```           |
|                              | 84.67 %       |
|                              | ```           |
+------------------------------+---------------+
| Norm. Range                  | ```           |
|                              | 86.13 %       |
|                              | ```           |
+------------------------------+---------------+
| Norm. Center-Scale           | ```           |
|                              | 86.13 %       |
|                              | ```           |
+------------------------------+---------------+
| Dummy+PCA                    | ```           |
|                              | 86.86 %       |
|                              | ```           |
+------------------------------+---------------+
| Dummy+Norm. Range            | ```           |
|                              | 86.86 %       |
|                              | ```           |
+------------------------------+---------------+
| Dummy+Norm. Center-Scale     | ```           |
|                              | 86.86 %       |
|                              | ```           |
+------------------------------+---------------+
| Dummy+PCA+Norm. Range        | ```           |
|                              | 86.86 %       |
|                              | ```           |
+------------------------------+---------------+
| Dummy+PCA+Norm. Center-Scale | ```           |
|                              | 86.86 %       |
|                              | ```           |
+------------------------------+---------------+
| PCA+Norm. Range              | ```           |
|                              | 84.67 %       |
|                              | ```           |
+------------------------------+---------------+
| PCA+Norm. Center-Scale       | ```           |
|                              | 84.67 %       |
|                              | ```           |
+------------------------------+---------------+
| Dummy+ICA+Norm. Range        | ```           |
|                              | 85.5 %        |
|                              | ```           |
+------------------------------+---------------+
| ICA                          | ```           |
|                              | 83.21 %       |
|                              | ```           |
+------------------------------+---------------+

: **Nota:** En esta tabla NO están todas las combinaciones posibles, ya que sería demasiado largo, hemos pensado en las que mejor casarían, evitado usar preprocesados similares juntos (por ejemplo las dos normalizaciones), y hemos evitado muchas combinaciones aplicando el razonamiento.

Esta tabla es muy representativa. En lo primero que nos fijamos es que parece ser que en 86.86 % hay un techo, que no aumenta. Si nos fijamos, si solo aplicamos Dummy, obtenemos el rendimiento máximo (de todos los rendimientos obtenidos), por lo tanto, nos da una idea de que realmente es muy necesario, tal y como se indicaba en el enlace comentado con anterioridad. En cuanto eliminamos Dummy todos los rendimientos son menores. PCA no aporta nada, de hecho, en algunos casos empeora el rendimiento, por ejemplo, cuando lo usamos junto a las normalizaciones. ICA más de lo mismo, al ser parecido a PCA (objetivos similares), obtiene rendimientos parecidos. En cuanto a las normalizaciones, son parecidas, no hay diferencias entre ellas, y obtienen buenos redimientos cuando las incluimos, pero menos que al usar Dummy.

Para concluir, lo único realmente necesario es hacer Dummy, lo demás, para este caso y este modelo es innecesario.

Según hemos podido encontrar en esta página web: <https://www.619.io/blog/2017/06/30/preliminary-investigation-pca-and-boosting/>\

"It is a bit disappointing to observe no improvement after reducing the number of features using PCA; and even worse, we get some noticeable degradation of the baseline result. Sometimes things like that happen… A more thorough feature tinkering might help here to reduce the number of features and achieve better performance!"

#### 1.12. Eliminar variables no importantes:

Podemos probar a eliminar las columnas que no se utilizan. Primero debemos de comprobar cuantas se utlizan:

```{r}
print(varImp(credit.modelo.final.gbm))
```

Como vemos solo hay 20 variables relevantes de 26 que hay. Por lo tanto, podemos probar a eliminarlas, y entrenar de nuevo:

```{r}
# Seleccionar las 10 variables más importantes
vars_imp <- rownames(varImp(credit.modelo.final.gbm)$importance)[1:20]
vars_imp <- union(vars_imp, "V16")

# Filtrar la base de datos para incluir solo esas columnas
credit.Datos.gbm.imp.Train <- credit.Datos.gbm.Train[, vars_imp, drop = FALSE]
credit.Datos.gbm.imp.Test <- credit.Datos.gbm.Test[, vars_imp, drop = FALSE]

credit.Var.Salida.Usada <- "V16"
credit.Vars.Entrada.Usadas <- setdiff(names(credit.Datos.gbm.imp.Train), credit.Var.Salida.Usada)


# Entrenar usando validación cruzada
credit.trainCtrl.cv <- trainControl(
  method = "cv",       # Validación cruzada estándar
  number = 10,         # 10 pliegues
  classProbs = TRUE,  
  summaryFunction = twoClassSummary
)
set.seed(2)
modelo_cv <- train(
  x = credit.Datos.gbm.imp.Train[, credit.Vars.Entrada.Usadas],
  y = credit.Datos.gbm.imp.Train[[credit.Var.Salida.Usada]],
  method = "gbm",
  trControl = credit.trainCtrl.cv,
  tuneGrid = data.frame(
    n.trees = 500,
    interaction.depth = 5,
    shrinkage = 0.01,
    n.minobsinnode = 10
  ),
  metric = "ROC",
  verbose = FALSE
)

# Evaluación con ambos modelos
pred_cv <- predict(modelo_cv, credit.Datos.gbm.imp.Test)
conf_mat_cv <- confusionMatrix(pred_cv, credit.Datos.gbm.imp.Test[[credit.Var.Salida.Usada]])
accuracy_cv <- conf_mat_cv$overall["Accuracy"]

# Mostrar resultados
cat("Porcentaje de Acierto (Validación Cruzada):", round(accuracy_cv * 100, 2), "%\n")
```

Obtenemos el mismo rendimiento. Explicación:

GBM es un modelo que automáticamente reduce la importancia de variables irrelevantes, por lo que eliminarlas manualmente puede no afectar significativamente.

#### 1.13. Conclusión:

Terminamos este entrenamiento habiendo probado muchísimas configuraciones de preprocesado, eligiendo la más prometedora, y buscando los parámetros idóneos que debería de tener nuestro modelo para obtener buenas predicciones. Siendo nuestro porcentaje de acierto experimental: 85.4%.

### 2. SVM

En primer lugar vamos a tratar de buscar información a cerca de las características del modelo. Gran parte de la informacion obtenida es de la siguiente pagina.\
[https://medium.com/\@vdeshpande551/understanding-the-inner-workings-of-svm-from-data-preprocessing-to-model-deployment-cdc9d72a2d34](https://medium.com/@vdeshpande551/understanding-the-inner-workings-of-svm-from-data-preprocessing-to-model-deployment-cdc9d72a2d34){.uri}\
El modelo SVM (support vector machines) funciona buscando el hiperplano óptimo que maximiza la separación entre las clases en el espacio de características. Es por esto que SVM es sensible a la escala de los datos porque utiliza medidas de distancias es por esto que vamos a probar varios modelos con diferentes escalados. Tambien hay que tener en cuenta que SVM no trabaja con variables categoricas por lo que tendremos que hacer dummys, para ello supondremos que todas son tipo ranked. Otro aspecto necesario para que nuestro modelo funcione es eliminar las variables con poca correlación. Por último realizaremos otras pruebas como reducir la dimensionalidad con diferentes metodos, tratar los datos desvalanceados y probaremos a eliminar ciertas variables que no tengan importancia en el modelo final.

```{r}
credit.Datos.Train.Svm <- credit.Datos.Train
credit.Datos.Test.Svm <- credit.Datos.Test
```

#### 2.1 Transformacion de distribuciones asimetricas

En primer lugar vamos a comenzar tratando las distribuciones asimetricas. Para ello vamos a usar **BoxCox** cuando los datos sean positivos y continuos y las distribuciones esten sesgadas a la derecha. Yeo-Johnson podemos usarla cuando los datos sean positivos, negativos o ceros y ademas funciona bien con asimetrias de ambos tipos. Por último expoTrans la emplearemos cuando los datos tengan una asimetria extremadamente sesgada.

```{r}
credit.Datos.Train.Asimetricas <- credit.Datos.Train.Svm
credit.Datos.Test.Asimetricas <- credit.Datos.Test.Svm
```

```{r}
#Edad
Var<-"Edad"
hist(credit.Datos.Train$Edad, 
     main = c("Histograma de ",Var),  # Título del gráfico
     xlab = Var,                # Etiqueta del eje X
     col = "blue",                 # Color de las barras
     border = "black",             # Color del borde de las barras
     breaks = 20)
preProcValues <- preProcess(credit.Datos.Train.Asimetricas[Var], method = "BoxCox")
credit.Datos.Train.Asimetricas[Var] <- predict(preProcValues, credit.Datos.Train.Asimetricas[Var])
credit.Datos.Test.Asimetricas[Var] <- predict(preProcValues, credit.Datos.Test.Asimetricas[Var])

# Histograma para credit.Datos.Train.Asimetricas$Edad
hist(credit.Datos.Train.Asimetricas$Edad, 
     main = paste("Histograma de", Var, "en credit.Datos.Train.Asimetricas"), 
     xlab = Var, col = "green", border = "black", breaks = 20)

```

```{r}
#Deuda
Var<-"Deuda"
hist(credit.Datos.Train$Deuda, 
     main = c("Histograma de ",Var),  # Título del gráfico
     xlab = Var,                # Etiqueta del eje X
     col = "blue",                 # Color de las barras
     border = "black",             # Color del borde de las barras
     breaks = 20)
preProcValues <- preProcess(credit.Datos.Train.Asimetricas[Var], method = "YeoJohnson")
credit.Datos.Train.Asimetricas[Var] <- predict(preProcValues, credit.Datos.Train.Asimetricas[Var])
credit.Datos.Test.Asimetricas[Var] <- predict(preProcValues, credit.Datos.Test.Asimetricas[Var])

hist(credit.Datos.Train.Asimetricas$Deuda, 
     main = paste("Histograma de", Var, "en credit.Datos.Train.Asimetricas"), 
     xlab = Var, col = "green", border = "black", breaks = 10)
```

```{r}
#AnyosContratado
Var<-"AnyosContratado"
hist(credit.Datos.Train$AnyosContratado, 
     main = c("Histograma de ",Var),  # Título del gráfico
     xlab = Var,                # Etiqueta del eje X
     col = "blue",                 # Color de las barras
     border = "black",             # Color del borde de las barras
     breaks = 20)
preProcValues <- preProcess(credit.Datos.Train.Asimetricas[Var], method = "YeoJohnson")
credit.Datos.Train.Asimetricas[Var] <- predict(preProcValues, credit.Datos.Train.Asimetricas[Var])
credit.Datos.Test.Asimetricas[Var] <- predict(preProcValues, credit.Datos.Test.Asimetricas[Var])

hist(credit.Datos.Train.Asimetricas$AnyosContratado, 
     main = paste("Histograma de", Var, "en credit.Datos.Train.Asimetricas"), 
     xlab = Var, col = "green", border = "black", breaks = 10)
```

```{r}
#Solvencia
Var<-"Solvencia"
hist(credit.Datos.Train$Solvencia, 
     main = c("Histograma de ",Var),  # Título del gráfico
     xlab = Var,                # Etiqueta del eje X
     col = "blue",                 # Color de las barras
     border = "black",             # Color del borde de las barras
     breaks = 20)
preProcValues <- preProcess(credit.Datos.Train.Asimetricas[Var], method = "BoxCox")
credit.Datos.Train.Asimetricas[Var] <- predict(preProcValues, credit.Datos.Train.Asimetricas[Var])
credit.Datos.Test.Asimetricas[Var] <- predict(preProcValues, credit.Datos.Test.Asimetricas[Var])

hist(credit.Datos.Train.Asimetricas$Solvencia, 
     main = paste("Histograma de", Var, "en credit.Datos.Train.Asimetricas"), 
     xlab = Var, col = "green", border = "black", breaks = 10)
```

```{r}
#v14
Var<-"V14"
hist(credit.Datos.Train$V14, 
     main = c("Histograma de ",Var),  # Título del gráfico
     xlab = Var,                # Etiqueta del eje X
     col = "blue",                 # Color de las barras
     border = "black",             # Color del borde de las barras
     breaks = 20)
preProcValues <- preProcess(credit.Datos.Train.Asimetricas[Var], method = "YeoJohnson")
credit.Datos.Train.Asimetricas[Var] <- predict(preProcValues, credit.Datos.Train.Asimetricas[Var])
credit.Datos.Test.Asimetricas[Var] <- predict(preProcValues, credit.Datos.Test.Asimetricas[Var])

hist(credit.Datos.Train.Asimetricas$V14, 
     main = paste("Histograma de", Var, "en credit.Datos.Train.Asimetricas"), 
     xlab = Var, col = "green", border = "black", breaks = 20)
```

```{r}
#v15
Var<-"V15"
hist(credit.Datos.Train$V15, 
     main = c("Histograma de ",Var),  # Título del gráfico
     xlab = Var,                # Etiqueta del eje X
     col = "blue",                 # Color de las barras
     border = "black",             # Color del borde de las barras
     breaks = 20)
preProcValuesBC <- preProcess(credit.Datos.Train.Asimetricas[Var], method = "YeoJohnson")
credit.Datos.Train.Asimetricas[Var] <- predict(preProcValuesBC, credit.Datos.Train.Asimetricas[Var])
credit.Datos.Test.Asimetricas[Var] <- predict(preProcValuesBC, credit.Datos.Test.Asimetricas[Var])

hist(credit.Datos.Train.Asimetricas$V15, 
     main = paste("Histograma de", Var, "en credit.Datos.Train.Asimetricas"), 
     xlab = Var, col = "green", border = "black", breaks = 20)
```

Si queremos que no aplique comentar lo siguiente:

```{r}
#credit.Datos.Train.Svm <- credit.Datos.Train.Asimetricas
#credti.datos.Test.Svm <- credit.Datos.Test.Asimetricas
```

#### 2.2 Normalización

```{r}
credit.Var.Salida.Usada <- c("V16")
credit.Var.Entrada.Usadas <- setdiff(names(credit.Datos.Train.Svm),credit.Var.Salida.Usada)
```

##### Usando Center-Scale

```{r}
credit.preProc.CS.Mod<-preProcess(credit.Datos.Train.Svm[credit.Var.Entrada.Usadas],
                                  method=c("center","scale"))
credit.Datos.Train.CS <- predict(credit.preProc.CS.Mod,credit.Datos.Train.Svm)
credit.Datos.Test.CS <- predict(credit.preProc.CS.Mod,credit.Datos.Test.Svm)
```

```{r}
#credit.Datos.Train.Svm <- credit.Datos.Train.CS
#credit.Datos.Test.Svm <- credit.Datos.Test.CS
```

##### Usando Range

```{r}
credit.preProc.CS.Mod<-preProcess(credit.Datos.Train.Svm[credit.Var.Entrada.Usadas],
                                  method=c("range"))
credit.Datos.Train.Rg <- predict(credit.preProc.CS.Mod,credit.Datos.Train.Svm)
credit.Datos.Test.Rg <- predict(credit.preProc.CS.Mod,credit.Datos.Test.Svm) 
```

```{r}
#credit.Datos.Train.Svm <- credit.Datos.Train.Rg
#credit.Datos.Test.Svm <- credit.Datos.Test.Rg
```

#### 2.3 Dummy

Esta transformación de los datos debera emplearse siempre para el correcto funcionamiento del modelo.

```{r}
# Definir la variable de salida y las variables de entrada
credit.Var.Salida.Usada <- "V16"
credit.Vars.Entrada.Usadas <- setdiff(names(credit.Datos.Train), credit.Var.Salida.Usada)
```

```{r}
# Convertir la variable de salida a factor (para clasificación)
credit.Datos.Train.Svm[[credit.Var.Salida.Usada]] <- as.factor(credit.Datos.Train.Svm[[credit.Var.Salida.Usada]])
credit.Datos.Test.Svm[[credit.Var.Salida.Usada]] <- as.factor(credit.Datos.Test.Svm[[credit.Var.Salida.Usada]])

# Crear el modelo dummy solo para las variables de entrada
dummy_model <- dummyVars(~ ., data = credit.Datos.Train.Svm[, credit.Vars.Entrada.Usadas], fullRank = TRUE)

# Transformar las variables de entrada a dummies (sin tocar la salida)
credit.Datos.Train.Dummies <- data.frame(predict(dummy_model, credit.Datos.Train.Svm[, credit.Vars.Entrada.Usadas]))
credit.Datos.Test.Dummies <- data.frame(predict(dummy_model, credit.Datos.Test.Svm[, credit.Vars.Entrada.Usadas]))

# Añadir la variable de salida de nuevo a los conjuntos transformados
credit.Datos.Train.Dummies[[credit.Var.Salida.Usada]] <- credit.Datos.Train.Svm[[credit.Var.Salida.Usada]]
credit.Datos.Test.Dummies[[credit.Var.Salida.Usada]] <- credit.Datos.Test.Svm[[credit.Var.Salida.Usada]]

credit.Datos.Train.Svm <- credit.Datos.Train.Dummies
credit.Datos.Test.Svm <- credit.Datos.Test.Dummies

```

#### 2.4 Eliminar variables con poca varianza

Este paso también debe realizarse en todos los prepocesados que apliquemos.

```{r}
nzv.Train <- nearZeroVar(credit.Datos.Train.Svm)
nzv.Train
credit.Datos.Train.Svm <- credit.Datos.Train.Svm[, -nzv.Train]
credit.Datos.Test.Svm <- credit.Datos.Test.Svm[, names(credit.Datos.Train.Svm)]
```

#### 2.5 Transformaciones de relaciones entre variables

##### PCA

```{r}
credit.PreProc.Pca.Mod <- preProcess(credit.Datos.Train.Svm, method = "pca", thresh = 0.95)  # 95% de la varianza explicada
credit.PreProc.Pca.Mod
credit.Datos.Train.Pca <- predict(credit.PreProc.Pca.Mod, credit.Datos.Train.Svm)
credit.Datos.Test.Pca <- predict(credit.PreProc.Pca.Mod, credit.Datos.Test.Svm)
```

```{r}
credit.Datos.Train.Svm <- credit.Datos.Train.Pca
credit.Datos.Test.Svm <- credit.Datos.Test.Pca
```

##### ICA

```{r}
credit.PreProc.Ica.Mod <- preProcess(credit.Datos.Train.Svm, method = "ica", n.comp = 10)  # 10 componentes independientes
credit.PreProc.Ica.Mod
credit.Datos.Train.Ica <- predict(credit.PreProc.Ica.Mod, credit.Datos.Train.Svm)
credit.Datos.Test.Ica <- predict(credit.PreProc.Ica.Mod, credit.Datos.Test.Svm)
```

```{r}
#credit.Datos.Train.Svm <- credit.Datos.Train.Ica
#credit.Datos.Test.Svm <- credit.Datos.Test.Ica
```

#### 2.6 Entrenamiento

```{r}
set.seed(1234)
credit.modelo.svm <- train(
  V16 ~ .,
  data = credit.Datos.Train.Svm,                    # Datos de entrenamiento
  method = "svmRadial",                             
  trControl = trainControl(method = "cv", number = 10),   # Validación cruzada d
  tuneLength = 20                                         # Probar diferentes valores de parámetros
)
```

El error de entrenamiento del modelo

```{r}
train_pred <- predict(credit.modelo.svm, credit.Datos.Train.Svm)
conf_matrix <- confusionMatrix(train_pred, credit.Datos.Train.Svm$V16)
accuracy <- conf_matrix$overall['Accuracy']
accuracy
```

El error final del modelo sería:

```{r}
#Mostrar porcentaje final
# Realizar predicciones en el conjunto de prueba
predicciones_test <- predict(credit.modelo.svm, credit.Datos.Test.Svm)

# Generar la matriz de confusión
matriz_confusion <- confusionMatrix(predicciones_test, credit.Datos.Test.Svm[[credit.Var.Salida.Usada]])

# Extraer el porcentaje de acierto
porcentaje_acierto <- matriz_confusion$overall["Accuracy"] * 100

# Mostrar el porcentaje de acierto
print(paste("Porcentaje de acierto:", round(porcentaje_acierto, 2), "%"))
```

#### 2.7 Hiper-parámetros

Tras probar 20 combinaciones de hiper-parametros vamos a cuales han sido los que finalmente se han elegido para el modelo final.

```{r}
modelLookup(("svmRadial"))
```

El parametro C es el parametro de penalización, cuanto más altos sean los valores que este tome más estricto será, dando prioridad a clasificar correctamente todos los puntos. Con valores mas pequeños permite un margen más amplio, tolerando así más errores de clasificación.

El parametro sigma, al igual que C cuanto mayor es su valor más cercanos serán los puntos considerados frontera de decisión y viceversa.

<https://stackabuse.com/understanding-svm-hyperparameters/>

```{r}
credit.modelo.svm$bestTune
```

#### 2.8 **¿Con qué configuración hemos hecho los entrenamientos?**

Como ya hemos dicho para que el modelo SVM funcione es necesario haber transformado las variables categoricas y haber eliminado las variables de poca varianza, es por esto q a esa configuracion la llamaremos 'Básica'.

+-----------------------------------+---------------+
| Configuración                     | Acierto       |
+===================================+===============+
| Básica                            | ```           |
|                                   | 85.4 %        |
|                                   | ```           |
+-----------------------------------+---------------+
| Básica + Asimetricas              | ```           |
|                                   | 44.53 %       |
|                                   | ```           |
+-----------------------------------+---------------+
| Básica + Norm. Center-Scale       | ```           |
|                                   | 85.4 %        |
|                                   | ```           |
+-----------------------------------+---------------+
| Básica + Norm. Range              | ```           |
|                                   | 85.4 %        |
|                                   | ```           |
+-----------------------------------+---------------+
| Básica + PCA                      | ```           |
|                                   | 86.13 %       |
|                                   | ```           |
+-----------------------------------+---------------+
| Básica + ICA                      | ```           |
|                                   | 82.48 %       |
|                                   | ```           |
+-----------------------------------+---------------+
| Básica + Norm. Center-Scale + PCA | ```           |
|                                   | 86.13 %       |
|                                   | ```           |
+-----------------------------------+---------------+

Tras las ejecuciones consideramos que para los datos para el modelo SVM haremos dummys, eliminaremos las variables con poca varianza y aplicaremos PCA.

#### 2.9 Submuestreo con clases desvalanceadas

Para ver si nuestro modelo mejora vamos a tratar las clases desvalanceadas en el conjunto de los datos de entrenamiento.

```{r}
credit.Var.Salida.Usada <- c("V16")
credit.Vars.Entrada.Usadas <- setdiff(names(credit.Datos.Train.Svm),credit.Var.Salida.Usada)
```

##### down-sampling:

Consiste en reducir el n´umero de ejemplos de las clases m´as frecuentes para igualar la clase menos frecuente.

```{r}
set.seed(12345)
credit.Datos.Train.svm.downsmpld<-downSample(x=credit.Datos.Train.Svm[credit.Vars.Entrada.Usadas],
                                       y=credit.Datos.Train.Svm[[credit.Var.Salida.Usada]],
                                       yname=credit.Var.Salida.Usada)
credit.Datos.Test.svm.downsmpld <- credit.Datos.Test.Svm
```

##### up-sampling:

En este caso hace lo contrario, remuestrea (con reemplazamiento) la clase minoritaria para igualarla a la mayoritaria (repite varios datos).

```{r}
set.seed(1234)
credit.Datos.Train.svm.upsmpld<-upSample(x=credit.Datos.Train.Svm[credit.Vars.Entrada.Usadas],
                                   y=credit.Datos.Train.Svm[[credit.Var.Salida.Usada]],
                                   yname=credit.Var.Salida.Usada)
credit.Datos.Test.svm.upsmpld<- credit.Datos.Test.Svm
```

Creamos y probamos un modelo para cada una de estas configuraciones:

```{r}
set.seed(1234)
credit.modelo.svm.downsmpld <- train(
  V16 ~ .,  # Variable de salida ~ Variables de entrada
  data = credit.Datos.Train.svm.downsmpld,                                    # Datos de entrenamiento
  method = "svmRadial",                                          # Especificamos el método SVM con kernel radial
  trControl = trainControl(method = "cv", number = 10),           # Validación cruzada de 5 pliegues
  tuneLength = 10                                                # Probar diferentes valores de parámetros
)
#Mostrar porcentaje final
# Realizar predicciones en el conjunto de prueba
predicciones_test <- predict(credit.modelo.svm.downsmpld, credit.Datos.Test.svm.downsmpld)

# Generar la matriz de confusión
matriz_confusion <- confusionMatrix(predicciones_test, credit.Datos.Test.svm.downsmpld[[credit.Var.Salida.Usada]])

# Extraer el porcentaje de acierto
porcentaje_acierto <- matriz_confusion$overall["Accuracy"] * 100

# Mostrar el porcentaje de acierto
print(paste("Porcentaje de acierto:", round(porcentaje_acierto, 2), "%"))
```

```{r}
set.seed(1234)
credit.modelo.svm.upsmpld <- train(
  V16 ~ .,  # Variable de salida ~ Variables de entrada
  data = credit.Datos.Train.svm.upsmpld,                                    # Datos de entrenamiento
  method = "svmRadial",                                          # Especificamos el método SVM con kernel radial
  trControl = trainControl(method = "cv", number = 10),           # Validación cruzada de 5 pliegues
  tuneLength = 10                                                # Probar diferentes valores de parámetros
)
#Mostrar porcentaje final
# Realizar predicciones en el conjunto de prueba
predicciones_test <- predict(credit.modelo.svm.upsmpld, credit.Datos.Test.svm.upsmpld)

# Generar la matriz de confusión
matriz_confusion <- confusionMatrix(predicciones_test, credit.Datos.Test.svm.upsmpld[[credit.Var.Salida.Usada]])

# Extraer el porcentaje de acierto
porcentaje_acierto <- matriz_confusion$overall["Accuracy"] * 100

# Mostrar el porcentaje de acierto
print(paste("Porcentaje de acierto:", round(porcentaje_acierto, 2), "%"))
```

#### 2.10 Eliminación de variables

```{r}
#Ver importancia de las variables de entrada
#Metodo -> varImp()
varImp(credit.modelo.svm)
#Ver seleccion de variables del modelo 
#Metodo -> predictors(modelo$finalModel)
predictors(credit.modelo.svm$finalModel)
```

Como vemos la variable PC5 no tiene importancia a la hora de realizar la clasificación. Vamos a probar a eliminarla para ver si nuestro modelo mejora.

```{r}
credit.Datos.Train.svm.Reduc <- credit.Datos.Train.Svm
credit.Datos.Test.svm.Reduc <- credit.Datos.Test.Svm
credit.Datos.Train.svm.Reduc$PC5 <- NULL
credit.Datos.Test.svm.Reduc$PC5 <- NULL
```

```{r}
set.seed(1234)
credit.modelo.svm.Reduc <- train(
  V16 ~ .,  # Variable de salida ~ Variables de entrada
  data = credit.Datos.Train.svm.Reduc,                                    # Datos de entrenamiento
  method = "svmRadial",                                          # Especificamos el método SVM con kernel radial
  trControl = trainControl(method = "cv", number = 10),           # Validación cruzada de 5 pliegues
  tuneLength = 10                                                # Probar diferentes valores de parámetros
)

predicciones_test <- predict(credit.modelo.svm.Reduc, credit.Datos.Test.svm.Reduc)

# Generar la matriz de confusión
matriz_confusion <- confusionMatrix(predicciones_test, credit.Datos.Test.svm.Reduc[[credit.Var.Salida.Usada]])

# Extraer el porcentaje de acierto
porcentaje_acierto <- matriz_confusion$overall["Accuracy"] * 100

# Mostrar el porcentaje de acierto
print(paste("Porcentaje de acierto:", round(porcentaje_acierto, 2), "%"))
```

La eliminacion de esta variable, a pesar de que no tiene imporatancia en el modelo final enternado no mejora el rendiminto.

#### 2.11 Conclusión

Para la configuración de nuestro modelo SVM, el mejor resultado lo obtienen los datos tras haber hecho Dummys, eliminado las variables con poca varianza y habiendo aplicado PCA. Tras esto eliminamos PC5

El modelo final sería:

```{r}
print(credit.modelo.svm.Reduc)
```

### 3. KNN

K-Nearest Neighbors es un algoritmo que clasifica o predice en función de los k vecinos mas cercanos. En nuestro caso, nos interesa la clasificación. Debido a como funciona KNN será fundamental la normalización de los datos asi como transformar las variables categoricas en dummys. No nos centraremos tanto en la reducción de los datos y por último realizaremos pruebas eliminando variables poco importantes asi como tratar las clases desvalanceadas o los sesgos.

<https://www.datacamp.com/tutorial/k-nearest-neighbors-knn-classification-with-r-tutorial>

<https://www.datacamp.com/tutorial/preprocessing-in-data-science-part-1-centering-scaling-and-knn>

```{r}
credit.Datos.Train.Knn <- credit.Datos.Train
credit.Datos.Test.Knn <- credit.Datos.Test
```

```{r}
credit.Var.Salida.Usada <- c("V16")
credit.Var.Entrada.Usadas <- setdiff(names(credit.Datos.Train.Knn),credit.Var.Salida.Usada)
```

#### 3.1 Normalización

##### Center-Scale

```{r}
credit.preProc.CS.Mod<-preProcess(credit.Datos.Train.Knn[credit.Var.Entrada.Usadas],
                                  method=c("center","scale"))
credit.Datos.Train.CS <- predict(credit.preProc.CS.Mod,credit.Datos.Train.Knn)
credit.Datos.Test.CS <- predict(credit.preProc.CS.Mod,credit.Datos.Test.Knn)
```

```{r}
credit.Datos.Train.Knn <- credit.Datos.Train.CS
credit.Datos.Test.Knn <- credit.Datos.Test.CS
```

##### Range

```{r}
credit.preProc.CS.Mod<-preProcess(credit.Datos.Train.Knn[credit.Var.Entrada.Usadas],
                                  method=c("range"))
credit.Datos.Train.Rg <- predict(credit.preProc.CS.Mod,credit.Datos.Train.Knn)
credit.Datos.Test.Rg <- predict(credit.preProc.CS.Mod,credit.Datos.Test.Knn) 
```

```{r}
#credit.Datos.Train.Knn <- credit.Datos.Train.Rg
#credit.Datos.Test.Knn <- credit.Datos.Test.Rg
```

#### 3.2 Dummys

```{r}
# Definir la variable de salida y las variables de entrada
credit.Var.Salida.Usada <- "V16"
credit.Vars.Entrada.Usadas <- setdiff(names(credit.Datos.Train.Knn), credit.Var.Salida.Usada)

# Convertir la variable de salida a factor (para clasificación)
credit.Datos.Train.Knn[[credit.Var.Salida.Usada]] <- as.factor(credit.Datos.Train.Knn[[credit.Var.Salida.Usada]])
credit.Datos.Test.Knn[[credit.Var.Salida.Usada]] <- as.factor(credit.Datos.Test.Knn[[credit.Var.Salida.Usada]])

# Crear el modelo dummy solo para las variables de entrada
dummy_model <- dummyVars(~ ., data = credit.Datos.Train.Knn[, credit.Vars.Entrada.Usadas], fullRank = TRUE)

# Transformar las variables de entrada a dummies (sin tocar la salida)
credit.Datos.Train.Dummies <- data.frame(predict(dummy_model, credit.Datos.Train.Knn[, credit.Vars.Entrada.Usadas]))
credit.Datos.Test.Dummies <- data.frame(predict(dummy_model, credit.Datos.Test.Knn[, credit.Vars.Entrada.Usadas]))

# Añadir la variable de salida de nuevo a los conjuntos transformados
credit.Datos.Train.Dummies[[credit.Var.Salida.Usada]] <- credit.Datos.Train.Knn[[credit.Var.Salida.Usada]]
credit.Datos.Test.Dummies[[credit.Var.Salida.Usada]] <- credit.Datos.Test.Knn[[credit.Var.Salida.Usada]]

credit.Datos.Train.Knn <- credit.Datos.Train.Dummies
credit.Datos.Test.Knn <- credit.Datos.Test.Dummies
```

#### 3.3 Eliminar variables con poca varianza

```{r}
nzv.Train <- nearZeroVar(credit.Datos.Train.Knn)
nzv.Train
credit.Datos.Train.Vari <- credit.Datos.Train.Knn[, -nzv.Train]
credit.Datos.Test.Vari <- credit.Datos.Test.Knn[, names(credit.Datos.Train.Knn)]
```

```{r}
#credit.Datos.Train.Knn <- credit.Datos.Train.Vari
#credit.Datos.Test.Knn <- credit.Datos.Test.Vari
```

#### 3.4 Entrenamiento

```{r}
# Configurar el modelo KNN
set.seed(1234)
# Entrenamiento del modelo k-NN
credit.modelo.knn <- train(V16 ~ .,  # Usamos "Class" como variable de salida
                           data = credit.Datos.Train.Knn,  # Datos de entrenamiento
                           method = "knn",  # Especificamos k-NN
                           trControl = trainControl(method = "cv", number = 10),  # Validación cruzada de 10 pliegues
                           tuneLength = 20)

```

El error de entrenamiento del modelo

```{r}
train_pred <- predict(credit.modelo.knn, credit.Datos.Train.Knn)
conf_matrix <- confusionMatrix(train_pred, credit.Datos.Train.Knn$V16)
accuracy <- conf_matrix$overall['Accuracy']
accuracy
```

El error final del modelo sería:

```{r}
#Mostrar porcentaje final
# Realizar predicciones en el conjunto de prueba
predicciones_test <- predict(credit.modelo.knn, credit.Datos.Test.Knn)

# Generar la matriz de confusión
matriz_confusion <- confusionMatrix(predicciones_test, credit.Datos.Test.Knn[[credit.Var.Salida.Usada]])

# Extraer el porcentaje de acierto
porcentaje_acierto <- matriz_confusion$overall["Accuracy"] * 100

# Mostrar el porcentaje de acierto
print(paste("Porcentaje de acierto:", round(porcentaje_acierto, 2), "%"))
```

#### 3.5 Hiper-Parametros

Tras probar 20 combinaciones de hiper-parametros vamos a cuales han sido los que finalmente se han elegido para el modelo final.

```{r}
modelLookup(("knn"))
```

```{r}
credit.modelo.knn$bestTune
```

#### 3.6 **¿Con qué configuración hemos hecho los entrenamientos?**

+------------------------------------------+---------------+
| Configuración                            | Acierto       |
+==========================================+===============+
| Dummy                                    | ```           |
|                                          | 71.53 %       |
|                                          | ```           |
+------------------------------------------+---------------+
| Dummy + Asimetricas                      | ```           |
|                                          | 82.48 %       |
|                                          | ```           |
+------------------------------------------+---------------+
| Dummy + Norm. Center-Scale               | ```           |
|                                          | 86.86 %       |
|                                          | ```           |
+------------------------------------------+---------------+
| Dummy + Norm. Range                      | ```           |
|                                          | 87.59 %       |
|                                          | ```           |
+------------------------------------------+---------------+
| Dummy + Norm. Range + Norm. Center-Scale | ```           |
|                                          | 87.59 %       |
|                                          | ```           |
+------------------------------------------+---------------+
| Dummy + Eliminar poca varianza           | ```           |
|                                          | 71.53 %       |
|                                          | ```           |
+------------------------------------------+---------------+
| Dummy+ Norm. Center-Scale + Asimetricas  | ```           |
|                                          | 84.67 %       |
|                                          | ```           |
+------------------------------------------+---------------+
| Dummy+ Norm. Center-Scale + PCA          | ```           |
|                                          | 80.29 %       |
|                                          | ```           |
+------------------------------------------+---------------+

#### 3.7 Submuestreo con clases desvalanceadas

Para ver si nuestro modelo mejora vamos a tratar las clases desvalanceadas en el conjunto de los datos de entrenamiento.

```{r}
credit.Var.Salida.Usada <- c("V16")
credit.Vars.Entrada.Usadas <- setdiff(names(credit.Datos.Train.Knn),credit.Var.Salida.Usada)
```

##### down-sampling:

Consiste en reducir el n´umero de ejemplos de las clases m´as frecuentes para igualar la clase menos frecuente.

```{r}
set.seed(12345)
credit.Datos.Train.knn.downsmpld<-downSample(x=credit.Datos.Train.Knn[credit.Vars.Entrada.Usadas],
                                       y=credit.Datos.Train.Knn[[credit.Var.Salida.Usada]],
                                       yname=credit.Var.Salida.Usada)
credit.Datos.Test.knn.downsmpld <- credit.Datos.Test.Knn
```

##### up-sampling:

En este caso hace lo contrario, remuestrea (con reemplazamiento) la clase minoritaria para igualarla a la mayoritaria (repite varios datos).

```{r}
set.seed(1234)
credit.Datos.Train.knn.upsmpld<-upSample(x=credit.Datos.Train.Knn[credit.Vars.Entrada.Usadas],
                                   y=credit.Datos.Train.Knn[[credit.Var.Salida.Usada]],
                                   yname=credit.Var.Salida.Usada)
credit.Datos.Test.knn.upsmpld<- credit.Datos.Test.Knn
```

Creamos y probamos un modelo para cada una de estas configuraciones:

```{r}
set.seed(1234)
credit.modelo.knn.downsmpld <- train(
  V16 ~ .,  # Variable de salida ~ Variables de entrada
  data = credit.Datos.Train.knn.downsmpld,                                    # Datos de entrenamiento
  method = "svmRadial",                                          # Especificamos el método SVM con kernel radial
  trControl = trainControl(method = "cv", number = 10),           # Validación cruzada de 5 pliegues
  tuneLength = 10                                                # Probar diferentes valores de parámetros
)
#Mostrar porcentaje final
# Realizar predicciones en el conjunto de prueba
predicciones_test <- predict(credit.modelo.knn.downsmpld, credit.Datos.Test.knn.downsmpld)

# Generar la matriz de confusión
matriz_confusion <- confusionMatrix(predicciones_test, credit.Datos.Test.knn.downsmpld[[credit.Var.Salida.Usada]])

# Extraer el porcentaje de acierto
porcentaje_acierto <- matriz_confusion$overall["Accuracy"] * 100

# Mostrar el porcentaje de acierto
print(paste("Porcentaje de acierto:", round(porcentaje_acierto, 2), "%"))
```

```{r}
set.seed(1234)
credit.modelo.knn.upsmpld <- train(
  V16 ~ .,  # Variable de salida ~ Variables de entrada
  data = credit.Datos.Train.knn.upsmpld,                                    # Datos de entrenamiento
  method = "svmRadial",                                          # Especificamos el método SVM con kernel radial
  trControl = trainControl(method = "cv", number = 10),           # Validación cruzada de 5 pliegues
  tuneLength = 10                                                # Probar diferentes valores de parámetros
)
#Mostrar porcentaje final
# Realizar predicciones en el conjunto de prueba
predicciones_test <- predict(credit.modelo.knn.upsmpld, credit.Datos.Test.knn.upsmpld)

# Generar la matriz de confusión
matriz_confusion <- confusionMatrix(predicciones_test, credit.Datos.Test.knn.upsmpld[[credit.Var.Salida.Usada]])

# Extraer el porcentaje de acierto
porcentaje_acierto <- matriz_confusion$overall["Accuracy"] * 100

# Mostrar el porcentaje de acierto
print(paste("Porcentaje de acierto:", round(porcentaje_acierto, 2), "%"))
```

#### 3.8 Eliminación de variables

```{r}
#Ver importancia de las variables de entrada
#Metodo -> varImp()
varImp(credit.modelo.knn)
#Ver seleccion de variables del modelo 
#Metodo -> predictors(modelo$finalModel)
predictors(credit.modelo.knn$finalModel)
```

#### 3.9 Conclusión
