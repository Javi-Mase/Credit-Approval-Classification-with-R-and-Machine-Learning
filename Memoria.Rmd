---
title: "Practica final Aprendizaje Computacional"
author: "Manuel Francisco Hidalgo Ros Javier García Masegosa Javier Prior Gomez"
output:
  html_document:
    df_print: paged
    highlight: kate
    number_sections: yes
    theme: paper
    toc: yes
    toc_float: yes
---

```{r, echo=FALSE, include=FALSE}


# No se que hace esta linea
knitr::opts_chunk$set(echo = TRUE)

# Lista de paquetes necesarios
paquetes <- c("base", "caret", "datasets", "ggplot2", "graphics", "grDevices", "lattice", "methods", "stats", "utils","randomForest", "gridExtra")

# Comprobar paquetes faltantes
paquetes_faltantes <- paquetes[!(paquetes %in% installed.packages()[,"Package"])]

# Instalar paquetes faltantes
if (length(paquetes_faltantes) > 0) {
  install.packages(paquetes_faltantes)
}

# Cargar todos los paquetes
lapply(paquetes, library, character.only = TRUE)

# Cargamos la base de datos, na.string = "?" quitamos los datos con ese valor y lo sustituimos por NA
credit <- read.table("crx.data", header = FALSE, sep = ",", na.strings = "?")

# Cargamos en credit.trainIdx la base de datos descargada del UCI
credit.trainIdx <- readRDS("credit.trainIdx.rds")
credit.Datos.Train <- credit[credit.trainIdx,]
credit.Datos.Test <- credit[-credit.trainIdx,]

# Crear un indicador para cada conjunto
credit.Datos.Train$Origen <- "train"
credit.Datos.Test$Origen <- "test"

# Combinar los conjuntos
combined_credit <- rbind(credit.Datos.Train, credit.Datos.Test)
```



# Introducción

El documento ha cargado automáticamente la base de datos, ahora vamos a ir haciendo un preprocesado de los datos, primero el siguiente comando muestra la cantidad de valores NA por columna:

```{r}
colSums(is.na(credit))
```

Esto es por si alguna columna tiene demasiados valores NA para eliminarla

Convertir automáticamente columnas de tipo 'chr' a 'factor'

```{r}
credit[sapply(credit, is.character)] <- lapply(credit[sapply(credit, is.character)], as.factor)
```

Despues de esto empezamos especulando el significado de cada uno de las variables. Esto lo vamos a hacer buscando en internet el significado esperado de cada una, haciendo suposiciones y comprobando que el análisis de cada una de las variables sea coherente.

# Análisis de Variables

## **V1**

Se trata de una variable categórica, con valores en el dominio `{a, b}`. Tiene un significado que podemos intuir de forma relativamente fácil. De hecho, podemos pensar que "a" se trata de mujeres y "b" de hombres. Esto se debe a que la base es de los años 80, y en esa época era más común que los hombres soliciten créditos, y en el caso de las mujeres, seguramente las que lo solicitaban eran las que estaban solteras. Se trata de una época con pensamientos cerrados, y esto es un simple estudio estadístico.

```{r}
# Eliminamos niveles no usados
credit$V1 <- droplevels(credit$V1)
summary(credit$V1)
```

Como podemos comprobar nuestra suposición puede ser cierta.

## **V2**

Esta columna puede ser la edad del cliente. Es continuo ya que los días los puede estar teniendo en cuenta de alguna forma. Sin embargo, en Estados Unidos los menores no pueden solicitar un crédito y hay algunos valores menores de 18. Vamos a calcular cuantos:

```{r}
num_menores_18 <- sum(credit$V2 < 18, na.rm = TRUE)
total_valores <- sum(!is.na(credit$V2))
porcentaje_menores_18 <- (num_menores_18 / total_valores) * 100
porcentaje_menores_18
```

Comprobamos que aproximadamente el 5% de los clientes son menores de edad, al no ser una cifra muy significativa, podemos asumir que son valores fuera de rango, y que efectivamente podríamos estar ante la variable de edad. En siguientes apartados veremos qué hacer con estos valores.


Decidimos que es interesante analizar V2 ya que su media y mediana son parecidas, lo cual sugiere que la distribución puede ser simétrica (cercana a normal).

Generamos un histograma base con la función hist() de R para la variable V2 con probability = TRUE para mostrar la densidad en vez de las frecuencias absolutas.

```{r}
hist(credit$V2, probability = TRUE, main = "Histograma de V2 con Curva de Densidad")
```


Añadimos una línea de densidad para observar la forma de la distribución de V2, usando la función lines() y especificando el color "blue" para la curva.
```{r}
lines(density(credit$V2, na.rm = TRUE), col = "blue")
```

Creamos un histograma con ggplot2 para la variable V2 del dataframe 'credit', estableciendo aesthetic mapping (aes) para usar V2 en el eje x y ademas y añadimos un histograma con color de contorno naranja y relleno naranja claro (alpha = 0.2) y definimos intervalos de 5 unidades usando el argumento breaks.

```{r}
myHist = ggplot(data = credit, aes(credit$V2))
  + geom_histogram(col = "orange", fill = "orange", alpha = 0.2, breaks = seq(0, 80, by = 5))
  + labs(title = "Histograma para la variable V2 con línea de densidad")
```

Añadimos una línea vertical en el histograma de ggplot para marcar la media de V2 usando geom_vline() y especificando la posición con xintercept = mean() y hacemos lo mismo para marcar la mediana.

```{r}
myHist = myHist + geom_vline(xintercept = mean(credit$V2, na.rm = TRUE), col = "blue")
myHist = myHist + geom_vline(xintercept = median(credit$V2, na.rm = TRUE), col = "red")
```

Mostramos el gráfico final en ggplot2 con el histograma de V2, junto con las líneas de media (azul) y mediana (roja).

```{r}
myHist
```

Como vemos, V2 no sigue exactamente una distribución normal, ya que la media y la mediana no son idénticas, lo que indica una posible asimetría en los datos.

Para confirmar formalmente la falta de normalidad, realizaremos un gráfico Q-Q (Quantile-Quantile), que nos permite comparar la distribución de V2 con la distribución teórica normal.


```{r}

# Creamos el gráfico Q-Q para V2 utilizando ggplot2.
p1 = ggplot(data = credit, aes(sample = V2)) +
  # Añadimos un título al gráfico Q-Q.
  ggtitle("QQ plot para V2") +
  # Añadimos los puntos del gráfico Q-Q, que compara los cuantiles de la muestra de V2 
  # con los cuantiles teóricos de una distribución normal.
  geom_qq() + 
  # Añadimos la línea Q-Q teórica (stat_qq_line), que muestra cómo se deberían 
  # alinear los puntos si la distribución de V2 fuera normal.
  stat_qq_line() + 
  # Etiquetas para los ejes: 'Distribución teórica' en el eje x y 'Distribución muestral' en el eje y.
  xlab("Distribución teórica") + 
  ylab("Distribución muestral")
```

Mostramos el gráfico Q-Q, que permite ver si los puntos se alinean (lo que indicaría normalidad) o si se desvían de la línea (indicando falta de normalidad en la distribución de V2).

```{r}
p1
```

Como vemos hay una desviación de la diagonal. Por tanto, podemos concluir que no se trata de una distribución normal.


## **V3**

Nuestra especulación en cuanto a esta columna es que se trata de la cantidad de deuda que tienen los clientes. Sin embargo, no tiene sentido que el que más debe, el total sea 28\$:

```{r}
summary(credit$V3)
```

Por ello, pensamos que pueda estar dividida por $10^{3}$ o similar. De igual manera no nos importa la proporción que lleve.

## **V4**

En esta columna podríamos estar ante el estado civil de cada uno de los clientes. Siendo cada uno de los valores {l=desconocido, u=casados, y=solteros, t=otros}. Si calculamos el porcentaje de clientes casados:

```{r}
num_mayores_18 <- sum(credit$V2 > 18, na.rm = TRUE)
num_casados <- sum(credit$V4 == "u", na.rm = TRUE)
porcent_casados <- (num_casados/num_mayores_18) * 100
porcent_casados
```

Si buscamos en internet en torno al 70% de la población adulta en Estados Unidos estaba casada. Como podemos ver se aproxima mucho a nuestro resultado. Esa pequeña diferencia puede deberse a que muchos matrimonios al casarse, compran una casa, y piden un préstamo para ello.


ESTO ES NECESARIO:

```{r}
# Eliminamos niveles no usados
credit$V4 <- droplevels(credit$V4)

# Ahora vamos a añadir un elemento que no aparece y luego lo hacemos factor
levels(credit$V4)<-c(levels(credit$V4),"t")
str(credit$V4)


```


## **V6**

Analizamos la variable categórica V6 para obtener una visión general de su distribución y composición. Mostramos un resumen estadístico de V6, que nos indica la frecuencia de cada categoría en esta variable.

```{r}
summary(credit$V6)
```

Verificamos la estructura de V6 para confirmar que es una variable categórica (factor o carácter).

```{r}
str(credit$V6)
```


Calculamos el porcentaje de cada categoría en V6, para ello primero, obtenemos la frecuencia de cada categoría con table(credit$V6) y luego, usamos prop.table() para calcular la proporción relativa de cada categoría y multiplicamos por 100 para expresarlo en porcentaje.

```{r}
porcent <- prop.table(table(credit$V6)) * 100
```

Creamos una tabla que combina tanto el número total de observaciones (frecuencia) como el porcentaje de cada categoría: cbind() se usa para unir la frecuencia y el porcentaje en una tabla única.

```{r}
porcent_table <- cbind(total = table(credit$V6), porcentaje = porcent)
```

Mostramos el vector de porcentajes, lo que nos permite visualizar el porcentaje de cada categoría en V6.

```{r}
porcent
```

Ahora que entendemos la distribución de frecuencias y porcentajes de cada categoría, generamos un gráfico de sectores para visualizar estos porcentajes de manera gráfica y comparativa.

Creamos el gráfico de sectores (o "quesos") usando la función pie(), pasándole el vector de porcentajes.
Establecemos un título con el argumento main y aplicamos diferentes colores a cada categoría usando rainbow().

```{r}
pie(porcent, main = "Diagrama de Quesos para V6", col = rainbow(length(porcent)))
```

Mostramos nuevamente el vector de porcentajes para recordar los valores antes de proceder con el análisis de los valores NA.

```{r}
porcent
```


Calculamos la suma del resumen de V6 para confirmar el número total de observaciones, incluyendo posibles NA.

```{r}
sum(summary(credit$V6))
```

Calculamos el porcentaje de valores NA en la variable V6, asumiendo que hay 9 valores NA de un total de 690 observaciones.

```{r}
porcentaje_na <- 9 / 690 * 100
porcentaje_na
```

Concluimos que, siendo aproximadamente un 1.3% de valores faltantes (NA), la eliminación de estas observaciones es razonable, ya que este porcentaje es bajo y es poco probable que afecte significativamente el análisis.


## **V8**

Esta variable creemos que puede ser los años de empleo. Si nos fijamos en los valores de las columnas de V8 (años contratado) y las comparamos por filas con las de V2 (edad) nos podemos dar cuenta que nunca va a superar la edad. De hecho en muchos casos tiene coherencia la edad con el número de años contratado. Esto puede ser un claro indicativo que estamos ante una buena especulación. Aún así vamos a verlo en R:

```{r}
summary(credit$V8)
```

Podemos apreciar que el valor máximo no es muy grande. Esto nos puede despistar un poco, pero basta con informarnos un poco sobre la variable edad:

```{r}
summary(credit$V2)
```

¿Aporta algo este resumen? Por supuesto, de hecho tenemos la razón de que nuestra variable de años contratados presente datos tan bajos. Estamos ante un conjunto de clientes jóvenes, y como es lógico no tienen muchos años como empleados.

## **V10**

Esta variable hemos podido intuir junto a alguna investigaciones que representa si un cliente está actualmente empleado. Esto es muy interesante de cara al análisis por temas obvios de solvencia. Suponemos que {f=empleado, t=no empleado}. Pero, ¿realmente es consistente la cifra de empleados? Sí, lo vemos con:

```{r}
porcentajes_empleados <- prop.table(summary(credit$V10))*100
porcentajes_empleados
```

Según hemos podido encontrar en los años ochenta en Estados Unidos había un 60% de población activa. Esto es de gran utilidad para hacer una correlación junto a V16 (aprobada/rechazada).

## **V11**

Esta variable es posible que sea \`\`credit score", que es es una expresión numérica que representa la solvencia de un individuo. Según hemos podido comprobar también ha sido retocada (los valores), ya que los valores no siguen la notación habitual. Pero sí sabemos que cuanto mayor es el valor, mayor es la solvencia. En la Figura\~\ref{fig:credit_score} podemos encontrar un gráfico interesante que ilustra este concepto:

![Descripción de la imagen](imagenes/creditScorepx.png)

Es importante conocer la forma que van a tener nuestros datos de esta variable:

```{r}
hist(credit$V11, 
     breaks = seq(min(credit$V11, na.rm = TRUE), max(credit$V11, na.rm = TRUE), by = 1),
     main = "Histograma de Credit Score V11",
     xlab = "Valores de V11", 
     ylab = "Frecuencia",
     col = "lightblue", 
     border = "black")
```

La mediana indica que hay muchos ceros. Y con el histograma vemos que esto efectivamente es así:

grafico v11

Lo deberíamos de tener en cuenta a la hora de hacer el análisis multivariable.

## **V13**

Pasamos a analizar la última variable interesante para nuestro análisis. Esta variable representa composición de la población, clasificando en: cuidadano, residente permenente o inmigrante. Según consta en los datos, la distribución es 86%, 2.6% y 6.2%. ¿Se aproximará?

```{r}
prop.table(summary(credit$V13))*100
```

Como vemos son muy parecidos. Esto es un indicativo significativo. Concluimos que {g=ciudadano, p=residente permanente, s=inmigrante}.

## Razonamiento para comparar V14 y V16

Una razonamiento útil que podemos hacer es relacionar los códigos postales con la aprobación de los créditos solicitados. Suponemos que los códigos postales han sidos modificados de alguna manera para proteger la privacidad de los datos. Esto lo hacemos porque dependiendo de la ubicación de los solicitantes, si están en barrios más ricos, o barrios más humildes, podemos hacer una distinción de gente con más poder adquisitivo, que por lo general, representarán mayor parte de solicitudes aprobadas.

```{r}
library(ggplot2)
    ggplot(credit, aes(x = V16, y = V14, fill = V16)) + 
        geom_violin() +
        labs(title = "Distribución de V14 (Edad) por V16 (Decisión de Crédito)",
            x = "Decisión de Crédito (V16)", 
            y = "Edad (V14)") +
    theme_minimal() +
    scale_fill_manual(values = c("skyblue", "orange"))
```

## **V15**

Vamos a analizar la variable V15, que parece tener una distribución con muchos valores cercanos a 0 pero también valores atípicos elevados, lo que podría afectar la media.

Realizamos un resumen estadístico de la variable V15 para obtener una visión general de los valores, incluyendo mínimos, máximos, media y mediana.

```{r}
summary(credit$V15)
```

Generamos un histograma básico para observar la distribución de V15. Usamos probability = TRUE para mostrar el histograma en términos de densidad.

```{r}
hist(credit$V15, probability = TRUE,
     main = "Histograma de V15 con Curva de Densidad")
```

Añadimos una curva de densidad a la gráfica para visualizar mejor la forma de la distribución, usando la función density() con na.rm = TRUE para excluir valores NA.

```{r}
lines(density(credit$V15, na.rm = TRUE), col = "blue")
```


Observamos que muchos valores altos de V15 hacen difícil ver los valores menores. Para solucionar esto, usamos ggplot2 para crear un histograma que enfoque la escala en valores menores, estableciendo los intervalos hasta 1000 (ajustable según el rango de interés).

Creamos un histograma de V15 con ggplot2, aplicando intervalos de 10 en el rango de 0 a 1000.

```{r}
myHist = ggplot(data = credit, aes(credit$V15)) +
  geom_histogram(col = "orange", fill = "orange", alpha = 0.2, breaks = seq(0, 1000, by = 10)) +
  labs(title = "Histograma para la variable V15 con línea de densidad")
```

Añadimos una línea vertical azul en el histograma para indicar la media de V15, utilizando mean() y especificando que se ignoren valores NA.

```{r}
myHist = myHist + geom_vline(xintercept = mean(credit$V15, na.rm = TRUE), col = "blue")
```

Añadimos una línea vertical roja para indicar la mediana de V15, usando median().

```{r}
myHist = myHist + geom_vline(xintercept = median(credit$V15, na.rm = TRUE), col = "red")
```

Mostramos el gráfico final en ggplot2, que incluye el histograma de V15 y las líneas de media y mediana.

```{r}
myHist
```

Al observar el histograma, vemos que la mayoría de los valores están cerca de 0, lo que indica una acumulación de valores bajos. Esto se confirma con el resumen estadístico: la media es de 1017.4 y la mediana es 5.0, lo que significa que la mayoría de los valores son pequeños, pero hay algunos valores muy altos (outliers) que elevan la media, creando una diferencia significativa entre media y mediana.

```{r}
p1 = ggplot(data=credit,aes(sample=V15)) +
  ggtitle("QQ plot para V15") +
  geom_qq() + 
  stat_qq_line() + 
  xlab("Distribución teórica") + ylab("Distribución muestral")
p1
```





```{r}
# Multivariable
summary(credit$V15)

library(reshape2)
# creando una estructura de datos para usar lattice con facilidad con "melt()"
library(reshape2)
# creando una estructura de datos para usar lattice con facilidad con "melt()"


```

## **V16** 

Comenzamos con el análisis de la variable categórica V16, que contiene las clases objetivo o categorías para nuestro análisis.

Ahora vamos a renombrar algunas columnas para ganar legibilidad

```{r}
levels(credit$V16) <- c("rechazada", "aprobada")
```

Verificamos la estructura de V16 para confirmar que es de tipo factor y ver las categorías presentes en la variable.

```{r}
str(credit$V16)
```

Calculamos el porcentaje de cada categoría en V16.
Primero, usamos table(credit$V16) para obtener las frecuencias de cada categoría, luego, aplicamos prop.table() para obtener la proporción relativa, multiplicando por 100 para obtener el porcentaje.

```{r}
porcent <- prop.table(table(credit$V16)) * 100
```


Creamos una tabla combinada que incluye tanto el número total (frecuencia) como el porcentaje de cada categoría. Usamos cbind() para unir el total (frecuencia) y el porcentaje en una tabla.

```{r}
porcent_table <- cbind(total = table(credit$V16), porcentaje = porcent)
```

Mostramos el vector de porcentajes, que nos permite ver el porcentaje de cada categoría.

```{r}
porcent
```

Ahora que entendemos la frecuencia y proporción de cada categoría, creamos un diagrama de sectores (o "quesos") para ilustrarlo de forma gráfica.

Creamos el gráfico de sectores con pie(), donde usamos el vector de porcentajes.

```{r}
pie(porcent, main = "Diagrama de Quesos para V16", col = rainbow(length(porcent)))
```

Con este gráfico de sectores, podemos ver visualmente la distribución de cada categoría en V16.
Dado que las categorías tienen frecuencias similares, podemos observar que la distribución es aproximadamente uniforme y no se identifican categorías con frecuencias extremas (outliers).

```{r}

melted_data <- melt(credit, id.vars = "V16", measure.vars = "V2", variable.name = "Variable", value.name = "Value")
melted_data

library(ggplot2)
ggplot(melted_data, aes(x = V16, y = Value)) +
  geom_boxplot() +
  xlab("Categoría de V16") +
  ylab("Valores de V2") +
  ggtitle("Distribución de V2 por Categoría de V16")
```

Observando el boxplot podemos decir que ambos tienen una distribución simétrica. En cuanto a los outliners, no hay en exceso (en rechazada algo más).

Ambas tienen un sesgo positivo, ya que los bigotes superiores tienen mayor longitud. 


```{r}
# Gráfico de densidad ajustado
ggplot(data = melted_data, aes(x = Value, color = V16, fill = V16)) +
  geom_density(alpha = 0.6) +
  scale_fill_discrete() +
  scale_color_discrete() +
  ylab("Densidad") +  # Etiqueta para el eje y
  xlab("Valores") +   # Etiqueta para el eje x
  ggtitle("Densidad de Valores por Especie")  # Título del gráfico

```

El análisis de la distribución de V2 muestra que ambas categorías (rechazada y aprobada) presentan una asimetría positiva, con picos de densidad más altos en valores bajos y una cola extendida hacia la derecha. La curva de las solicitudes rechazadas tiene una densidad más pronunciada cerca de los valores bajos (alrededor de 20), mientras que la curva de las solicitudes aprobadas es más dispersa y se extiende hacia valores más altos, sugiriendo que las aprobaciones están asociadas con un rango más amplio de valores.

La superposición de las dos curvas, especialmente en el rango bajo de V2, indica que esta variable por sí sola no es un fuerte discriminante entre aprobaciones y rechazos. Sin embargo, a medida que los valores de V2 aumentan, las aprobaciones se vuelven más frecuentes.

En resumen, V2 parece influir en la decisión de crédito, pero debido a la considerable superposición, podría requerir análisis adicionales junto con otras variables para mejorar la capacidad predictiva.

```{r}
# Boxplot chetados con puntos
ggplot(melted_data, aes(x = V16, y = Value, color=V16, fill=V16)) +
  geom_boxplot(alpha=0.6) +
  geom_jitter(color="black") +
  scale_fill_discrete() +
  scale_color_discrete() +
  xlab("Categoría de V16") +
  ylab("Valores de V2") +
  ggtitle("Distribución de V2 por Categoría de V16")
```

# Analisis multivariable

##  V16 V9

```{r}
melted_data <- melt(credit, id.vars = "V16", measure.vars = "V9", variable.name = "Variable", value.name = "Value")

melted_data

library(ggplot2)
ggplot(melted_data, aes(x = V16, y = Value)) +
  geom_boxplot() +
  xlab("Categoría de V16") +
  ylab("Valores de V2") +
  ggtitle("Distribución de V2 por Categoría de V16")

```

Observando el boxplot podemos decir que ambos tienen una distribución simétrica. En cuanto a los outliners, no hay en exceso (en rechazada algo más). Ambas tienen un sesgo positivo, ya que los bigotes superiores tienen mayor longitud. 

```{r}

# Gráfico de densidad ajustado
ggplot(data = melted_data, aes(x = Value, color = V16, fill = V16)) +
  geom_density(alpha = 0.6) +
  scale_fill_discrete() +
  scale_color_discrete() +
  ylab("Densidad") +  # Etiqueta para el eje y
  xlab("Valores") +   # Etiqueta para el eje x
  ggtitle("Densidad de Valores por Especie")  # Título del gráfico

```

Es interesante apreciar que hay ciertas zonas en las que se ensancha el gráfico de aprobadas y adelgaza el gráfico de rechazadas y viceversa. Por la relación comentada con anterioridad. Puede ser que en los intervalos [0,100] y [300,500] aproximadamente, correspondan a casas pertenecientes a barrios más adinerados.


# Pre-procesado de datos (I): Tratamiento de outliers y nulos:

Comenzamos renombrando las columnas en base a la información que tenemos:

```{r}
colnames(combined_credit)[colnames(combined_credit) == "V1"] <- "Genero"
colnames(combined_credit)[colnames(combined_credit) == "V2"] <- "Edad"
colnames(combined_credit)[colnames(combined_credit) == "V3"] <- "Deuda"
colnames(combined_credit)[colnames(combined_credit) == "V4"] <- "EstadoCivil"
colnames(combined_credit)[colnames(combined_credit) == "V8"] <- "AnyosContratado"
colnames(combined_credit)[colnames(combined_credit) == "V10"] <- "Empleado"
colnames(combined_credit)[colnames(combined_credit) == "V11"] <- "Solvencia"
colnames(combined_credit)[colnames(combined_credit) == "V13"] <- "composicionPoblacion"
```

## Tratamiento de valores fuera de rango

Para ello, vamos a diseñar una función que detecte outliners. Como hemos aprendido a hacerlo es con la siguiente fórmula: si valor **∈** [Q1 - 1.5 \* RI, Q3 + 1.5 \* RI] se considera outliner. Siendo RI (Rango Intercuatil).

```{r}
credit.Datos.Train <- combined_credit[combined_credit$Origen == "train", ]
credit.Datos.Test <- combined_credit[combined_credit$Origen == "test", ]

detectar_outliers <- function(x) {
  Q1 <- quantile(x, 0.25, na.rm = TRUE)
  Q3 <- quantile(x, 0.75, na.rm = TRUE)
  RI <- Q3 - Q1
  limite_inferior <- Q1 - 1.5 * RI
  limite_superior <- Q3 + 1.5 * RI
  return(x < limite_inferior | x > limite_superior)
}

credit.Datos.continuas <- credit.Datos.Train[, sapply(credit.Datos.Train, is.numeric)]
outliners <- lapply(credit.Datos.continuas, detectar_outliers)

# Contar los outliers por variable
sumas_outliners <- sapply(outliners, sum)
sumas_outliners
```

Sin embargo, estos datos no son tan representativos. Necesitamos saber qué porcentaje son atípicos del total de datos.

```{r}
numero_filas <- nrow(na.omit(credit.Datos.Train))

proporcion <- function(x) {
  return(x/numero_filas*100)
}

proporciones <- lapply(sumas_outliners, proporcion)
proporciones
```

Según hemos estado investigando cuando el porcentaje de outliners se encuentra entre 5% y 15% hay que tratarlos ya que pueden ser problemáticos. Siendo estos: AnyosContratado, Solvencia, V14 y V15:

```{r}
boxplot(credit.Datos.Train$AnyosContratado,boxwex=0.15,ylab="AnyosContratado")
rug(jitter(credit.Datos.Train$AnyosContratado),side=2)
abline(h=mean(credit.Datos.Train$AnyosContratado,na.rm=T),lty=2)

boxplot(credit.Datos.Train$Solvencia,boxwex=0.15,ylab="Solvencia")
rug(jitter(credit.Datos.Train$Solvencia),side=2)
abline(h=mean(credit.Datos.Train$Solvencia,na.rm=T),lty=2)

boxplot(credit.Datos.Train$V14,boxwex=0.15,ylab="V14")
rug(jitter(credit.Datos.Train$V14),side=2)
abline(h=mean(credit.Datos.Train$V14,na.rm=T),lty=2)

boxplot(credit.Datos.Train$V15,boxwex=0.15,ylab="V15")
rug(jitter(credit.Datos.Train$V15),side=2)
abline(h=mean(credit.Datos.Train$V15,na.rm=T),lty=2)
```

Decidimos que queremos equiparar a los valores fuera de rango con los valores extremos. Esto se llama winsorización:\

```{r}
# Función para calcular límites de winsorización (IR)
calcular_limites <- function(x) {
  Q1 <- quantile(x, 0.25, na.rm = TRUE)
  Q3 <- quantile(x, 0.75, na.rm = TRUE)
  RI <- Q3 - Q1
  limite_inferior <- Q1 - 1.5 * RI
  limite_superior <- Q3 + 1.5 * RI
  return(c(limite_inferior, limite_superior))
}

# Función para aplicar winsorización con límites definidos
winsorizar_con_limites <- function(x, limites) {
  limite_inferior <- limites[1]
  limite_superior <- limites[2]
  x[x < limite_inferior] <- limite_inferior
  x[x > limite_superior] <- limite_superior
  return(x)
}

columnas_a_winsorizar <- c("AnyosContratado", "Solvencia", "V14", "V15")

credit.Datos.Train.wins <- credit.Datos.Train
credit.Datos.Test.wins <- credit.Datos.Test

# Calcular límites en el conjunto de entrenamiento
limites_winsorizacion <- lapply(credit.Datos.Train.wins[, columnas_a_winsorizar], calcular_limites)

# Winsorizar el conjunto de entrenamiento
for (col in columnas_a_winsorizar) {
  credit.Datos.Train.wins[[col]] <- winsorizar_con_limites(credit.Datos.Train.wins[[col]], limites_winsorizacion[[col]])
}

# Aplicar los mismos límites al conjunto de prueba
for (col in columnas_a_winsorizar) {
  credit.Datos.Test.wins[[col]] <- winsorizar_con_limites(credit.Datos.Test.wins[[col]], limites_winsorizacion[[col]])
}

# En el caso de que NO queramos usar la winsorización solo debemos de comentar líneas
credit.Datos.Train <- credit.Datos.Train.wins
credit.Datos.Test <- credit.Datos.Test.wins
```

Comprobamos que ya no hay valores fuera de rango, excepto en "Deuda" que el porcentaje es muy bajo.

```{r}
credit.Datos.continuas <- credit.Datos.Train[, sapply(credit.Datos.Train, is.numeric)]
outliners <- lapply(credit.Datos.continuas, detectar_outliers)

# Contar los outliers por variable
sumas_outliners <- sapply(outliners, sum)
sumas_outliners
```

Sin embargo, debemos de analizar si en "Deuda" hay errores evidentes que puedan perjudicar el rendimiento del modelo:

```{r}
summary(credit.Datos.Train$Deuda)
```

No parece que haya valores incorrectos. En caso de que hubiera valores negativos, sería sospechoso, pero no parece que sea así.

Sin embargo en Edad nos podemos dar cuenta que hay valores erroneos, ya que los menores de edad en Estados Unidos no pueden solicitar créditos. Por tanto, podemos sustituir los valores menos a 18, por 18:

```{r}

ajustar_edad <- function(x) {
  x[x < 18] <- 18
  return(x)
}

credit.Datos.Train$Edad <- ajustar_edad(credit.Datos.Train$Edad)
credit.Datos.Test$Edad <- ajustar_edad(credit.Datos.Test$Edad)

summary(credit.Datos.Train$Edad)
summary(credit.Datos.Test$Edad)
```

Como vemos hemos conseguido transformarlo de forma correcta.

## Tratamiento de valores nulos

Primero vamos a ver cuántos valores nulos tiene cada variable. Esto nos indica que variables hay que transformar:

```{r}
combined_credit <- rbind(credit.Datos.Train, credit.Datos.Test)
num_na <- sapply(combined_credit, function(x) sum(is.na(x)))
print(num_na)
```

Primero de todo, vamos a analizar las variables continuas para analizar su distribución y elegir el tipo de sustitución idónea para cada una de ellas. Pero antes debemos de asegurarnos de no violar el principio de independencia de conjuntos, es necesario volver a separar los datos, para tener actualizadas las bases de datos de entrenamiento y validación:

**Breve inciso:** Realmente, no hace falta separar la base de datos, ya que en la base de datos de validación no hay nulos. Sin embargo, hemos considerado que se trata de una buena práctica, para siempre caer en ese detalle.

```{r}
credit.Datos.Train <- combined_credit[combined_credit$Origen == "train", ]
credit.Datos.Test <- combined_credit[combined_credit$Origen == "test", ]

# Verificar dimensiones
dim(credit.Datos.Train)  # Debe coincidir con la tabla original de entrenamiento
dim(credit.Datos.Test)   # Debe coincidir con la tabla original de validación

```

Como vemos las dimensiones son las correctas. De hecho tenemos una columna más, ya que con ella distinguimos si se trata de un conjunto de validación y testing.

Ahora si podemos analizar las variables de credit:\

```{r}
# histograma enriquecido para Edad
hist(credit.Datos.Train$Edad, xlab="",
main="Máximo valor de Edad", ylim=c(0,0.07),probability=T)
lines(density(credit.Datos.Train$Edad,na.rm=T))
rug(jitter(credit.Datos.Train$Edad))
```

La mediana es robusta frente a sesgos y outliers, lo que la hace ideal para distribuciones asimétricas como esta. Mantendrá el equilibrio en el rango más común (20–40 años). Tampoco es necesario complicar la imputación ya que hay pocos valores nulos. En este caso 12.

```{r}

credit.Datos.Test.imp <- credit.Datos.Test
credit.Datos.Train.imp <- credit.Datos.Train

# Calcular la mediana en el conjunto de entrenamiento
mediana_edad <- median(credit.Datos.Train.imp$Edad, na.rm = TRUE)

# Imputar en el conjunto de entrenamiento
credit.Datos.Train.imp$Edad[is.na(credit.Datos.Train.imp$Edad)] <- mediana_edad

# Imputar en el conjunto de validación usando la mediana del entrenamiento
credit.Datos.Test.imp$Edad[is.na(credit.Datos.Test.imp$Edad)] <- mediana_edad

test_imp_num_na <- sapply(credit.Datos.Test.imp, function(x) sum(is.na(x)))
train_imp_num_na <- sapply(credit.Datos.Train.imp, function(x) sum(is.na(x)))
print(train_imp_num_na+test_imp_num_na)
```

Ahora vamos a analizar la variable V14 (continua).

```{r}
# histograma enriquecido para V14
hist(credit.Datos.Train$V14, xlab="",
main="Máximo valor de V14", ylim=c(0,0.004),probability=T)
lines(density(credit.Datos.Train$V14,na.rm=T))
rug(jitter(credit.Datos.Train$V14))

summary(credit.Datos.Train$V14)
```

La variable V14, según la gráfica y el resumen estadístico, presenta una distribución muy asimétrica, con la mayoría de los valores concentrados en el rango bajo (entre 0 y 280), pero con algunos valores muy altos (hasta 2000, outliners). Dado que hay 13 valores faltantes (NA), que no son muchos, pero sí más que las anteriores, debemos de elegir una buena ténica de imputación:

```{r}
# Calcular la mediana solo en el conjunto de entrenamiento
mediana_v14 <- median(credit.Datos.Train.imp$V14, na.rm = TRUE)

# Imputar NA en el conjunto de entrenamiento
credit.Datos.Train.imp$V14[is.na(credit.Datos.Train.imp$V14)] <- mediana_v14

# Imputar NA en el conjunto de validación
credit.Datos.Test.imp$V14[is.na(credit.Datos.Test.imp$V14)] <- mediana_v14

test_imp_num_na <- sapply(credit.Datos.Test.imp, function(x) sum(is.na(x)))
train_imp_num_na <- sapply(credit.Datos.Train.imp, function(x) sum(is.na(x)))
print(train_imp_num_na+test_imp_num_na)
```

Ahora solo nos quedan variables categóricas.

Procedemos con la primera variable cetegórica, Género:\

```{r}
frecuencias <- table(credit.Datos.Train$Genero)

barplot(frecuencias, 
        main = "Distribución de Genero", 
        xlab = "Genero", 
        ylab = "Frecuencia", 
        col = "skyblue")

```

Debido al bajo número de NA y sabiendo que la categoría altamente dominante de b (hombres, según hemos especulado). Pensamos que lo más apropiado es asumir que son hombres, imputación por la moda. Imputar por "unknown" pensamos que no beneficia en absoluto el algoritmo.

```{r}
moda_genero <- "b"

# Imputar en el conjunto de entrenamiento
credit.Datos.Train.imp$Genero[is.na(credit.Datos.Train.imp$Genero)] <- moda_genero

# Imputar en el conjunto de validación
credit.Datos.Test.imp$Genero[is.na(credit.Datos.Test.imp$Genero)] <- moda_genero

test_imp_num_na <- sapply(credit.Datos.Test.imp, function(x) sum(is.na(x)))
train_imp_num_na <- sapply(credit.Datos.Train.imp, function(x) sum(is.na(x)))
print(train_imp_num_na+test_imp_num_na)
```

La siguiente es la variable "EstadoCivil". Variable categórica con 3 valores categóricos, siendo su dominio {l,u,y}. Vamos a ver la distribución que sigue:

```{r}
frecuencias <- table(credit.Datos.Train$EstadoCivil)

barplot(frecuencias, 
        main = "Distribución de EstadoCivil", 
        xlab = "EstadoCivil", 
        ylab = "Frecuencia", 
        col = "skyblue")
```

Como en el caso anterior, y con más razón aún, vamos a imputar por la moda. Es evidente que hay una categoría muy dominante, y el bajo número de NA hace que no vaya a variar prácticamente la distribución:

```{r}
moda_genero <- "u"

# Imputar en el conjunto de entrenamiento
credit.Datos.Train.imp$EstadoCivil[is.na(credit.Datos.Train.imp$EstadoCivil)] <- moda_genero

# Imputar en el conjunto de validación
credit.Datos.Test.imp$EstadoCivil[is.na(credit.Datos.Test.imp$EstadoCivil)] <- moda_genero

test_imp_num_na <- sapply(credit.Datos.Test.imp, function(x) sum(is.na(x)))
train_imp_num_na <- sapply(credit.Datos.Train.imp, function(x) sum(is.na(x)))
print(train_imp_num_na+test_imp_num_na)
```

Analizamos la variable categórica V5 para saber qué forma tienen los datos:

```{r}
frecuencias <- table(credit.Datos.Train$V5)

barplot(frecuencias, 
        main = "Distribución de V5", 
        xlab = "V5", 
        ylab = "Frecuencia", 
        col = "skyblue")
```

Podemos imputar por la moda, ya que la categoría "g" es claramente dominante, y no cambiará mucho la distribución:

```{r}
moda_genero <- "g"

# Imputar en el conjunto de entrenamiento
credit.Datos.Train.imp$V5[is.na(credit.Datos.Train.imp$V5)] <- moda_genero

# Imputar en el conjunto de validación
credit.Datos.Test.imp$V5[is.na(credit.Datos.Test.imp$V5)] <- moda_genero

test_imp_num_na <- sapply(credit.Datos.Test.imp, function(x) sum(is.na(x)))
train_imp_num_na <- sapply(credit.Datos.Train.imp, function(x) sum(is.na(x)))
print(train_imp_num_na+test_imp_num_na)
```

Procedemos a evaluar V6 (categórica):

```{r}
frecuencias <- table(credit.Datos.Train$V6)

barplot(frecuencias, 
        main = "Distribución de V6", 
        xlab = "V6", 
        ylab = "Frecuencia", 
        col = "skyblue")
```

Como V6 no tiene categorías tan predominantes, por tanto, necesitamos emplear otra técnica. Como tenemos 9 NA, que no son muchos, tampoco tenemos por qué complicarlo mucho. Algo interesante que podemos hacer es imputación aleatoria ponderada. Como su propio nombre indica consiste en generar categorías de forma aleatoria teniendo en cuenta su participación en la variable.

```{r}
set.seed(123)
categorias <- names(table(credit.Datos.Train.imp$V6))
probabilidades <- prop.table(table(credit.Datos.Train.imp$V6))

# Imputar valores NA
credit.Datos.Train.imp$V6[is.na(credit.Datos.Train.imp$V6)] <- sample(categorias, size = sum(is.na(credit.Datos.Train.imp$V6)), replace = TRUE, prob = probabilidades)

sum(is.na(credit.Datos.Train.imp$V6))

test_imp_num_na <- sapply(credit.Datos.Test.imp, function(x) sum(is.na(x)))
train_imp_num_na <- sapply(credit.Datos.Train.imp, function(x) sum(is.na(x)))
print(train_imp_num_na+test_imp_num_na)
```

Por último, analizamos la variable categórica V7:

```{r}
frecuencias <- table(credit.Datos.Train$V7)

barplot(frecuencias, 
        main = "Distribución de V7", 
        xlab = "V7", 
        ylab = "Frecuencia", 
        col = "skyblue")
```

Es evidente que una sustitución por la moda es muy interesante en esta variable también:

```{r}
moda_genero <- "v"

# Imputar en el conjunto de entrenamiento
credit.Datos.Train.imp$V7[is.na(credit.Datos.Train.imp$V7)] <- moda_genero

# Imputar en el conjunto de validación
credit.Datos.Test.imp$V7[is.na(credit.Datos.Test.imp$V7)] <- moda_genero

test_imp_num_na <- sapply(credit.Datos.Test.imp, function(x) sum(is.na(x)))
train_imp_num_na <- sapply(credit.Datos.Train.imp, function(x) sum(is.na(x)))
print(train_imp_num_na+test_imp_num_na)
```

A continuación, vamos a comprobar que la distribución de las variables imputadas, siguen siendo casi iguales (no hayan cambiado mucho):

```{r}
# Configurar la ventana gráfica para dos gráficos lado a lado
par(mfrow = c(1, 2))  # 1 fila, 2 columnas

#____________________Genero_________________________
frecuencias <- table(credit.Datos.Train$Genero)
barplot(frecuencias, 
        main = "Distribución de Genero", 
        xlab = "Genero", 
        ylab = "Frecuencia", 
        col = "skyblue")

frecuencias <- table(credit.Datos.Train.imp$Genero)
barplot(frecuencias, 
        main = "Distribución de Genero", 
        xlab = "Genero", 
        ylab = "Frecuencia", 
        col = "lightgreen")

#____________________Edad_________________________
hist(credit.Datos.Train$Edad, 
     main = "Histograma de Edad", 
     xlab = "Edad", 
     col = "skyblue")

hist(credit.Datos.Train.imp$Edad, 
     main = "Histograma de Edad", 
     xlab = "Edad", 
     col = "lightgreen")

#________________________EstadoCivil______________________
frecuencias <- table(credit.Datos.Train$EstadoCivil)
barplot(frecuencias, 
        main = "Distribución de EstadoCivil", 
        xlab = "EstadoCivil", 
        ylab = "Frecuencia", 
        col = "skyblue")

frecuencias <- table(credit.Datos.Train.imp$EstadoCivil)
barplot(frecuencias, 
        main = "Distribución de EstadoCivil", 
        xlab = "EstadoCivil", 
        ylab = "Frecuencia", 
        col = "lightgreen")

#____________________V5_______________
frecuencias <- table(credit.Datos.Train$V5)
barplot(frecuencias, 
        main = "Distribución de V5", 
        xlab = "V5", 
        ylab = "Frecuencia", 
        col = "skyblue")

frecuencias <- table(credit.Datos.Train.imp$V5)
barplot(frecuencias, 
        main = "Distribución de V5", 
        xlab = "V5", 
        ylab = "Frecuencia", 
        col = "lightgreen")

#____________________V6_______________
frecuencias <- table(credit.Datos.Train$V6)
barplot(frecuencias, 
        main = "Distribución de V6", 
        xlab = "V6", 
        ylab = "Frecuencia", 
        col = "skyblue")

frecuencias <- table(credit.Datos.Train.imp$V6)
barplot(frecuencias, 
        main = "Distribución de V6", 
        xlab = "V6", 
        ylab = "Frecuencia", 
        col = "lightgreen")

#____________________V7_______________
frecuencias <- table(credit.Datos.Train$V7)
barplot(frecuencias, 
        main = "Distribución de V7", 
        xlab = "V7", 
        ylab = "Frecuencia", 
        col = "skyblue")

frecuencias <- table(credit.Datos.Train.imp$V7)
barplot(frecuencias, 
        main = "Distribución de V7", 
        xlab = "V7", 
        ylab = "Frecuencia", 
        col = "lightgreen")

#____________________V14_______________
hist(credit.Datos.Train$V14, 
     main = "Histograma de V14", 
     xlab = "V14", 
     col = "skyblue")

hist(credit.Datos.Train.imp$V14, 
     main = "Histograma de V14", 
     xlab = "V14", 
     col = "lightgreen")
```

Como es evidente no ha cambiado casi nada (inapreciable). Entre otras cosas, debido al bajo número de NA.

En cuanto a la sustitución mediante estudio de correlaciones, podemos ver si hay alguna de la siguiente manera (lo hacemos con los datos sin imputar para justificar que no era necesario usarlo):

```{r}
# Solo las columnas numéricas
credit.Datos.numericas <- credit.Datos.Train[, sapply(credit.Datos.Train, is.numeric)]

matriz_correlacion <- cor(credit.Datos.numericas, use = "complete.obs")

print(matriz_correlacion)
```

Es evidente que no hay ninguna fuerte correlación entre variables, por tanto, hemos excluido esta opción de imputación. Al no haber correlación tampoco es interesante la sustitución de variables numéricas mediante clustering (knn).

```{r}
gen_plots <- function() {

  credit.Datos.continuas <- credit.Datos.Train.imp[, sapply(credit.Datos.Train.imp, is.numeric)]
  
  for (name in colnames(credit.Datos.continuas)) {
    hist(credit.Datos.continuas[[name]], 
         main = paste("Histograma de", name),
         xlab = name,
         col = "lightgreen")
  }
}

# Ejecutar la función
gen_plots()

```

En el caso en el que queramos quitar toda la imputación, comentar este código:

```{r}
credit.Datos.Test <- credit.Datos.Test.imp
credit.Datos.Train <- credit.Datos.Train.imp
```

# Pre-procesado de datos (II): Eliminar predictores correlados o de poca Varianza

## 1. Eliminar variables con poca Varianza

Debemos de comprobar si la base de datos tiene columnas con poca varianza. Esto lo podemos comprobar con nearZero():

**Aclaración:** No tenemos en cuenta la última columna con valores {train, test} ya que no cuentan para el análisis.

```{r}
nearZeroVar(credit.Datos.Train[, 1:16])
nearZeroVar(credit.Datos.Test[, 1:16])
```

Como vemos no hay ninguna columna que tenga varianza cercana a cero, lo que nos indica que los datos están ya bastante "limpios".

## 2. Eliminar variables correladas:

Ya hemos justificado antes que no hay variables correladas, observando la matriz de correlación.

```{r}
#Dummys
credit.Data.To.Dummy <- rbind(credit.Datos.Train, credit.Datos.Test)
credit.Var.Salida <- c("V16")
credit.Var.Salida.Usada <- c("V16", "Origen")
credit.Var.Entrada.Usadas <- setdiff(names(credit.Data.To.Dummy),credit.Var.Salida.Usada)
credit.Var.Entrada.Dummys <- credit.Var.Entrada.Usadas
summary(credit.Data.To.Dummy)
# También qué factores serán Ranked  (tienen un orden implícito o una jerarquía definida, 
# pero no se consideran completamente ordinales en el sentido clásico)
credit.Ranked.Factors <- c("Genero", "EstadoCivil", "Empleado","composicionPoblacion", "V9", "V12")
# Se separan los factores ordenados (no hay que hacer Dummy)
credit.All.Factors<-names(credit.Data.To.Dummy[,credit.Var.Entrada.Usadas])[sapply(
  credit.Data.To.Dummy[,credit.Var.Entrada.Usadas], FUN=is.factor)]
credit.Ordered.Factors<-names(credit.Data.To.Dummy[,credit.Var.Entrada.Usadas])[sapply(
  credit.Data.To.Dummy[,credit.Var.Entrada.Usadas], FUN=is.ordered)]
# No hay ordenados 
credit.Non.Ordered.Factors<-setdiff(credit.All.Factors,credit.Ordered.Factors)
credit.No.Ranked<-setdiff(credit.Non.Ordered.Factors,credit.Ranked.Factors)

# Se calculan las dummy de los ranked y no ranked
credit.Cols.Ranked <- NULL
if (length(credit.Ranked.Factors) > 0) {
  credit.Dummy.Ranked <- dummyVars(
    paste("~", paste(credit.Ranked.Factors, sep = "", collapse = " + "), collapse = ""),
    data = credit.Data.To.Dummy, fullRank = T)
  credit.Cols.Ranked <- data.frame(predict(credit.Dummy.Ranked,
                                           newdata = credit.Data.To.Dummy))
}
credit.Cols.No.Ranked <- NULL
if (length(credit.No.Ranked) > 0) {
  credit.Dummy.No.Ranked <- dummyVars(
    paste("~", paste(credit.No.Ranked, sep = "", collapse = " + "), collapse = ""),
    data = credit.Data.To.Dummy)
  credit.Cols.No.Ranked <- data.frame(predict(credit.Dummy.No.Ranked,
                                              newdata = credit.Data.To.Dummy))
}


credit.Cols.Salida <- data.frame(credit.Data.To.Dummy[, credit.Var.Salida.Usada])
names(credit.Cols.Salida) <- credit.Var.Salida.Usada

# Se eliminan de los datos originales todas las columnas a reemplazar
credit.Data.With.Dummy <- credit.Data.To.Dummy[, setdiff(names(credit.Data.To.Dummy),credit.Var.Salida)]

credit.Data.With.Dummy <- credit.Data.With.Dummy[, setdiff(names(credit.Data.With.Dummy),credit.No.Ranked)]

credit.Data.With.Dummy <- credit.Data.With.Dummy[, setdiff(names(credit.Data.With.Dummy), credit.Ranked.Factors)]

credit.Data.With.Dummy <- credit.Data.With.Dummy[, setdiff(names(credit.Data.With.Dummy), credit.Ordered.Factors)]

# Se añaden todas las columnas nuevas
if (!is.null(credit.Cols.Ranked))
  credit.Data.With.Dummy <- cbind(credit.Data.With.Dummy, credit.Cols.Ranked)
if (!is.null(credit.Cols.No.Ranked))
  credit.Data.With.Dummy <- cbind(credit.Data.With.Dummy, credit.Cols.No.Ranked)
credit.Data.With.Dummy <- cbind(credit.Data.With.Dummy, credit.Cols.Salida)
```

En caso de querer eliminar el paso de Dummy, podemos simplemente comentar el siguiente código:

```{r}
credit.Datos.Train <- credit.Data.With.Dummy[credit.Data.With.Dummy$Origen == "train", ]
credit.Datos.Test <- credit.Data.With.Dummy[credit.Data.With.Dummy$Origen == "test", ]
```

# Pre-procesado de datos (III): Transformando y construyendo variables (Feature Engineering).

## 1. Sobre transformaciones de las variables de salida

No hay casos en los que a escala pueda dar errores de representación de los valores por la precisión de la representación. Y por temas de legibilidad ya hemos cambiado los valores de salida. Siendo el signo más y menos, aprobada y rechazada respectivamente.

## 2. Transformación de Variables

### 2.1. Escalado:

NORMALIZACIÓN CENTER SCALE

Mostramos los datos antes de hacer la normalización:

```{r}
credit.Datos.Todo <- rbind(credit.Datos.Train, credit.Datos.Test)
credit.Continuas <- credit.Datos.Todo[, sapply(credit.Datos.Todo, is.numeric)]
ggplot() + 
  geom_density(aes(x = credit.Continuas$Edad, fill = "Edad"), alpha = 0.5) +
  geom_density(aes(x = credit.Continuas$Deuda, fill = "Deuda"), alpha = 0.5) +
  geom_density(aes(x = credit.Continuas$AnyosContratado, fill = "Años Contratado"), alpha = 0.5) +
  geom_density(aes(x = credit.Continuas$Solvencia, fill = "Solvencia"), alpha = 0.5) +
  geom_density(aes(x = credit.Continuas$V14, fill = "V14"), alpha = 0.5) +
  geom_density(aes(x = credit.Continuas$V15, fill = "V15"), alpha = 0.5) +
  labs(
    title = "Distribución de Variables Continuas (Density Plot)",
    x = "Valor",
    y = "Densidad"
  ) +
  scale_fill_manual(
    values = c("lightgreen", "lightblue", "lightcoral", "lightyellow", "lightpink", "lightgray"),
    name = "Variables",
    labels = c("Edad", "Deuda", "Años Contratado", "Solvencia", "V14", "V15")
  ) +
  theme_minimal() +
  xlim(-1, 100)
theme(legend.title = element_blank())
```

Hay que destacar que hemos limitado la x a 100, para poder visualizar el gráfico (de otra forma sería ilegible). Los datos llegarían hasta el 1000. Ahora vamos a normalizar y ver el gráfico de nuevo:

Anotar que las categóricas se quitan solas

```{r}
credit.Datos.Todo <- rbind(credit.Datos.Train, credit.Datos.Test)

credit.Var.Salida.Usada <- c("V16")
credit.Var.Entrada.Usadas <- setdiff(names(credit.Datos.Todo),credit.Var.Salida.Usada)
credit.Var.Entrada.Escaladas <- credit.Var.Entrada.Usadas

credit.preProc.CS.Mod<-preProcess(credit.Datos.Train[credit.Var.Entrada.Escaladas],method=c("center","scale"))
credit.Datos.Train.Transf.CS<-predict(credit.preProc.CS.Mod,credit.Datos.Train)
credit.Datos.Test.Transf.CS<-predict(credit.preProc.CS.Mod,credit.Datos.Test)

VarToPlot<-credit.Var.Entrada.Escaladas

d1<-densityplot(formula(paste("~",paste(VarToPlot,sep="",collapse =" + "),collapse="")),data=credit.Datos.Train,main="Variables sin Normalizar",plot.points=F)

d2<-densityplot(formula(paste("~",paste(VarToPlot,sep="",collapse =" + "),collapse="")),data=credit.Datos.Train.Transf.CS, main="Variables Normalizadas",plot.points=F)

# Gráficos en blanco y negro (mejor para imprimir)
trellis.par.set(theme = standard.theme("pdf",color=FALSE))
print(d1,position=c(0,0,0.5,1),more=T)
print(d2,position=c(0.5,0,1,1))
# Volvemos a gráficos normales
trellis.par.set(theme = standard.theme("pdf"))


credit.Datos.Train.Transf.CS <- credit.Datos.Train.Transf.CS  # Este es el dataset normalizado

# Identificar las variables numéricas (continuas)
credit.Vars.Continuas <- sapply(credit.Datos.Train.Transf.CS, is.numeric)
credit.normalizadasCS <- credit.Datos.Train.Transf.CS[, credit.Vars.Continuas]

##############
# Crear el gráfico directamente
ggplot() + 
  geom_density(aes(x = credit.normalizadasCS$Edad, fill = "Edad"), alpha = 0.5) +
  geom_density(aes(x = credit.normalizadasCS$Deuda, fill = "Deuda"), alpha = 0.5) +
  geom_density(aes(x = credit.normalizadasCS$AnyosContratado, fill = "Años Contratado"), alpha = 0.5) +
  geom_density(aes(x = credit.normalizadasCS$Solvencia, fill = "Solvencia"), alpha = 0.5) +
  geom_density(aes(x = credit.normalizadasCS$V14, fill = "V14"), alpha = 0.5) +
  geom_density(aes(x = credit.normalizadasCS$V15, fill = "V15"), alpha = 0.5) +
  labs(
    title = "Distribución de Variables Continuas (Density Plot)",
    x = "Valor",
    y = "Densidad"
  ) +
  scale_fill_manual(
    values = c("lightgreen", "lightblue", "lightcoral", "lightyellow", "lightpink", "lightgray"),
    name = "Variables",
    labels = c("Edad", "Deuda", "Años Contratado", "Solvencia", "V14", "V15")
  ) +
  theme_minimal() +
  #xlim(-1, 2.5)
theme(legend.title = element_blank())
```

En caso de no querer incluir la normalización center scale en nuestros datos, comentar esto:

```{r}
credit.Datos.Train <- credit.normalizadasCS[credit.normalizadasCS$Origen == "train", ]
credit.Datos.Test <- credit.normalizadasCS[credit.normalizadasCS$Origen == "test", ]
```

RANGE:\

```{r}
#Normalizacion Range [0,1]
credit.preProc.Range.Mod<-preProcess(credit.Datos.Train[credit.Var.Entrada.Escaladas],
                                  method="range")
credit.Datos.Train.Transf.Range<-predict(credit.preProc.Range.Mod,credit.Datos.Train)
credit.Datos.Test.Transf.Range<-predict(credit.preProc.Range.Mod,credit.Datos.Test)

credit.Vars.Continuas <- sapply(credit.Datos.Train.Transf.Range, is.numeric)
credit.normalizadasRange <- credit.Datos.Train.Transf.Range[, credit.Vars.Continuas]

##############
# Crear el gráfico directamente
ggplot() + 
  geom_density(aes(x = credit.normalizadasRange$Edad, fill = "Edad"), alpha = 0.5) +
  geom_density(aes(x = credit.normalizadasRange$Deuda, fill = "Deuda"), alpha = 0.5) +
  geom_density(aes(x = credit.normalizadasRange$AnyosContratado, fill = "Años Contratado"), alpha = 0.5) +
  geom_density(aes(x = credit.normalizadasRange$Solvencia, fill = "Solvencia"), alpha = 0.5) +
  geom_density(aes(x = credit.normalizadasRange$V14, fill = "V14"), alpha = 0.5) +
  geom_density(aes(x = credit.normalizadasRange$V15, fill = "V15"), alpha = 0.5) +
  labs(
    title = "Distribución de Variables Continuas (Density Plot)",
    x = "Valor",
    y = "Densidad"
  ) +
  scale_fill_manual(
    values = c("lightgreen", "lightblue", "lightcoral", "lightyellow", "lightpink", "lightgray"),
    name = "Variables",
    labels = c("Edad", "Deuda", "Años Contratado", "Solvencia", "V14", "V15")
  ) +
  theme_minimal() +
  xlim(-1, 1)
theme(legend.title = element_blank())
```

En caso de NO querer incluir esta normalización de tipo rango, comentar este código:

```{r}
#credit.Datos.Train <- credit.normalizadasRange[credit.normalizadasRange$Origen == "train", ]
#credit.Datos.Test <- credit.normalizadasRange[credit.normalizadasRange$Origen == "test", ]
```

PCA:

```{r}

#Eliminar previamente valores atipicos, muy sebsible
#Sobre los datos de entrenamiento IMPORTANTE
credit.Var.Entrada.Continuas <- which(sapply(credit.Datos.Train[, -ncol(credit.Datos.Train)], is.numeric))

credit.PreProc.Pca.Mod<-preProcess(credit.Datos.Train[credit.Var.Entrada.Continuas],method=c("pca"),thresh = 0.9)
credit.Datos.Train.Transf.PCA <- predict(credit.PreProc.Pca.Mod, credit.Datos.Train)
credit.Datos.Test.Transf.PCA <- predict(credit.PreProc.Pca.Mod, credit.Datos.Test)

print(credit.PreProc.Pca.Mod)
summary(credit.Datos.Train.Transf.PCA)
summary(credit.Datos.Test.Transf.PCA)
```

En caso de NO querer incluir PCA, comentar el siguiente código:

```{r}
credit.Datos.Train <- credit.Datos.Train.Transf.PCA
credit.Datos.Test <- credit.Datos.Test.Transf.PCA
```

ICA:\

```{r}
# IMPORTANTE LEER: EN MI CASO HE TENIDO QUE INSTALAR:
#           sudo apt install liblapack-dev libblas-dev gfortran

# Sobre credit (se calcula sobre train)
library(fastICA)
library(caret)


credit.PreProc.Ica.Mod <- preProcess(credit.Datos.Train[credit.Var.Entrada.Continuas], method = c("ica"), n.comp = 2)
credit.Datos.Train.Transf.ICA <- predict(credit.PreProc.Ica.Mod, credit.Datos.Train)
credit.Datos.Test.Transf.ICA <- predict(credit.PreProc.Ica.Mod, credit.Datos.Test)
print(credit.PreProc.Ica.Mod)
summary(credit.Datos.Train.Transf.ICA)
summary(credit.Datos.Test.Transf.ICA)
```

En caso de NO querer incluir ICA, comentar el siguiente código:

```{r}
#credit.Datos.Train <- credit.Datos.Train.Transf.ICA
#credit.Datos.Test <- credit.Datos.Test.Transf.ICA
```

--------------------------------------------------------------------------------
# Entrenamiento de modelos



[lista de requisitos probados:]{.underline}

1.  Imputación estándar

2.  dummies

3.  Normalización CENTER SCALE (NO ME DEJA)

4.  PCA (NO ME DEJA AL NORMALIZAR)\


## GBM

```{r}
# Antes de nada:

# Eliminar la última columna del conjunto de entrenamiento
credit.Datos.Train <- credit.Datos.Train[, -ncol(credit.Datos.Train)]

# Eliminar la última columna del conjunto de prueba
credit.Datos.Test <- credit.Datos.Test[, -ncol(credit.Datos.Test)]



# Definir las variables de entrada y salida
credit.Var.Salida.Usada <- "V16"  # Cambiar según tu variable de salida
credit.Vars.Entrada.Usadas <- setdiff(names(credit.Datos.Train), credit.Var.Salida.Usada)

# Configurar el modelo GBM
set.seed(1234)
credit.modelo.gbm <- train(
  credit.Datos.Train[credit.Vars.Entrada.Usadas], # Variables de entrada
  credit.Datos.Train[[credit.Var.Salida.Usada]], # Variable de salida
  method = "gbm",                                # Método GBM
  trControl = trainControl(method = "cv", number = 5), # Validación cruzada
  verbose = FALSE                                # Evitar logs innecesarios
)

# Ver los detalles del modelo entrenado
print(credit.modelo.gbm)

# Predecir en el conjunto de prueba
credit.Pred.Test <- predict(credit.modelo.gbm, credit.Datos.Test)

# Evaluar el rendimiento en el conjunto de prueba
confusionMatrix(credit.Pred.Test, credit.Datos.Test[[credit.Var.Salida.Usada]])

```

Vemos procentaje:\

```{r}
# Realizar predicciones en el conjunto de prueba
predicciones_test <- predict(credit.modelo.gbm, credit.Datos.Test)

# Generar la matriz de confusión
matriz_confusion <- confusionMatrix(predicciones_test, credit.Datos.Test[[credit.Var.Salida.Usada]])

# Extraer el porcentaje de acierto
porcentaje_acierto <- matriz_confusion$overall["Accuracy"] * 100

# Mostrar el porcentaje de acierto
print(paste("Porcentaje de acierto:", round(porcentaje_acierto, 2), "%"))

```

























## Random Forest

Vamos a entreenar este modelo, pero antes vamos a ver los hiperparametros que tiene para ello usamos el siguiente comando:

```{r}
modelLookup(("rf"))
```
Como en random forest no permite datos NA hay que imputarlas o eliminarlas:


si las imputamos

```{r}
preProc <- preProcess(credit.Datos.Train[credit.Vars.Entrada.Usadas], method = "medianImpute")
credit.Datos.Train.Imputed <- predict(preProc, credit.Datos.Train)

```



si las eliminamos
```{r}

credit.Datos.Train.SinNA <- na.omit(credit.Datos.Train)

```


para verificar si sigue habiendo valores NA

```{r}
any(is.na(credit.Datos.Train.SinNA))

```


```{r}

# Definir las variables de entrada y salida
credit.Var.Salida.Usada <- "V16"  # Cambiar según tu variable de salida
credit.Vars.Entrada.Usadas <- setdiff(names(credit.Datos.Train), credit.Var.Salida.Usada)


set.seed(1234) # Para reproducibilidad

# Configuración de validación cruzada
control_rf <- trainControl(
  method = "cv",      # Validación cruzada
  number = 5,         # Número de folds
  verboseIter = FALSE # Evitar logs en la consola
)

# Entrenamiento del modelo RF
system.time(
  credit.modelo.rf <- train(
    x = credit.Datos.Train.SinNA[credit.Vars.Entrada.Usadas], 
    y = credit.Datos.Train.SinNA[[credit.Var.Salida.Usada]], 
    method = "rf",
    trControl = trainControl(method = "cv", number = 5),
    tuneGrid = expand.grid(mtry = c(2, 3, 4)),
    ntree = 500
  )
)


# Resumen del modelo
print(credit.modelo.rf)


# Resumen del modelo
print(credit.modelo.rf)



# Predicciones en el conjunto de prueba
credit.Pred.Test.rf <- predict(credit.modelo.rf, credit.Datos.Test)

# Matriz de confusión para evaluar el rendimiento
confusion_rf <- confusionMatrix(credit.Pred.Test.rf, credit.Datos.Test[[credit.Var.Salida.Usada]])

# Ver la matriz de confusión y la precisión
print(confusion_rf)

# Porcentaje de acierto
porcentaje_acierto_rf <- confusion_rf$overall["Accuracy"] * 100
print(paste("Porcentaje de acierto con RF:", round(porcentaje_acierto_rf, 2), "%"))


## NO TERMINADO FALTAN MAS COSAS



















```








